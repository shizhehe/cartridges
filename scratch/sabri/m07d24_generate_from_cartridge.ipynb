{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nHello hello<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from cartridges.initialization.tokenization_utils import CARTRIDGES_LLAMA3_CHAT_TEMPLATE\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4b\")\n",
    "\n",
    "data = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello hello\"\n",
    "        },\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    "    template=None\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta-llama/Llama-3.2-3B-Instruct'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you can only change requires_grad flags of leaf variables.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m z = x * y\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(z.requires_grad)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(z.requires_grad)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(x.requires_grad)\n",
      "\u001b[31mRuntimeError\u001b[39m: you can only change requires_grad flags of leaf variables."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1, 10, 10, requires_grad=True)\n",
    "y = torch.randn(1, 10, 10, requires_grad=True)\n",
    "\n",
    "z = x * y\n",
    "\n",
    "print(z.requires_grad)\n",
    "\n",
    "z.requires_grad = True\n",
    "\n",
    "print(z.requires_grad)\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabri/miniconda3/envs/cartridges12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 128000, Token: <|begin_of_text|>\n",
      "Token ID: 128006, Token: <|start_header_id|>\n",
      "Token ID: 9125, Token: system\n",
      "Token ID: 128007, Token: <|end_header_id|>\n",
      "Token ID: 271, Token: \n",
      "\n",
      "\n",
      "Token ID: 38766, Token: Cut\n",
      "Token ID: 1303, Token: ting\n",
      "Token ID: 33025, Token:  Knowledge\n",
      "Token ID: 2696, Token:  Date\n",
      "Token ID: 25, Token: :\n",
      "Token ID: 6790, Token:  December\n",
      "Token ID: 220, Token:  \n",
      "Token ID: 2366, Token: 202\n",
      "Token ID: 18, Token: 3\n",
      "Token ID: 198, Token: \n",
      "\n",
      "Token ID: 15724, Token: Today\n",
      "Token ID: 2696, Token:  Date\n",
      "Token ID: 25, Token: :\n",
      "Token ID: 220, Token:  \n",
      "Token ID: 1313, Token: 22\n",
      "Token ID: 10263, Token:  Jul\n",
      "Token ID: 220, Token:  \n",
      "Token ID: 2366, Token: 202\n",
      "Token ID: 20, Token: 5\n",
      "Token ID: 271, Token: \n",
      "\n",
      "\n",
      "Token ID: 128009, Token: <|eot_id|>\n",
      "Token ID: 128006, Token: <|start_header_id|>\n",
      "Token ID: 882, Token: user\n",
      "Token ID: 128007, Token: <|end_header_id|>\n",
      "Token ID: 271, Token: \n",
      "\n",
      "\n",
      "Token ID: 128009, Token: <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "data = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\n",
    "        },\n",
    "    ],\n",
    "    tokenize=True,\n",
    ")\n",
    "token_ids = data\n",
    "tokens_with_ids = [(token_id, tokenizer.decode([token_id])) for token_id in token_ids]\n",
    "for token_id, token in tokens_with_ids:\n",
    "    print(f\"Token ID: {token_id}, Token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = data[\"rows\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8586,\n",
       " 374,\n",
       " 264,\n",
       " 6369,\n",
       " 1984,\n",
       " 21745,\n",
       " 287,\n",
       " 459,\n",
       " 445,\n",
       " 11237,\n",
       " 311,\n",
       " 63179,\n",
       " 264,\n",
       " 3230,\n",
       " 3857,\n",
       " 315,\n",
       " 279,\n",
       " 43194,\n",
       " 1473,\n",
       " 1,\n",
       " 9370,\n",
       " 5730,\n",
       " 553,\n",
       " 279,\n",
       " 6593,\n",
       " 3335,\n",
       " 5296,\n",
       " 369,\n",
       " 8893,\n",
       " 364,\n",
       " 41,\n",
       " 484,\n",
       " 21293,\n",
       " 6,\n",
       " 320,\n",
       " 926,\n",
       " 25,\n",
       " 8893,\n",
       " 62,\n",
       " 2371,\n",
       " 8,\n",
       " 449,\n",
       " 279,\n",
       " 2768,\n",
       " 3649,\n",
       " 25,\n",
       " 9405,\n",
       " 389,\n",
       " 220,\n",
       " 4468,\n",
       " 20,\n",
       " 12,\n",
       " 2589,\n",
       " 12,\n",
       " 2705,\n",
       " 220,\n",
       " 410,\n",
       " 25,\n",
       " 410,\n",
       " 25,\n",
       " 410,\n",
       " 11,\n",
       " 29704,\n",
       " 449,\n",
       " 98144,\n",
       " 9572,\n",
       " 11,\n",
       " 323,\n",
       " 3549,\n",
       " 389,\n",
       " 220,\n",
       " 2371,\n",
       " 14,\n",
       " 1721,\n",
       " 14,\n",
       " 2366,\n",
       " 18,\n",
       " 13,\n",
       " 45863,\n",
       " 11,\n",
       " 4587,\n",
       " 5357,\n",
       " 389,\n",
       " 279,\n",
       " 3857,\n",
       " 25891,\n",
       " 78265,\n",
       " 7560,\n",
       " 41014,\n",
       " 22241,\n",
       " 320,\n",
       " 2304,\n",
       " 14,\n",
       " 1721,\n",
       " 14,\n",
       " 2366,\n",
       " 18,\n",
       " 33395,\n",
       " 6,\n",
       " 323,\n",
       " 8819,\n",
       " 279,\n",
       " 1401,\n",
       " 3585,\n",
       " 5552,\n",
       " 311,\n",
       " 18083,\n",
       " 13,\n",
       " 21293,\n",
       " 596,\n",
       " 6514,\n",
       " 11,\n",
       " 23842,\n",
       " 11,\n",
       " 323,\n",
       " 904,\n",
       " 28289,\n",
       " 14955,\n",
       " 505,\n",
       " 279,\n",
       " 19084,\n",
       " 8737,\n",
       " 10887,\n",
       " 389,\n",
       " 220,\n",
       " 2371,\n",
       " 14,\n",
       " 975,\n",
       " 12,\n",
       " 1419,\n",
       " 13,\n",
       " 5321,\n",
       " 9955,\n",
       " 1137,\n",
       " 279,\n",
       " 2038,\n",
       " 1139,\n",
       " 264,\n",
       " 64694,\n",
       " 12399,\n",
       " 315,\n",
       " 13489,\n",
       " 220,\n",
       " 1049,\n",
       " 12,\n",
       " 5154,\n",
       " 4339,\n",
       " 1210,\n",
       " 128009]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.messages[0].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'Message' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m token_ids = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cartridges12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1641\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1638\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcontinue_final_message is not compatible with return_assistant_tokens_mask.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1640\u001b[39m template_kwargs = {**\u001b[38;5;28mself\u001b[39m.special_tokens_map, **kwargs}  \u001b[38;5;66;03m# kwargs overwrite special tokens if both are present\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1641\u001b[39m rendered_chat, generation_indices = \u001b[43mrender_jinja_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[32m   1653\u001b[39m     rendered_chat = rendered_chat[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cartridges12/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py:498\u001b[39m, in \u001b[36mrender_jinja_template\u001b[39m\u001b[34m(conversations, tools, documents, chat_template, return_assistant_tokens_mask, continue_final_message, add_generation_prompt, **kwargs)\u001b[39m\n\u001b[32m    496\u001b[39m     all_generation_indices.append(generation_indices)\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m     rendered_chat = \u001b[43mcompiled_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[32m    506\u001b[39m     final_message = chat[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cartridges12/lib/python3.12/site-packages/jinja2/environment.py:1295\u001b[39m, in \u001b[36mTemplate.render\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.environment.concat(\u001b[38;5;28mself\u001b[39m.root_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cartridges12/lib/python3.12/site-packages/jinja2/environment.py:942\u001b[39m, in \u001b[36mEnvironment.handle_exception\u001b[39m\u001b[34m(self, source)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[32m    938\u001b[39m \u001b[33;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source=source)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<template>:68\u001b[39m, in \u001b[36mtop-level template code\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: argument of type 'Message' is not iterable"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        *row.messages[1:]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>',\n",
       " 'user',\n",
       " '\\n',\n",
       " 'Please',\n",
       " ' summarize',\n",
       " ' the',\n",
       " ' medical',\n",
       " ' note',\n",
       " ' with',\n",
       " ' ID',\n",
       " ' **',\n",
       " 'text',\n",
       " '_',\n",
       " '9',\n",
       " '**',\n",
       " ' from',\n",
       " ' the',\n",
       " ' medical',\n",
       " ' record',\n",
       " ' of',\n",
       " ' **',\n",
       " 'J',\n",
       " 'ill',\n",
       " ' Anderson',\n",
       " '**',\n",
       " ' (',\n",
       " 'patient',\n",
       " ' ID',\n",
       " ':',\n",
       " ' patient',\n",
       " '_',\n",
       " '0',\n",
       " '4',\n",
       " '),',\n",
       " ' born',\n",
       " ' on',\n",
       " ' **',\n",
       " '1',\n",
       " '9',\n",
       " '7',\n",
       " '5',\n",
       " '-',\n",
       " '0',\n",
       " '7',\n",
       " '-',\n",
       " '0',\n",
       " '6',\n",
       " '**',\n",
       " '.',\n",
       " ' The',\n",
       " ' note',\n",
       " ' was',\n",
       " ' dated',\n",
       " ' **',\n",
       " '1',\n",
       " '2',\n",
       " '/',\n",
       " '0',\n",
       " '2',\n",
       " '/',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '**',\n",
       " ' and',\n",
       " ' is',\n",
       " ' part',\n",
       " ' of',\n",
       " ' the',\n",
       " ' ',\n",
       " '1',\n",
       " '3',\n",
       " ' notes',\n",
       " ' in',\n",
       " ' her',\n",
       " ' medical',\n",
       " ' record',\n",
       " '.',\n",
       " ' The',\n",
       " ' summary',\n",
       " ' should',\n",
       " ' include',\n",
       " ' the',\n",
       " ' following',\n",
       " ' details',\n",
       " ':\\n\\n',\n",
       " '-',\n",
       " ' Patient',\n",
       " ' name',\n",
       " ':',\n",
       " ' **',\n",
       " 'J',\n",
       " 'ill',\n",
       " ' Anderson',\n",
       " '**\\n',\n",
       " '-',\n",
       " ' Patient',\n",
       " ' ID',\n",
       " ':',\n",
       " ' **',\n",
       " 'patient',\n",
       " '_',\n",
       " '0',\n",
       " '4',\n",
       " '**\\n',\n",
       " '-',\n",
       " ' Date',\n",
       " ' of',\n",
       " ' birth',\n",
       " ':',\n",
       " ' **',\n",
       " '1',\n",
       " '9',\n",
       " '7',\n",
       " '5',\n",
       " '-',\n",
       " '0',\n",
       " '7',\n",
       " '-',\n",
       " '0',\n",
       " '6',\n",
       " '**\\n',\n",
       " '-',\n",
       " ' Date',\n",
       " ' of',\n",
       " ' the',\n",
       " ' medical',\n",
       " ' note',\n",
       " ':',\n",
       " ' **',\n",
       " '1',\n",
       " '2',\n",
       " '/',\n",
       " '0',\n",
       " '2',\n",
       " '/',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '**\\n',\n",
       " '-',\n",
       " ' Diagnosis',\n",
       " ':',\n",
       " ' **',\n",
       " 'Progress',\n",
       " 'ive',\n",
       " ' tumor',\n",
       " ' disease',\n",
       " ' under',\n",
       " ' gem',\n",
       " 'cit',\n",
       " 'ab',\n",
       " 'ine',\n",
       " '/n',\n",
       " 'ab',\n",
       " '-p',\n",
       " 'acl',\n",
       " 'it',\n",
       " 'ax',\n",
       " 'el',\n",
       " ' for',\n",
       " ' pancreatic',\n",
       " ' head',\n",
       " ' carcinoma',\n",
       " '**,',\n",
       " ' with',\n",
       " ' onset',\n",
       " ' on',\n",
       " ' **',\n",
       " '0',\n",
       " '9',\n",
       " '/',\n",
       " '2',\n",
       " '2',\n",
       " '/',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '**\\n',\n",
       " '-',\n",
       " ' Imaging',\n",
       " ' findings',\n",
       " ' from',\n",
       " ' **',\n",
       " 'CT',\n",
       " ' pancre',\n",
       " 'as',\n",
       " ',',\n",
       " ' thor',\n",
       " 'ax',\n",
       " ',',\n",
       " ' abdomen',\n",
       " ',',\n",
       " ' pel',\n",
       " 'vis',\n",
       " '**',\n",
       " ':\\n',\n",
       " ' ',\n",
       " ' -',\n",
       " ' **',\n",
       " 'C',\n",
       " 'hest',\n",
       " '**:',\n",
       " ' N',\n",
       " 'od',\n",
       " 'ular',\n",
       " ' go',\n",
       " 'iter',\n",
       " ',',\n",
       " ' port',\n",
       " ' placement',\n",
       " ' in',\n",
       " ' the',\n",
       " ' right',\n",
       " ' chest',\n",
       " ',',\n",
       " ' no',\n",
       " ' suspicious',\n",
       " ' pulmonary',\n",
       " ' nod',\n",
       " 'ules',\n",
       " ',',\n",
       " ' no',\n",
       " ' increase',\n",
       " ' in',\n",
       " ' medi',\n",
       " 'ast',\n",
       " 'inal',\n",
       " ' or',\n",
       " ' ax',\n",
       " 'illary',\n",
       " ' lymph',\n",
       " ' nodes',\n",
       " '.\\n',\n",
       " ' ',\n",
       " ' -',\n",
       " ' **',\n",
       " 'Ab',\n",
       " 'dom',\n",
       " 'en',\n",
       " '**:',\n",
       " ' F',\n",
       " 'atty',\n",
       " ' liver',\n",
       " ' with',\n",
       " ' uneven',\n",
       " ' contrast',\n",
       " ',',\n",
       " ' possible',\n",
       " ' spl',\n",
       " 'enic',\n",
       " ' cyst',\n",
       " ',',\n",
       " ' right',\n",
       " ' kidney',\n",
       " ' cyst',\n",
       " 's',\n",
       " ',',\n",
       " ' decreasing',\n",
       " ' pancreatic',\n",
       " ' tumor',\n",
       " ',',\n",
       " ' mes',\n",
       " 'enter',\n",
       " 'ic',\n",
       " ' lymph',\n",
       " ' nodes',\n",
       " ' (',\n",
       " 'up',\n",
       " ' to',\n",
       " ' ',\n",
       " '9',\n",
       " 'mm',\n",
       " '),',\n",
       " ' divert',\n",
       " 'ic',\n",
       " 'ula',\n",
       " ' in',\n",
       " ' the',\n",
       " ' left',\n",
       " ' colon',\n",
       " ',',\n",
       " ' hardened',\n",
       " ' abdominal',\n",
       " ' vessels',\n",
       " ',',\n",
       " ' elong',\n",
       " 'ated',\n",
       " ' right',\n",
       " ' il',\n",
       " 'iac',\n",
       " ' artery',\n",
       " '.\\n',\n",
       " ' ',\n",
       " ' -',\n",
       " ' **',\n",
       " 'Sp',\n",
       " 'ine',\n",
       " '**:',\n",
       " ' Deg',\n",
       " 'enerative',\n",
       " ' changes',\n",
       " ',',\n",
       " ' s',\n",
       " 'pond',\n",
       " 'yl',\n",
       " 'olist',\n",
       " 'h',\n",
       " 'esis',\n",
       " ' of',\n",
       " ' the',\n",
       " ' fifth',\n",
       " ' lum',\n",
       " 'bar',\n",
       " ' vert',\n",
       " 'ebra',\n",
       " ' over',\n",
       " ' the',\n",
       " ' first',\n",
       " ' sac',\n",
       " 'ral',\n",
       " ' vert',\n",
       " 'ebra',\n",
       " ' (',\n",
       " 'grade',\n",
       " ' ',\n",
       " '1',\n",
       " '-',\n",
       " '2',\n",
       " '),',\n",
       " ' indentation',\n",
       " ' at',\n",
       " ' the',\n",
       " ' top',\n",
       " ' of',\n",
       " ' the',\n",
       " ' tenth',\n",
       " ' thor',\n",
       " 'acic',\n",
       " ' vert',\n",
       " 'ebra',\n",
       " '.\\n',\n",
       " '-',\n",
       " ' Imp',\n",
       " 'ression',\n",
       " ':',\n",
       " ' Advanced',\n",
       " ' pancreatic',\n",
       " ' cancer',\n",
       " ' decreasing',\n",
       " ' in',\n",
       " ' size',\n",
       " ',',\n",
       " ' smaller',\n",
       " ' lymph',\n",
       " ' nodes',\n",
       " ',',\n",
       " ' no',\n",
       " ' metast',\n",
       " 'atic',\n",
       " ' spread',\n",
       " '.\\n',\n",
       " '-',\n",
       " ' Summary',\n",
       " ':',\n",
       " ' Patient',\n",
       " ' completed',\n",
       " ' ne',\n",
       " 'oad',\n",
       " 'ju',\n",
       " 'vant',\n",
       " ' chemotherapy',\n",
       " ',',\n",
       " ' pancreatic',\n",
       " ' head',\n",
       " ' re',\n",
       " 'section',\n",
       " ' is',\n",
       " ' now',\n",
       " ' possible',\n",
       " ',',\n",
       " ' appointment',\n",
       " ' scheduled',\n",
       " ' for',\n",
       " ' next',\n",
       " ' week',\n",
       " '.\\n\\n',\n",
       " 'Ensure',\n",
       " ' the',\n",
       " ' summary',\n",
       " ' is',\n",
       " ' concise',\n",
       " ',',\n",
       " ' clear',\n",
       " ',',\n",
       " ' and',\n",
       " ' includes',\n",
       " ' all',\n",
       " ' the',\n",
       " ' key',\n",
       " ' clinical',\n",
       " ' findings',\n",
       " ' and',\n",
       " ' conclusions',\n",
       " ' from',\n",
       " ' the',\n",
       " ' note',\n",
       " '.',\n",
       " '<|im_end|>',\n",
       " '\\n',\n",
       " '<|im_start|>',\n",
       " 'assistant',\n",
       " '\\n',\n",
       " '<think>',\n",
       " '\\n\\n',\n",
       " '</think>',\n",
       " '\\n\\n',\n",
       " '**',\n",
       " 'Medical',\n",
       " ' Note',\n",
       " ' Summary',\n",
       " ' â€“',\n",
       " ' Text',\n",
       " '_',\n",
       " '9',\n",
       " ' (',\n",
       " 'Patient',\n",
       " '_',\n",
       " '0',\n",
       " '4',\n",
       " ')**',\n",
       " '  \\n',\n",
       " '**',\n",
       " 'Patient',\n",
       " ' Name',\n",
       " ':**',\n",
       " ' Jill',\n",
       " ' Anderson',\n",
       " '  \\n',\n",
       " '**',\n",
       " 'Patient',\n",
       " ' ID',\n",
       " ':**',\n",
       " ' patient',\n",
       " '_',\n",
       " '0',\n",
       " '4',\n",
       " '  \\n',\n",
       " '**',\n",
       " 'Date',\n",
       " ' of',\n",
       " ' Birth',\n",
       " ':**',\n",
       " ' ',\n",
       " '1',\n",
       " '9',\n",
       " '7',\n",
       " '5',\n",
       " '-',\n",
       " '0',\n",
       " '7',\n",
       " '-',\n",
       " '0',\n",
       " '6',\n",
       " '  \\n',\n",
       " '**',\n",
       " 'Date',\n",
       " ' of',\n",
       " ' Note',\n",
       " ':**',\n",
       " ' ',\n",
       " '1',\n",
       " '2',\n",
       " '/',\n",
       " '0',\n",
       " '2',\n",
       " '/',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '  \\n',\n",
       " '**',\n",
       " 'Di',\n",
       " 'agnosis',\n",
       " ':**',\n",
       " ' Progressive',\n",
       " ' tumor',\n",
       " ' disease',\n",
       " ' under',\n",
       " ' gem',\n",
       " 'cit',\n",
       " 'ab',\n",
       " 'ine',\n",
       " '/n',\n",
       " 'ab',\n",
       " '-p',\n",
       " 'acl',\n",
       " 'it',\n",
       " 'ax',\n",
       " 'el',\n",
       " ' for',\n",
       " ' pancreatic',\n",
       " ' head',\n",
       " ' carcinoma',\n",
       " ' (',\n",
       " 'on',\n",
       " 'set',\n",
       " ':',\n",
       " ' ',\n",
       " '0',\n",
       " '9',\n",
       " '/',\n",
       " '2',\n",
       " '2',\n",
       " '/',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " ')',\n",
       " '  \\n\\n',\n",
       " '**',\n",
       " 'Im',\n",
       " 'aging',\n",
       " ' Find',\n",
       " 'ings',\n",
       " ' (',\n",
       " 'CT',\n",
       " ' Pan',\n",
       " 'cre',\n",
       " 'as',\n",
       " ',',\n",
       " ' Thor',\n",
       " 'ax',\n",
       " ',',\n",
       " ' Ab',\n",
       " 'dom',\n",
       " 'en',\n",
       " ',',\n",
       " ' Pel',\n",
       " 'vis',\n",
       " '):',\n",
       " '**',\n",
       " '  \\n',\n",
       " '-',\n",
       " ' **',\n",
       " 'C',\n",
       " 'hest',\n",
       " ':**',\n",
       " ' N',\n",
       " 'od',\n",
       " 'ular',\n",
       " ' go',\n",
       " 'iter',\n",
       " ',',\n",
       " ' port',\n",
       " ' placement',\n",
       " ' in',\n",
       " ' the',\n",
       " ' right',\n",
       " ' chest',\n",
       " ',',\n",
       " ' no',\n",
       " ' suspicious',\n",
       " ' pulmonary',\n",
       " ' nod',\n",
       " 'ules',\n",
       " ',',\n",
       " ' no',\n",
       " ' increase',\n",
       " ' in',\n",
       " ' medi',\n",
       " 'ast',\n",
       " 'inal',\n",
       " ' or',\n",
       " ' ax',\n",
       " 'illary',\n",
       " ' lymph',\n",
       " ' nodes',\n",
       " '.',\n",
       " '  \\n',\n",
       " '-',\n",
       " ' **',\n",
       " 'Ab',\n",
       " 'dom',\n",
       " 'en',\n",
       " ':**',\n",
       " ' F',\n",
       " 'atty',\n",
       " ' liver',\n",
       " ' with',\n",
       " ' uneven',\n",
       " ' contrast',\n",
       " ',',\n",
       " ' possible',\n",
       " ' spl',\n",
       " 'enic',\n",
       " ' cyst',\n",
       " ',',\n",
       " ' right',\n",
       " ' kidney',\n",
       " ' cyst',\n",
       " 's',\n",
       " ',',\n",
       " ' decreasing',\n",
       " ' pancreatic',\n",
       " ' tumor',\n",
       " ',',\n",
       " ' mes',\n",
       " 'enter',\n",
       " 'ic',\n",
       " ' lymph',\n",
       " ' nodes',\n",
       " ' (',\n",
       " 'up',\n",
       " ' to',\n",
       " ' ',\n",
       " '9',\n",
       " 'mm',\n",
       " '),',\n",
       " ' divert',\n",
       " 'ic',\n",
       " 'ula',\n",
       " ' in',\n",
       " ' the',\n",
       " ' left',\n",
       " ' colon',\n",
       " ',',\n",
       " ' hardened',\n",
       " ' abdominal',\n",
       " ' vessels',\n",
       " ',',\n",
       " ' elong',\n",
       " 'ated',\n",
       " ' right',\n",
       " ' il',\n",
       " 'iac',\n",
       " ' artery',\n",
       " '.',\n",
       " '  \\n',\n",
       " '-',\n",
       " ' **',\n",
       " 'Sp',\n",
       " 'ine',\n",
       " ':**',\n",
       " ' Deg',\n",
       " 'enerative',\n",
       " ' changes',\n",
       " ',',\n",
       " ' s',\n",
       " 'pond',\n",
       " 'yl',\n",
       " 'olist',\n",
       " 'h',\n",
       " 'esis',\n",
       " ' of',\n",
       " ' the',\n",
       " ' fifth',\n",
       " ' lum',\n",
       " 'bar',\n",
       " ' vert',\n",
       " 'ebra',\n",
       " ' over',\n",
       " ' the',\n",
       " ' first',\n",
       " ' sac',\n",
       " 'ral',\n",
       " ' vert',\n",
       " 'ebra',\n",
       " ' (',\n",
       " 'grade',\n",
       " ' ',\n",
       " '1',\n",
       " '-',\n",
       " '2',\n",
       " '),',\n",
       " ' indentation',\n",
       " ' at',\n",
       " ' the',\n",
       " ' top',\n",
       " ' of',\n",
       " ' the',\n",
       " ' tenth',\n",
       " ' thor',\n",
       " 'acic',\n",
       " ' vert',\n",
       " 'ebra',\n",
       " '.',\n",
       " '  \\n\\n',\n",
       " '**',\n",
       " 'Imp',\n",
       " 'ression',\n",
       " ':**',\n",
       " '  \\n',\n",
       " '-',\n",
       " ' Advanced',\n",
       " ' pancreatic',\n",
       " ' cancer',\n",
       " ' decreasing',\n",
       " ' in',\n",
       " ' size',\n",
       " '.',\n",
       " '  \\n',\n",
       " '-',\n",
       " ' L',\n",
       " 'ymph',\n",
       " ' nodes',\n",
       " ' smaller',\n",
       " ' than',\n",
       " ' before',\n",
       " '.',\n",
       " '  \\n',\n",
       " '-',\n",
       " ' No',\n",
       " ' signs',\n",
       " ' of',\n",
       " ' metast',\n",
       " 'atic',\n",
       " ' spread',\n",
       " '.',\n",
       " '  \\n\\n',\n",
       " '**',\n",
       " 'Summary',\n",
       " ':**',\n",
       " '  \\n',\n",
       " 'Mrs',\n",
       " '.',\n",
       " ' Anderson',\n",
       " ' completed',\n",
       " ' ne',\n",
       " 'oad',\n",
       " 'ju',\n",
       " 'vant',\n",
       " ' chemotherapy',\n",
       " '.',\n",
       " ' Pan',\n",
       " 'cre',\n",
       " 'atic',\n",
       " ' head',\n",
       " ' re',\n",
       " 'section',\n",
       " ' is',\n",
       " ' now',\n",
       " ' possible',\n",
       " ',',\n",
       " ' and',\n",
       " ' an',\n",
       " ' appointment',\n",
       " ' has',\n",
       " ' been',\n",
       " ' scheduled',\n",
       " ' for',\n",
       " ' next',\n",
       " ' week',\n",
       " '.',\n",
       " ' If',\n",
       " ' symptoms',\n",
       " ' persist',\n",
       " ' or',\n",
       " ' wors',\n",
       " 'en',\n",
       " ',',\n",
       " ' immediate',\n",
       " ' follow',\n",
       " '-up',\n",
       " ' is',\n",
       " ' recommended',\n",
       " '.',\n",
       " '<|im_end|>',\n",
       " '\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(token_id) for token_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5501,\n",
       " 62079,\n",
       " 279,\n",
       " 6457,\n",
       " 5185,\n",
       " 448,\n",
       " 3034,\n",
       " 3070,\n",
       " 1318,\n",
       " 62,\n",
       " 24,\n",
       " 334,\n",
       " 504,\n",
       " 279,\n",
       " 6457,\n",
       " 3255,\n",
       " 315,\n",
       " 3070,\n",
       " 41,\n",
       " 483,\n",
       " 20642,\n",
       " 334,\n",
       " 320,\n",
       " 22722,\n",
       " 3034,\n",
       " 25,\n",
       " 8720,\n",
       " 62,\n",
       " 15,\n",
       " 19,\n",
       " 701,\n",
       " 9223,\n",
       " 389,\n",
       " 3070,\n",
       " 16,\n",
       " 24,\n",
       " 22,\n",
       " 20,\n",
       " 12,\n",
       " 15,\n",
       " 22,\n",
       " 12,\n",
       " 15,\n",
       " 21,\n",
       " 334,\n",
       " 13,\n",
       " 576,\n",
       " 5185,\n",
       " 572,\n",
       " 29005,\n",
       " 3070,\n",
       " 16,\n",
       " 17,\n",
       " 14,\n",
       " 15,\n",
       " 17,\n",
       " 14,\n",
       " 17,\n",
       " 15,\n",
       " 17,\n",
       " 17,\n",
       " 334,\n",
       " 323,\n",
       " 374,\n",
       " 949,\n",
       " 315,\n",
       " 279,\n",
       " 220,\n",
       " 16,\n",
       " 18,\n",
       " 8388,\n",
       " 304,\n",
       " 1059,\n",
       " 6457,\n",
       " 3255,\n",
       " 13,\n",
       " 576,\n",
       " 12126,\n",
       " 1265,\n",
       " 2924,\n",
       " 279,\n",
       " 2701,\n",
       " 3565,\n",
       " 1447,\n",
       " 12,\n",
       " 28924,\n",
       " 829,\n",
       " 25,\n",
       " 3070,\n",
       " 41,\n",
       " 483,\n",
       " 20642,\n",
       " 1019,\n",
       " 12,\n",
       " 28924,\n",
       " 3034,\n",
       " 25,\n",
       " 3070,\n",
       " 22722,\n",
       " 62,\n",
       " 15,\n",
       " 19,\n",
       " 1019,\n",
       " 12,\n",
       " 2631,\n",
       " 315,\n",
       " 7194,\n",
       " 25,\n",
       " 3070,\n",
       " 16,\n",
       " 24,\n",
       " 22,\n",
       " 20,\n",
       " 12,\n",
       " 15,\n",
       " 22,\n",
       " 12,\n",
       " 15,\n",
       " 21,\n",
       " 1019,\n",
       " 12,\n",
       " 2631,\n",
       " 315,\n",
       " 279,\n",
       " 6457,\n",
       " 5185,\n",
       " 25,\n",
       " 3070,\n",
       " 16,\n",
       " 17,\n",
       " 14,\n",
       " 15,\n",
       " 17,\n",
       " 14,\n",
       " 17,\n",
       " 15,\n",
       " 17,\n",
       " 17,\n",
       " 1019,\n",
       " 12,\n",
       " 94352,\n",
       " 25,\n",
       " 3070,\n",
       " 9496,\n",
       " 533,\n",
       " 35154,\n",
       " 8457,\n",
       " 1212,\n",
       " 18747,\n",
       " 53861,\n",
       " 370,\n",
       " 482,\n",
       " 9612,\n",
       " 370,\n",
       " 2268,\n",
       " 47736,\n",
       " 275,\n",
       " 706,\n",
       " 301,\n",
       " 369,\n",
       " 97044,\n",
       " 1968,\n",
       " 88368,\n",
       " 97219,\n",
       " 448,\n",
       " 40980,\n",
       " 389,\n",
       " 3070,\n",
       " 15,\n",
       " 24,\n",
       " 14,\n",
       " 17,\n",
       " 17,\n",
       " 14,\n",
       " 17,\n",
       " 15,\n",
       " 17,\n",
       " 17,\n",
       " 1019,\n",
       " 12,\n",
       " 64506,\n",
       " 14613,\n",
       " 504,\n",
       " 3070,\n",
       " 1162,\n",
       " 61168,\n",
       " 300,\n",
       " 11,\n",
       " 72733,\n",
       " 706,\n",
       " 11,\n",
       " 63672,\n",
       " 11,\n",
       " 11814,\n",
       " 2682,\n",
       " 334,\n",
       " 510,\n",
       " 220,\n",
       " 481,\n",
       " 3070,\n",
       " 34,\n",
       " 6402,\n",
       " 95518,\n",
       " 451,\n",
       " 347,\n",
       " 1276,\n",
       " 728,\n",
       " 2015,\n",
       " 11,\n",
       " 2635,\n",
       " 21448,\n",
       " 304,\n",
       " 279,\n",
       " 1290,\n",
       " 15138,\n",
       " 11,\n",
       " 902,\n",
       " 31327,\n",
       " 69424,\n",
       " 16004,\n",
       " 2425,\n",
       " 11,\n",
       " 902,\n",
       " 5263,\n",
       " 304,\n",
       " 24127,\n",
       " 559,\n",
       " 977,\n",
       " 476,\n",
       " 3859,\n",
       " 34505,\n",
       " 42645,\n",
       " 7798,\n",
       " 624,\n",
       " 220,\n",
       " 481,\n",
       " 3070,\n",
       " 5830,\n",
       " 5600,\n",
       " 268,\n",
       " 95518,\n",
       " 434,\n",
       " 22908,\n",
       " 25506,\n",
       " 448,\n",
       " 60337,\n",
       " 12872,\n",
       " 11,\n",
       " 3204,\n",
       " 12503,\n",
       " 55889,\n",
       " 62481,\n",
       " 11,\n",
       " 1290,\n",
       " 37942,\n",
       " 62481,\n",
       " 82,\n",
       " 11,\n",
       " 43549,\n",
       " 97044,\n",
       " 35154,\n",
       " 11,\n",
       " 10846,\n",
       " 1950,\n",
       " 292,\n",
       " 42645,\n",
       " 7798,\n",
       " 320,\n",
       " 454,\n",
       " 311,\n",
       " 220,\n",
       " 24,\n",
       " 3821,\n",
       " 701,\n",
       " 35998,\n",
       " 292,\n",
       " 5607,\n",
       " 304,\n",
       " 279,\n",
       " 2115,\n",
       " 14889,\n",
       " 11,\n",
       " 70736,\n",
       " 55956,\n",
       " 29980,\n",
       " 11,\n",
       " 73495,\n",
       " 657,\n",
       " 1290,\n",
       " 3815,\n",
       " 17569,\n",
       " 64315,\n",
       " 624,\n",
       " 220,\n",
       " 481,\n",
       " 3070,\n",
       " 6406,\n",
       " 482,\n",
       " 95518,\n",
       " 57237,\n",
       " 74889,\n",
       " 4344,\n",
       " 11,\n",
       " 274,\n",
       " 3511,\n",
       " 3923,\n",
       " 34675,\n",
       " 71,\n",
       " 13774,\n",
       " 315,\n",
       " 279,\n",
       " 17702,\n",
       " 40163,\n",
       " 2257,\n",
       " 5198,\n",
       " 50213,\n",
       " 916,\n",
       " 279,\n",
       " 1156,\n",
       " 11100,\n",
       " 3461,\n",
       " 5198,\n",
       " 50213,\n",
       " 320,\n",
       " 6937,\n",
       " 220,\n",
       " 16,\n",
       " 12,\n",
       " 17,\n",
       " 701,\n",
       " 69592,\n",
       " 518,\n",
       " 279,\n",
       " 1909,\n",
       " 315,\n",
       " 279,\n",
       " 55666,\n",
       " 72733,\n",
       " 93157,\n",
       " 5198,\n",
       " 50213,\n",
       " 624,\n",
       " 12,\n",
       " 14390,\n",
       " 11185,\n",
       " 25,\n",
       " 21159,\n",
       " 97044,\n",
       " 9387,\n",
       " 43549,\n",
       " 304,\n",
       " 1379,\n",
       " 11,\n",
       " 9155,\n",
       " 42645,\n",
       " 7798,\n",
       " 11,\n",
       " 902,\n",
       " 67270,\n",
       " 774,\n",
       " 8865,\n",
       " 624,\n",
       " 12,\n",
       " 21517,\n",
       " 25,\n",
       " 28924,\n",
       " 8145,\n",
       " 834,\n",
       " 2731,\n",
       " 8613,\n",
       " 76182,\n",
       " 61630,\n",
       " 11,\n",
       " 97044,\n",
       " 1968,\n",
       " 312,\n",
       " 2809,\n",
       " 374,\n",
       " 1431,\n",
       " 3204,\n",
       " 11,\n",
       " 17635,\n",
       " 13537,\n",
       " 369,\n",
       " 1790,\n",
       " 2003,\n",
       " 382,\n",
       " 64439,\n",
       " 279,\n",
       " 12126,\n",
       " 374,\n",
       " 63594,\n",
       " 11,\n",
       " 2797,\n",
       " 11,\n",
       " 323,\n",
       " 5646,\n",
       " 678,\n",
       " 279,\n",
       " 1376,\n",
       " 14490,\n",
       " 14613,\n",
       " 323,\n",
       " 30242,\n",
       " 504,\n",
       " 279,\n",
       " 5185,\n",
       " 13,\n",
       " 151645]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.messages[0].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, let me start by understanding the user's request. They want me to generate a single instruction for an LLM to summarize the most important part of the email conversation. \\n\\nFirst, I need to look at the email content provided. The conversation starts with a brief issue brief about adverse event reporting for AI, mentioning Stanford RegLab and HAI. It talks about the challenges in AI deployment, like unexpected behaviors by systems like GPT-4. Then there's a mention of a Washington Post article discussing child safety tech and a European Commission event. The user also provided some links and contact info.\\n\\nThe main points here are the issue brief's focus on adverse event reporting as a solution to AI risks, the importance of real-world testing, and the collaboration between policymakers, industry, and stakeholders. The key elements are the problem (underappreciated risks of AI post-deployment), the solution (adverse event reporting systems), and the implications for policy and industry.\\n\\nI need to make sure the instruction is concise and captures the essence. The user wants the LLM to summarize the most important part, so I should combine all these elements into a single, clear summary. Avoid any extra details and focus on the key elements: the problem, the solution, and the collaboration aspects. Let me check if I missed any critical points. The issue brief's recommendations for tailoring systems without requiring massive spending seem important too. \\n\\nPutting it all together, the instruction should highlight the problem of underappreciated AI risks post-deployment, the need for adverse event reporting systems to address these, and the collaborative approach between policymakers and stakeholders. That should cover the main points without being too verbose.\\n</think>\\n\\n**Instruction for LLM:** Summarize the key points of the email conversation by focusing on the issue brief about adverse event reporting for AI, the challenges in addressing AI risks post-deployment, and the recommendations for policymakers, industry, and stakeholders to enhance AI safety through real-world data-driven insights.<|im_end|>\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"rows\"][0].messages[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'token_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer.decode(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrows\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoken_ids\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'token_ids'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(data[\"rows\"].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[\"rows\"][0].messages[0].token_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cartridges12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
