\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage[normalem]{ulem} 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[breakable]{tcolorbox}
\usepackage{amsfonts}
\usepackage{bbold}
\usepackage{fontawesome5}
\usepackage{microtype}
\usepackage{nicefrac}
\usepackage{enumitem}
\usepackage{booktabs}

\newcommand{\mlblock}[2][2em]{%
  {\begingroup
   \par\noindent
   \parshape=1 #1 \dimexpr\linewidth-#1\relax
   #2\par
   \endgroup}%
}

% Examples and code
\newtcolorbox{examplebox}[1][]{
    colback=lightgray!10,
    colframe=black,
    boxrule=0.75pt,
    title=#1,
    fonttitle=\bfseries,
    left=3pt,
    right=3pt,
    top=2pt,
    bottom=2pt,
    breakable,  % Allow the box to span pages
    fontupper=\tiny, 
    fontlower=\tiny
}
\newcommand{\tab}{\hspace*{2em}}

\definecolor{maroon}{RGB}{128,0,0}

\newtcolorbox{exampleboxmaroon}[1][]{
    colback=lightgray!10,
    colframe=maroon,
    boxrule=0.75pt,
    title=#1,
    fonttitle=\bfseries,
    left=3pt,
    right=3pt,
    top=2pt,
    bottom=2pt,
    breakable,  % Allow the box to span pages
    fontupper=\tiny, 
    fontlower=\tiny
}

\newtcolorbox{exampleboxblue}[1][]{
    colback=lightgray!10,
    colframe=blue,
    boxrule=0.75pt,
    title=#1,
    fonttitle=\bfseries,
    left=3pt,
    right=3pt,
    top=2pt,
    bottom=2pt,
    breakable,  % Allow the box to span pages
    fontupper=\tiny, 
    fontlower=\tiny
}

\definecolor{darkforestgreen}{RGB}{34,139,34}

\newtcolorbox{exampleboxprinciples}[1][]{
    colback=lightgray!10,
    colframe=darkforestgreen,
    boxrule=0.75pt,
    title=#1,
    fonttitle=\bfseries,
    left=3pt,
    right=3pt,
    top=2pt,
    bottom=2pt,
    breakable,  % Allow the box to span pages
    fontupper=\tiny, 
    fontlower=\tiny
}

\title{An Information Theoretic Perspective on Agentic System Design}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Agentic language model (LM) systems have rapidly become central to modern workflows, powering applications like ``Deep Research'' and ``Claude Code.'' Beneath their apparent diversity lies a recurring pattern: smaller ``compressor'' LMs distill raw context into compact text that is then consumed by larger ``predictor'' LMs that interact with the user. Despite their popularity, the design of \emph{compressor-predictor} systems remains largely ad-hoc. Little guidance exists on how compressor and predictor choices shape downstream performance. Attributing gains to compression versus prediction typically requires exhaustive pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a \emph{noisy channel}, we introduce a simple estimator of the \emph{mutual information} between the context and its compression to quantify compression quality in a task-independent way. Using a \emph{rate-distortion} analysis, we show that mutual information strongly predicts downstream performance. With this toolkit, we perform a comprehensive empirical analysis across four datasets and three model families. Results reveal that larger compressors are both more accurate and more token-efficient, conveying more bits of mutual information per token. 
A 7B \textsc{Qwen-2.5} compressor, for instance, is $1.6\times$ more accurate, $4.6\times$ more concise, and conveys $5.5\times$ more bits of mutual information per token. Across the datasets studied, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors.
When applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover $99\%$ of frontier-LM accuracy at $26\%$ of API costs.



% Crucially, in the \textsc{Qwen-2.5} family, a larger $7$B compressor produces summaries that are up to $4.6\times$ shorter and yield up to $1.6\times$ higher accuracy with only $1.3\%$ more FLOPs. 

% Within agentic pipelines, a standard architectural pattern has emerged in which smaller ``compressor'' language models (LMs) distill raw context into compact summaries, which are then consumed by larger ``predictor'' LMs.

% While these systems have gained millions of users, a more principled analysis studying the design space of these agentic communication settings has been lacking. In this work, we adopt an information theoretic perspective on agentic systems by framing this asymmetric communication as an Information Bottleneck, where compressor quality governs how much task-relevant information flows downstream. Across multiple question-answering and open-domain settings, we find that larger compressors tend to be more concise and yield higher accuracy, with gains flattening at larger scales. Crucially, in the \textsc{Qwen-2.5} family, a larger $7$B compressor produces summaries that are up to $4.6\times$ shorter and yield up to $1.6\times$ higher accuracy with only $1.3\%$ more FLOPs. 
% We aim to explain this scaling behavior through mutual information (MI), a unifying metric for communication quality between LMs. 
% We propose a simple estimator for MI based on LM log-probabilities. Using it as a metric of compression fidelity, we show that the \textsc{Qwen-2.5} $7$B compressor transmits up to $5.5\times$ more MI per token than its $1.5$B counterpart. A rate-distortion analysis further reveals that compression quality is more critical than the capacity of the prediction model. When applied to a Deep Research system, these principles enable local compressors as small as $3$B parameters to recover $99\%$ of frontier-LM accuracy at $26\%$ of the cost. We conclude with practical guidelines for designing asymmetric agentic systems and allocating compute resources, highlighting compressors as the primary bottleneck.
\end{abstract}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/figure 0.png}
    \caption{
    \textbf{Why compressors matter.} Many agentic LM systems rely on compressors, and evaluating their quality in a task-independent way is essential as personal devices grow powerful enough to host them. \textbf{(Left)} A compressor condenses a long input \(X\) into a shorter summary \(Z\) that a predictor expands into the final answer \(Y\).
    \textbf{(Right)} Consumer hardware can now run increasingly large open-weight LMs, shown for Google Pixel phones and Apple MacBook laptops (2020-2025) under FP16 precision with memory estimates from Modal \citep{modal2024vram}. LM-Arena ranks (in parenthesis, lower is better) indicate relative performance.}
    \label{fig:figure-0-compressors-matter}
\end{figure}


\section{Introduction}\label{introduction}
Agentic language model (LM) systems have quickly become the backbone of modern AI workflows. From ``Deep Research'' systems \citep{hadfield2025multiagent} to 
Claude Code \citep{anthropic2025claude37}, 
millions of users now interact with pipelines where one model processes information and another builds on its outputs. 
While multi-LM architectures vary widely, this recurring pattern emerges across domains: smaller ``compressor'' models distill raw contexts into compact texts, which are then consumed by larger ``predictor'' models that output an answer and interact with the user (Figure~\ref{fig:figure-0-compressors-matter}).

At present, however, designing \emph{compressor-predictor} agentic systems remains largely trial-and-error. We lack a basic understanding of how the choice of compressor and predictor affects downstream performance.
Specifically, we cannot determine whether credit belongs to the compressor's distillation or the predictor's reasoning---we lack task-agnostic methods to evaluate the compressor's outputs independently from downstream performance. This attribution problem has immediate practical consequences: as new models are released and practitioners swap components, they have no principled way to identify which module to improve without sweeping across the compound system.

To address this gap, we take an information-theoretic perspective, viewing the compressor as a \emph{noisy channel} between the raw data and the predictor model. We propose using \emph{mutual information} (MI) between the raw context and its compression as a task-agnostic measure of compressor efficacy---analogous to how perplexity serves as a task-agnostic predictor of downstream performance \citep{scalinglaws, hoffmann2022empirical}. We then conduct a \emph{rate-distortion analysis} to measure how downstream task performance varies with the degree of compression. While it is intractable to calculate MI between two token sequences linked via a nonlinear model, we develop a simple, unbiased estimator that can be computed via modern inference servers without requiring full vocabulary log probabilities.

With this new information-theoretic lens, we perform extensive empirical studies on four datasets (\textsc{LongHealth} \citep{adams2024longhealth}, \textsc{FinanceBench} \citep{islam2023financebench}, \textsc{WildChat} \citep{zhao2024wildchat}, and \textsc{FineWeb} \citep{penedo2024the}) to answer the following questions:

\begin{enumerate}[leftmargin=20pt]
    \item \emph{Should you spend compute on the compressor or predictor?} We find that compressor quality overwhelmingly governs performance: scaling a \textsc{Qwen-2.5} compressor from 1B to 7B improves accuracy by $60\%$ whereas scaling the predictor from 70B to 405B yields only a $12\%$ improvement on \textsc{LongHealth}. This establishes a simple design principle: ``front-load" compute into compressors, perhaps running on-device, to reduce dependence on massive cloud-hosted predictors. (Section~\ref{sec:compressor-predictor-intelligence})
    \item \emph{Which compressors are more communication-efficient?} We find that larger compressors emit fewer output tokens while maintaining quality: in many model families, scaling compressor size not only improves accuracy but also produces compressions that are up to $4.6\times$ more concise. This token-efficiency yields sublinear scaling of FLOPs-per-generation as a function of model size.
    Strikingly, increasing \textsc{Qwen-2.5} compressor from 1.5B to 7B, only adds $1.3\%$ more FLOPs-per-generation. (Section~\ref{sec:compressor-predictor-intelligence})
    \item \emph{Which factors determine compression quality and how do they relate to downstream performance?} We find that compressors' outputs carry up to $5.4\times$ more MI about the context (Section~\ref{sec:mi-be-scaling}). Rate-distortion analysis reveals that information rate (MI per token) correlates strongly with downstream performance and perplexity ($r=-0.84$, $R^2 = 0.71$), providing a practical proxy for predicting system performance without full end-to-end evaluation (Section~\ref{sec:rate-distortion-correlation}). 
    \item \emph{With so many knobs to turn, which factors should you focus on for agentic system design?} We perform a meta-analysis across model families, sizes, and datasets, exposing a clear hierarchy of importance: compressor model family $>$ compressor size $>$ predictor size. (Section~\ref{sec:glm-analysis})
\end{enumerate}

As a practical demonstration, we apply our findings to a simplified Deep Research pipeline, where a single predictor aggregates outputs from multiple compressors. This system achieves $99\%$ of frontier-LM accuracy on the \textsc{DeepResearch Bench} benchmark \citep{du2025deepresearch} using local compressor models as small as 3B, reducing API costs by $74\%$ (Section~\ref{sec:scaling-deepresearch}).

% In summary, this work makes the following contributions:
% \begin{itemize}[leftmargin=20pt]
%     %\item Comprehensive scaling analysis of compressor and predictor model families and sizes, detailing when to invest in (local) compressors versus (remote) predictors (Section~\ref{sec:compressor-predictor-intelligence} and \ref{sec:scale-total-compute}).
%     \item Comprehensive scaling analysis of compressor and predictor families and sizes and practical takeaways (Section~\ref{sec:compressor-predictor-intelligence}).
%     %\item Practical guidelines on when to invest in (local) compressors versus (remote) predictors (Section \ref{sec:compressor-predictor-intelligence}).
%     \item Qualitative analysis of what errors occur at compression (Section \ref{sec:scale-total-compute}).
%     \item Information-theoretic analysis of compressor model families and sizes (\ref{sec:mi-be-scaling} and \ref{sec:supervisor-choice}) \textcolor{red}{(will shift subsection ordering in results section as well)}.
%     %\item Demonstration of a Deep Research system based on our design principles that achieves frontier-LM accuracy with local compressors, cutting total cost by $74\%$ (Section~\ref{sec:scaling-deepresearch}).
%     \item Demonstration of a Deep Research system based on our design principles that achieves frontier-LM accuracy with local compressors (Section~\ref{sec:scaling-deepresearch}).
% \end{itemize}

\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Related Work}
\label{sec:related-work}

Agentic architectures have become a practical way to improve cost-efficiency and accuracy on complex tasks \citep{du2025deepresearch, narayan2025minions}. Most prior work reports end-to-end utility such as accuracy, latency, and cost, while overlooking the communication channel itself. We instead analyze the intermediate communication, focusing on asymmetric compressor-predictor setups and their scaling. Design choices studied in the literature include model size \citep{narayan2025minions, wang2024mixture}, number of agents and communication rounds \citep{chen2024more, schluntz2025building}, research depth \citep{zhang2025web}, and planning or decomposition strategies \citep{saad2024archon, erdogan2025plan}. Overall, they find that performance improves with larger models, more agents, and additional rounds.



%In contrast to our work, these evaluations generally treat intermediary and final results as opaque strings and report only coarse high-level metrics such as downstream accuracy, text quality, latency, and cost. The do not quantify the amount of relevant information retained in the final text output. In our work, we generalize each predictor-compressor interaction in a Deep Research system as an information bottleneck, establishing fundamental principles on information transmitted through the bottleneck and the trade-off between communication rate and compute cost as multi-agent systems are scaled.

\subsection{Information Theoretic Problem Setup}
\label{sec:info-theoretic-setup}
Information theory has been applied to deep learning in several ways, primarily within single models. Researchers have used it to analyze intermediate representations in neural networks \citep{kawaguchi2023does, tishby2015deep}, evaluate natural language generations including summaries and token sequences \citep{arda2025rate, darrin-etal-2024-cosmic, shani2025tokens}, and define training objectives  \citep{kirsch2020unpacking, goldfeld2020information}. We extend this framework beyond single-model analysis to study communication between two LMs.

We start with a simple compressor-predictor system, including one LM compressor, and one LM predictor.
Let $X$ be the input context, $Q$ be the query, and $Y$ the answer to that query. We consider a two‐stage process
\[
X \;\xrightarrow{p(z\mid x)}\; Z \;\xrightarrow{p(y\mid z)}\; Y.
\]
The compressor $p(z\mid x)$ is modeled as a noisy channel, which compresses the context into a summary \(Z\). The predictor $p(y\mid z)$ then uses this summary to generate the answer \(Y\).
%We compress each document $X$ into $M=20$ distinct summaries, and evaluate the correctness of each answer $Y$ using a \textsc{GPT-4o-mini} judge.

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=1\linewidth]{figures/figure 1.png}
%    \caption{%
%    \textbf{Compression-prediction workflow.} 
%    \textbf{(Left)} Problem set-up: (remote) predictor and (local) compressor LM collaborate to solve a long-context QA task.
%    \textbf{(Center)} Compression-prediction workflow: A compressor LM compresses a long input document \(X\) into a short summary \(Z\) given a question \(Q\) and a predictor LM then expands \(Z\) into a final answer \(Y\).
%    \textbf{(Right)} Agentic design principles derived in this study.} 
%    \label{fig:figure-1}
%\end{figure}

% \paragraph*{Deep Research setup}
% We extend our study to model open-domain “Deep Research” workflows, where an predictor LM decomposes research tasks into subtasks and aggregates compressor LM outputs into a final report as illustrated in Figure~\ref{fig:deepres_setup}. This setting allows us to study more a complex collaboration setting. See Appendix~\ref{sec:appendix-deepresearch-workflow-setup} for full workflow details and experimental setup.

 We proceed to define our MI estimator. %We propose approximations of information-theoretic quantities to enable our analysis.
%While estimating these quantities with large nonlinear LMs is intractable, we propose approximations to enable our analysis.

\paragraph*{Estimating mutual information} 
%\subsection{Estimating mutual information}
\label{sec:mi-methods}
We want to measure the amount of information $Z$ contains about $X$, denoted as $I(X;Z)$.
Larger MI values $I(X;Z)$ indicate that the compression retains more information about the original context. To estimate MI, we start with the KL divergence \citep{kullback1951information} representation:
\begin{align*}
I(X;Z) &= D_{\mathrm{KL}}\bigl(p(x,z)\,\|\,p(x)\,p(z)\bigr) \\
&= \mathbb{E}_{x,z \sim p(x,z)} \left[ \log \frac{p(z|x)}{p(z)} \right].
\intertext{While computing $p(z)$ is intractable, we can sample from and evaluate our encoder $p(z|x)$, and take samples from the data distribution $p(x)$,}
&= \mathbb{E}_{x,z \sim p(x,z)} \left[ \log \frac{p(z|x)}{\mathbb{E}_{x'} [p(z|x')]} \right], \\
&\approx \frac{1}{NM} \sum_{i=1}^{N} \sum_{j=1}^{M} \left[ \log p(z_{ij} | x_i) - \log \left( \frac{1}{N} \sum_{l=1}^{N} p(z_{ij} | x_l) \right) \right] \equiv \hat{I}(X;Z),
\end{align*}
Where $z_{ij} \sim p(z | x_i), ~i=1, \dots, N, ~j=1, \dots, M$ and $x_l \sim p(x), ~ l=1, \dots, N$. 

Note that $\hat{I}(X;Z) \leq \log(N)$, where the maximum is achieved when $p(z_{ij} | x_i) = 1$ for all $i,j$ and $p(z_{ij} | x_l) = 0$ for all $l \neq i$. With this estimator, we do not need access to the full probability distribution over the vocabulary, which allows us to use accelerated inference engines such as SGLang \citep{zheng2024sglang}. In our experiments, each compression $Z$ is generated conditioned on a query $Q$, so we estimate \(I(X;Z \mid Q)\), which simply requires conditioning all terms on $Q$. While \(I(X;Z \mid Q) \geq 0\), in practice, our Monte Carlo estimate can produce small negative values due to finite-sample variance. We correct these artifacts by clipping MI to zero. 
%We explore an alternative estimator that approximates the upper bound on MI based on the Rao-Blackwell theorem on token-level logits:
%\begin{align*}
%    I(X;Z) &\leq \mathbb{E}_{p(x)} \left[ D_{KL}(p(z|x) \| q(z)) \right] \\
%    &\approx \frac{1}{nm} \sum_{i=1}^n \sum_{j=1}^m\sum_{t=1}^{T^{(i,j)}}D_{\text{KL}}\!\left( p(z_t \mid z_{1:t-1}^{(i,j)}, x^{(i)})\,\|\, q(z_t \mid z_{1:t-1}^{(i,j)}) \right)
%\end{align*}
%However, this estimator requires access to token-level logits over the full LM vocabulary, which is not provided by modern LM inference engines. Further details are presented in Appendix~\ref{appendix-mi-rb}.




%To more accurately capture compression output density with respect to the amount of bits retained from the original raw document, we define:
%\begin{align*}
%    R = \frac{I(X;Z \mid Q)}{N_{\text{out}}}
%\end{align*}
%as bit efficiency, measured in bits of mutual information per token.

\emph{Rate-distortion} theory quantifies the trade-off between \emph{rate}---i.e., the amount of information the compression carries about the input---and \emph{distortion}---the error in the prediction.
We define \emph{rate} (or \emph{bit-efficiency}) as $R = \frac{I(X;Z \mid Q)}{L}$, for $L$ output tokens (measured in bits of mutual information per token). For simplicity, we define distortion as $D = 1 - \text{ACC}(Z)$, by using the accuracy $0\leq ACC(Z) \leq 1$. See the $R$-$D$ curve in Figure~\ref{fig:figure-6/7-combined} (left). As rate increases, we expect distortion to converge towards a lower bound (irreducible error).
See Appendix~\ref{sec:appendix-rate-distortion} for further details.

%Rate captures how much information is preserved through the information bottleneck. 
%In our compression-prediction agent setting,
%Bit efficiency can be interpreted as rate.
%We measure distortion as $D = 1 - \text{ACC}(Z)$.
%\begin{align*}
%D = 1 - \text{ACC}(Z).
%\end{align*}
% For simplicity, we compute accuracy $0\leq ACC(Z) \leq 1$, and define distortion as $D = 1 - \text{ACC}(Z)$.
% T
% Plotting distortion \(D\), measured as $D = 1 - \text{ACC}(Z)$, versus rate \(R\), measured as bit efficiency, shows the trade-off as a rate–distortion curve (Figure~\ref{fig:figure-6/7-combined}, Left). 
%: each curve represents the tradeoff between \emph{rate} and \emph{distortion} for a single predictor model paired with a family of compressors. 
%, where a \textsc{Llama-3.1-70B} model makes a QA prediction based on the compression results of \textsc{Llama} compressor models. 
%As expected, we find in Figure~\ref{fig:figure-6/7-combined} that as we increase rate by increasing compressor size, distortion decreases sharply. 
% As rate increases, we expect distortion to converge towards a lower bound. 
%This curve allows us to examine the trade-off between communication efficiency and downstream utility in the context of an information bottleneck between agents. 

\section{Results}
\label{sec:results}

We evaluate the compressor-predictor system as an information bottleneck across different tasks and domains. 
%We begin by comparing the effects of scaling compressors versus predictors, showing that compressors dominate in driving downstream performance. 
Beginning with a comprehensive scaling analysis of compressor and predictor model family and sizes, we find that larger compressors are not only more accurate but also more concise, leading to sublinear FLOPs growth relative to model size. 
We conclude that scaling compressors is more effective than scaling predictors.
Building on this, we show that mutual information rate closely tracks downstream accuracy and perplexity, providing a task-agnostic signal of compression quality. 
A meta-analysis highlights compressor model family and compressor size outweighing predictor size as the most important factors. 
Finally, we validate these principles in a Deep Research pipeline, where local compressors deliver frontier-level accuracy at a fraction of the cost.
% Based on our results, we argue that: First, larger compressor LMs are more token-efficient; you can increase compressor model capacity while keeping compute cost roughly constant for certain LM families. Second, ``front-load" your intelligence and compute into local compressors to reduce predictor cloud costs. Third, scaling compressor LMs maximizes communication efficiency and the information bottleneck objective. Lastly, we demonstrate our proposed design principles based on a cost-effective Deep Research system with local compressor models achieving frontier-LM-only performance.

\paragraph*{Datasets} 
We study our setup on four datasets. (a) \textsc{LongHealth}, a set of synthetic clinical reports and $20$ patient histories \citep{adams2024longhealth}. We hide the multiple-choice options from the LMs to treat it as a question-answering (QA) dataset. (b) \textsc{FinanceBench}, a collection of $150$ 10-K filings and reports paired with QA tasks \citep{islam2023financebench}. (c) \textsc{WildChat} is a large-scale LM conversation dataset \citep{zhao2024wildchat}. (d) \textsc{FineWeb}, a dataset of processed web pages from CommonCrawl \citep{penedo2024the}. See Appendix~\ref{sec:appendix-dataset} for more details on datasets.

\paragraph*{Evaluation procedure} We run each experiment with $S=5$ random seeds. We evaluate prediction quality using accuracy for \textsc{LongHealth} and \textsc{FinanceBench}, assessing correctness of the predictions against the ground-truth using a \textsc{GPT-4o-mini} judge. We use perplexity for \textsc{WildChat} and \textsc{FineWeb}, evaluating the log probabilities of a \textsc{Llama-3.1-8B} model.
%For clarity, we focus on the \textsc{LongHealth} and \textsc{FinanceBench} datasets in the remainder of the main text. Unless stated otherwise, scaling trends can be assumed to hold across both benchmarks. Similar experiments are conducted on \textsc{Wildchat} (Appendix~\ref{sec:wildchat-results-appendix}) and \textsc{FineWeb} (Appendix~\ref{sec:fineweb-results-appendix}).

\paragraph*{Compressor model} 
%\textbf{Compressor model} 
As compressors, we use smaller open-source LMs of the model families \textsc{Llama-3} \citep{grattafiori2024llama3herdmodels}, \textsc{Qwen-2.5} \citep{qwen2025qwen25technicalreport}, and \textsc{Gemma-3} \citep{team2025gemma}. For simplicity, we choose \textsc{GPT}-style models, and postpone analysis of reasoning models for future work (which will require a breakdown of reasoning and answer tokens).
%In all plots, data points of the compressor model family \textsc{Qwen-2.5} appear in a shade of blue, and data points of the compressor model family \textsc{Llama3} in a shade of pink. 
See Appendix~\ref{sec:appendix-compressor-model} for further details.

\paragraph*{Predictor model} 
%\textbf{Predictor model} 
We evaluate larger frontier models \textsc{GPT-4o} \citep{openai2024gpt4ocard} as well as LMs of the \textsc{Llama-3} (1B, 8B, 70B, 405B) and \textsc{Qwen-2.5} (72B) families as predictors. %See Appendix~\ref{sec:appendix-supervisor-model} for further details.

%\paragraph*{Compute cost of MoE compressor/predictor LMs}\label{sec:moe-compute}
%For a mixture-of-experts (MoE) transformer LM with \(n_{\mathrm{MoE}}\) expert layers, \(E\) experts per layer, and a top-\(k\) router, the forward-pass FLOPs consist of FLOPs for dense and expert feed-forward components, self-attention, and expert routing:
%\begin{align*}
%    C_{\text{MoE}}
%        &\approx 2\left(N_{\mathrm{dense}} + k\,n_{\mathrm{MoE}}\,N_{\mathrm{exp}}\right)
%        \;+\; 2\,n_{\mathrm{layer}}\,n_{\mathrm{ctx}}\,d_{\mathrm{attn}}
%        \;+\;
%        2\,n_{\mathrm{MoE}}\,E\,d_{\mathrm{model}} \\
%        &\approx 2N_{\mathrm{active}}
%        \;+\; 2\,n_{\mathrm{layer}}\,n_{\mathrm{ctx}}\,d_{\mathrm{attn}}
%        \;+\;
%        2\,n_{\mathrm{MoE}}\,E\,d_{\mathrm{model}},
%\end{align*}
%where \(N_{\mathrm{dense}}\) is the number of dense layer parameters, \(N_{\mathrm{exp}}\) is the number of parameters in an expert feed-forward network, \(N_{\mathrm{active}}\) is the number of activated parameters, \(n_{\mathrm{layer}}\) is the number of layers, \(d_{\mathrm{attn}}\) is the number of attention heads, \(d_{\mathrm{model}}\) is the hidden dimension of the model, and \(n_{\mathrm{ctx}}\) the number of input context tokens. We see that compute in MoE LMs primarily scales with the number of activated parameters. 

\subsection{Where Do You Need the FLOPs: In Compressors or Predictors?}
\label{sec:compressor-predictor-intelligence}

We ask: should we scale compressors, which distill large amount of information into concise summaries, or predictors, which reason over the provided summaries to solve complex tasks?

In the following section, we show that scaling the compressor LM yields more significant gains. We vary the compressor model size and study its effects on downstream accuracy, the length of the compressed summaries, and the number of overall FLOPs-per-generation. We also establish scaling laws linking compressor model size to downstream performance.

First, we examine question-answering (QA) accuracy when scaling both compressor and predictor model size.

%\paragraph*{Downstream performance is a function of compressor size}

%\paragraph*{Larger compressors yield higher accuracy.} 
\paragraph*{Downstream performance is a function of compressor size.} 
We illustrate how downstream accuracy increases as model size increases (Figure~\ref{fig:figure-3-properties}).
On \textsc{LongHealth}, 7B and 8B models are up to \(3.1\times\) more accurate than 1B models and surpass the \textsc{GPT-4o}-only baseline by $4$pp. On \textsc{FinanceBench}, 7B and 8B models are up to \(2.6\times\) more accurate than 1B models and are able to recover \(97\%\) of the \textsc{GPT-4o}-only baseline accuracy. The same scaling behavior holds for \textsc{Gemma-3} models.
%Across both datasets, the $14$B \textsc{Gemma-3} model is \(6.4\times\) (\textsc{LongHealth}) and \(5\times\) (\textsc{FinanceBench}) more accurate than its $1$B variation. 
% at the higher end of the lightweight model sizes. 

%\paragraph*{Scaling compressors yields more significant accuracy gains than scaling predictors.}
%We observe this for both \textsc{Qwen-2.5} and \textsc{Llama} compressor families.
% PRINCIPLE: invest in compressor scaling first to extract more task-relevant information from contexts
%We confirm this finding through a GLM analysis in Figure~\ref{fig:figure-10-glm}, where scaling compressor LM ($\text{Coeff}_1\approx 0.48$, $\text{Coeff}_2\approx 0.48$) matters more than scaling predictor LM ($\text{Coeff}_1\approx 0.12$, $\text{Coeff}_2\approx 0.12$).
\begin{figure}[ht]
    \centering
    %\includegraphics[width=0.7\linewidth]{figures/figure 3.pdf}
    \includegraphics[width=\linewidth]{figures/figure 3 horizontal.pdf}
    \caption{\textbf{Downstream accuracy, length of compression, and inference cost scale with compressor size.} The $y$-axis shows the \textbf{(Left)} accuracy, with the black dashed line showing the GPT-4o model baseline, \textbf{(Middle)} number of compression output tokens, \textbf{(Right)} GFLOPs-per-compression on: \textbf{(Top)} \textsc{LongHealth}, \textbf{(Bottom)} \textsc{FinanceBench}. Larger models are more token-efficient, trading off accuracy for compute. We see consistent scaling trends on \textsc{Wildchat} (Appendix~\ref{sec:wildchat-results-appendix}) and \textsc{FineWeb} (Appendix~\ref{sec:fineweb-results-appendix})}
    %We see consistent trends in perplexity, length of summary, and inference cost on \textsc{Wildchat} (Appendix~\ref{sec:wildchat-results-appendix}) and \textsc{FineWeb} (Appendix~\ref{sec:fineweb-results-appendix})}
    \label{fig:figure-3-properties}
\end{figure}

\paragraph*{Analysis of compressor errors.} 
Errors in the compression step can be characterized into one of three categories: (a) the compression contains an incorrect answer ($36.3\%$ of compressor errors); (b) the compression contains no answer ($33.3\%$ of compressor errors); and (c) the compression omits details or parts of the information necessary for the answer ($30.4\%$ of compressor errors).
For more details on compressor errors, refer to Appendix~\ref{sec:appendix-failure-modes-compression}.

\paragraph*{Larger compressors are more concise.}
%In addition to downstream accuracy, we argue that scaling compressors also reduces communication overhead by producing more concise summaries. 
%Communication cost is tied to compression output size, which is the number of tokens transmitted in each compressor-predictor interaction. 
In this section, we study the number of compression output tokens as a function of compressor size. We find that larger compressors are more concise (Figure~\ref{fig:figure-3-properties}) without sacrificing accuracy.
%\paragraph*{Larger compressors are more concise.}
%We observe consistent compression output size trends in Figure~\ref{fig:figure-3-properties}. 
%Intuitively, small models should tend to over-compress and omit fine-grained details. However, 
Specifically, 7--12B compressors are up to \(4.6\times\) more token-efficient than their 1--1.5B counterparts within the same model family. % on \textsc{LongHealth} and \textsc{FinanceBench}.
%Figure~\ref{fig:figure-3-properties} shows that as the size of the compressor LM model increases, the length of the compression output decreases, without sacrifice of QA performance (Section~\ref{sec:qa-performance}). 
\textsc{Qwen-2.5} models tend to be more concise than \textsc{Llama} and \textsc{Gemma-3}, suggesting that models can significantly vary in their communication profiles.

\paragraph*{Compression compute cost scales sublinearly with compressor size.}
We combine the number of parameters with output token counts to estimate FLOPs-per-generation for each model family and size, i.e., the \emph{actual} compute cost (Appendix~\ref{sec:appendix-compute-cost}). Because larger compressors generate fewer tokens while maintaining accuracy, FLOPs-per-generation scale sublinearly with model size. Different model families exhibit distinct scaling behaviors (Figure \ref{fig:figure-3-properties}): \textsc{Qwen-2.5} compressors can scale from 1.5B to 7B parameters with only a \(1.3\%\) increase in FLOPs-per-generation on \textsc{LongHealth}.

%The trends of compression output size give rise to the question, how does compressor model size affect the compute cost of each compression?
%\paragraph*{How does compressor model size affect the compute cost of each compression?}
%The inference cost per token for dense models grows roughly linearly with model size.%:
%\begin{align*}
%    C_{\text{forward}} &\approx 2N_{\text{params}} + 2n_{\text{layer}}n_{\text{ctx}}d_{\text{attn}}.
%\end{align*}
%The trends of the compression output size give rise to the question of compute cost per compression. 

%For \textsc{Llama} and \textsc{Gemma-3} compressor models, the increase in compute cost is more significant than for \textsc{Qwen-2.5}. 
% For \textsc{Qwen-2.5} compressors, we observe that model size can be increased to $7$B with only an \(1.3\%\) increase in FLOPs-per-generation on \textsc{LongHealth}.
%compute cost per compressor generation scales sublinearly (Figure \ref{fig:figure-3-properties}). The compute cost of \textsc{Qwen-2.5-7B} in FLOPs-per-generation is within \(1.3\%\) of the cost of \textsc{Qwen-2.5-1.5B} on \textsc{LongHealth} and within \(48.7\%\) on \textsc{FinanceBench}. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/figure 9.pdf}
    \caption{\textbf{QA Accuracy versus total compute cost on \textsc{LongHealth}.} y-axis shows accuracy and x-axis plots total compute cost in FLOPs-per-generation on a log-scale for \textbf{(Left)} \textsc{Qwen-2.5}, \textbf{(Right)} \textsc{Llama-3} compressor LMs. Markers indicate compressor sizes in the \textsc{Qwen-2.5} (1.5B, 3B, 7B, 14B) and \textsc{Llama-3} (1B, 3B, 8B) compressor model family; vertical and horizontal bars denote standard errors. Appendix~\ref{sec:financebench-mi-appendix} shows consistent trends on \textsc{FinanceBench}.}
    \label{fig:figure-9-totalcost}
\end{figure}

\paragraph*{Scaling compressors is more effective than scaling predictors.} Figure~\ref{fig:figure-9-totalcost} shows that scaling the predictor LM provides only marginal improvements in accuracy once a baseline predictor capacity ($\approx$ 8B--70B) is reached. The gains in accuracy by increasing predictor size from 70B to 405B are within $12\%$ (\textsc{LongHealth}) and $1\%$ (\textsc{FinanceBench}). In contrast, scaling compressor LM for both families leads to steeper increases in performance for fewer FLOPs-per-generation spent. For the \textsc{Qwen-2.5} compressor family, FLOPs-per-generation meaningfully increase only when transitioning from 7B to 14B (models up to 7B all have roughly constant FLOPs-per-generation). 

\paragraph*{You can trade local for remote compute.} 
%In real-world systems, available hardware and acceptable latency determine the compute budget available to a multi-LM system. 
As shown in Figure~\ref{fig:figure-0-compressors-matter} (right), powerful models up to 27B can run without aggressive quantization on current-generation laptops. We anticipate the trends to continue and that even bigger models could run locally, and for free. Our results motivate ``front-loading" FLOPs into local compressors to reduce cloud costs for serving the predictor (Figure~\ref{fig:figure-9-totalcost}).

%\subsection{You can trade local FLOPs for remote FLOPs} \label{sec:scale-total-compute}
%When designing asymmetric agentic systems, a trade-off commonly faced is investing into local compute resources versus expensive cloud-compute \citep{narayan2025minions}. Recognizing that local FLOPs $\neq$ remote FLOPs, we attempt to derive general guidelines about dividing compute budget between local and remote. 
%We examine the end-to-end compute cost of our system in Figure~\ref{fig:figure-9-totalcost}.
%\paragraph*{Trade local for remote compute.} 
%Assume a compressor LM running on local hardware and predictor LM running on remote hardware. Since compressor model capacity can be scaled for relatively little compute cost compared to the predictor, ``front-loading" FLOPs into local compressors can effectively reduce remote cost. Larger compressors running on-device reduce the need for massive predictors on cloud-servers. Specifically, small 8B predictor with a large 14B compressor LM achieves better performance than a 405B predictor with a 7B compressor at a fraction of the total FLOPs (Figure~\ref{fig:figure-9-totalcost}).

\subsection{Which Compressors Maximize Communication Efficiency?}
\label{sec:mi-be-scaling}

We want to select compressors that provide maximal task-relevant information, ideally communicated in as few tokens as possible. The downstream QA accuracy and compression length do not fully capture compression quality. Instead, we turn to our information-theoretic framework:
%To better quantify \emph{how much} of the full raw document's content is retained with regards to the question, we turn to our information-theoretic framework:
we estimate the mutual information \(I(X;Z \mid Q)\) between context $X$ and generated compression $Z$ conditioned on query $Q$ for each compressor model in our scaling analysis, using the Monte Carlo estimator described in Section~\ref{sec:mi-methods}.

We find that smaller LMs at 1--3B were often miscalibrated and could assign high likelihoods to nonsensical token sequences. Thus, we evaluated the log probabilities using a proxy model at the 7--8B scale. To mitigate biases, the proxy model was selected from a different family.

\paragraph*{Larger compressors retain more mutual information and are more bit efficient.}
%In Section~\ref{sec:qa-performance}, we find that downstream accuracy increases and compression size decreases as compressor model size increases. 
%We seek to quantify the quality of the compression with regards to the original input context using mutual information \(I(X;Z \mid Q)\). 
We observe that $I(X;Z \mid Q)$ increases as compressor size increases (Figure~\ref{fig:figure-4-mi}). Larger, more expressive compressor models carry more mutual information between the original document and the compression into the summary.

On \textsc{LongHealth}, while \textsc{Llama} compressors are far from the theoretical maximum, we find that \textsc{Qwen-2.5} and \textsc{Gemma-3} models produce compressions that saturate in mutual information at the largest model sizes. By contrast, on \textsc{FinanceBench}, mutual information saturates already at the 3B scale. We observe that this occurs primarily on datasets with a highly heterogeneous corpus of context documents.
%This is because the context documents are highly heterogeneous and distinct, which reduces the uncertainty between them and enables smaller compressors to capture key differentiating feature.

Combining the scaling effects of mutual information with the observation that larger compressors omit fewer tokens, we find that larger compressors are more bit efficient (Figure~\ref{fig:figure-4-mi}).

%\paragraph*{Larger compressors are more bit efficient.}
%Larger compressors achieve higher bit efficiency, retaining more information about the original document while compressing it into fewer tokens (Figure~\ref{fig:figure-4-mi}). This creates a favorable scaling dynamic where increased model capacity improves both information retention and communication efficiency simultaneously.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/figure 4.pdf}
    \caption{\textbf{Larger compressors have outputs that carry more information about their inputs (conditioned on the query) on \textsc{LongHealth}.} Larger compressor model sizes compress documents with higher \textbf{(Left)} mutual information, \textbf{(Right)} bit efficiency (bits of mutual information per token; higher is better). The black dotted line represents the theoretical maximum of mutual information at the natural logarithm $\log(N)$, where $N$ is the number of documents mutual information is computed across. We find consistent trends on \textsc{FinanceBench} (Appendix~\ref{sec:financebench-mi-appendix})}
    \label{fig:figure-4-mi}
\end{figure}

\paragraph*{Compressor scaling effects are consistent across prompt conditions.}
A natural concern is whether our scaling results depend on specific prompt formatting. To test robustness, we instructed compressor models to output 3, 6, or 9 sentences, varying conciseness levels. Scaling behavior in accuracy, compression output size, FLOPs-per-generation, MI, and bit efficiency remained consistent across all conciseness instructions on both \textsc{LongHealth} and \textsc{FinanceBench} (Figures \ref{fig:figure-5-prompting}, \ref{fig:figure-5-appendix}). The relative improvements from larger compressors persist regardless of prompted compression output length, confirming that model capacity drives these efficiency gains.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/figure 5.pdf}
    \caption{\textbf{Scaling behaviors of compressor model size hold across instructed conciseness (\textsc{Compressor = Qwen-2.5}).}
    \textbf{(Left)} Accuracy \textbf{(Middle)} GFLOPs-per-generation \textbf{(Right)} MI versus compressor model size. 
    Accuracy and MI are largely unaffected by conciseness instructions. Compressors instructed to be more concise are more token-efficient, and thus compute-efficient. Trends in accuracy, compute cost, and MI as we scale compressor hold across conciseness constraints. Appendix~\ref{sec:financebench-prompting-appendix} shows analogous results on \textsc{FinanceBench}.}
    \label{fig:figure-5-prompting}
\end{figure}

\subsection{Information Rate Correlates Strongly with Downstream Performance}
\label{sec:rate-distortion-correlation}

\paragraph*{Mutual information and bit-efficiency are proxies for system performance.} \noindent Information rate (bit-efficiency) is closely related to distortion (1 $-$ accuracy). Motivated by the classical form of the rate-distortion function for a independent Gaussian source $X$ (Appendix~\ref{sec:appendix-rate-distortion}), we fit decaying exponential functions to the rate-distortion data. This fit characterizes the correlation between information rate and distortion and corroborates our previous finding that scaling predictors beyond 70B yield only marginal improvements in distortion (Figure~\ref{fig:figure-1000-mi-correlation}).
%To characterize this relationship, we model the rate-distortion curve as a decaying exponential function (Appendix~\ref{sec:appendix-rate-distortion}), motivated by the classical form of the rate-distortion function under the assumption that $X$ is an independent Gaussian random variable.
%Fitting decaying exponential functions to the rate-distortion data points, we confirm that beyond a baseline predictor size of $70$B, further Th in predictor size yield only marginal reductions in distortion (Figure~\ref{fig:figure-1000-mi-correlation}).
%We find that information rate (bit-efficiency) is strongly related to distortion (1 - accuracy). We fit the two using a decaying exponential function \textcolor{red}{TODO: explain why} (Appendix~\ref{sec:appendix-rate-distortion}).

Furthermore, we find in Figure~\ref{fig:figure-1000-mi-correlation} that mutual information is also strongly correlated with perplexity ($r=-0.84$, $R^2 = 0.71$) for extractive tasks on \textsc{FineWeb} (setup detailed in Appendix~\ref{sec:appendix-fineweb}).

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/figure -1.pdf}
    \caption{\textbf{Mutual information and bit efficiency correlate strongly with downstream performance.} \textbf{(Left)} Rate-distortion analysis across predictor sizes 1B, 8B, 70B, and 405B on \textsc{LongHealth}. Dotted lines show fitted exponential-decay functions. \textbf{(Right)} Perplexity versus mutual information on \textsc{FineWeb}. Black line shows a fitted linear function ($r=-0.84$, $R^2 = 0.71$). Black dotted line represents theoretical maximum of MI.
    Vertical and horizontal bars denote standard errors.
    }
    \label{fig:figure-1000-mi-correlation}
\end{figure}

\paragraph*{Predictors do not prefer compressors of the same family.} 
Further rate-distortion analysis across \textsc{Qwen-2.5} and \textsc{Llama} models reveal that distortion is primarily dependent on model family and size. Crucially, predictors do not perform better when paired with compressors of the same family (Figure~\ref{fig:figure-6/7-combined}). 
%We pair \textsc{Qwen-2.5} and \textsc{Llama} compressor models with a \textsc{Qwen-2.5-72B} and a \textsc{Llama-3.1-70B} predictor. Our results in Figure~\ref{fig:figure-6/7-combined} reveal that predictors have no preference for compressors of the same model family. More noticeably, we find that the \textsc{Qwen-2.5} family as both compressor and predictor display lower distortion. 

%\subsubsection{Choice of compressor-predictor model size pairing}
%\label{sec:compressor-predictor-scaling}
%We ask, \textbf{how does the predictor's capacity to decode impact downstream QA performance?} We find that as we scale predictor model size, the information bottleneck slowly shifts from the decompression to the compression. In other words, the benefits of scaling compressor size from 7B to 14B are more significant for larger predictors (Figure~\ref{fig:figure-9-totalcost}), as the compression output becomes the primary bottleneck. \textcolor{red}{(we need a principled takeaway here)}

\subsection{Which Knobs to Turn?}
\label{sec:glm-analysis}

To guide practical system design, we analyze which components of the compression-prediction pipeline most strongly drive downstream QA accuracy. We fit a logistic regression predicting binary correctness on \textsc{LongHealth} and \textsc{FinanceBench} using the features specified in Appendix~\ref{sec:appendix-glm-analysis}. The compressors we consider are \textsc{Qwen-2.5} and \textsc{Llama} models and predictors are \textsc{Llama} models of sizes 1B, 8B, 70B, and 405B.

We find that compressor model family is the most important factor (Figure~\ref{fig:figure-10-glm}) with \textsc{Qwen-2.5} compressors outperforming \textsc{Llama}. Additionally, scaling the compressor LM matters substantially more than scaling the predictor LM, confirming previous findings in Section~\ref{sec:compressor-predictor-intelligence}.

% \textbf{\textsc{Qwen-2.5} compressors outperform \textsc{Llama}.} Compressor model family is the most important factor across both datasets (Figure~\ref{fig:figure-10-glm}).
% \textbf{Compressor size $>$ predictor size.}
% We confirm that scaling compressor LM matters substantially more than scaling predictor LM across both datasets. 

% \begin{itemize}[leftmargin=0.2in]
%     \item Z-score normalized lengths of the input document, prediction output, and compression output,
%     \item Z-score normalized predictor and compressor model size,
%     \item Indicator $\mathbf{1}\{\text{Compressor} = \text{Qwen}\}$ for the compressor model family,
% \end{itemize}

% where compressors are \textsc{Qwen-2.5} and \textsc{Llama} models and predictors are \textsc{Llama} models of sizes 1B, 8B, 70B, and 405B. Full experimental details are in Appendix~\ref{sec:appendix-glm-analysis}.

%\paragraph*{More tokens hinder accuracy}
%Our results in Figure~\ref{fig:figure-10-glm} reveal that longer contexts ($\text{Coeff}_1\approx -0.75$, $\text{Coeff}_2\approx -0.07$), prediction output ($\text{Coeff}_1\approx -0.56$, $\text{Coeff}_2\approx -0.33$), and compression output ($\text{Coeff}_1\approx -0.03$, $\text{Coeff}_2\approx -0.05$) hinder QA success. 
%Our results in Figure~\ref{fig:figure-10-glm} reveal that longer contexts, prediction output, and compression output hinder QA success. 


%The binary indicator for \textsc{Qwen} family compressors ($\text{Coeff}_1\approx 0.51$, $\text{Coeff}_2\approx 0.53$) has the largest coefficient magnitude. 
%The binary indicator compressor family has the largest coefficient. 


%\textbf{More tokens hinder accuracy.}
%Our analysis reveals that longer contexts, prediction output, and compression output hinder QA success. Longer compressions produced by smaller compressors, however minor, compound to have a negative effect on prediction accuracy. % ($\text{Coeff}_1\approx -0.06$, $\text{Coeff}_2\approx -0.06$). 

\subsection{Scaling Deep Research}
\label{sec:scaling-deepresearch}
We evaluate our compression-prediction framework on open-domain ``Deep Research" workflows, where a predictor LM decomposes research tasks into subtasks and aggregates compressor outputs into final reports (Figure~\ref{fig:deepres_setup}). We use \textsc{DeepResearch Bench} \citep{du2025deepresearch}, which assesses system performance across four dimensions: Comprehensiveness, Depth, Instruction-following, and Readability. These four dimensions form a quantitative \textit{RACE} (Reference-based Adaptive Criteria Evaluation) score. We vary predictor sizes across the \textsc{Llama} family and compressor sizes across the \textsc{Qwen-2.5} family. Full experimental details are in Appendix~\ref{sec:appendix-deepresearch-setup}.

Larger predictor models consistently improve \textit{RACE} scores, while larger compressors provide substantial performance gains at minimal additional API costs (Figure~\ref{fig:deepres_main}).

As a baseline, we evaluate the results of providing uncompressed web search data to a \textsc{GPT-4o} predictor. A 14B \textsc{Qwen-2.5} compressor paired with a \textsc{GPT-4o} predictor achieves $2\%$ higher RACE scores at only 30\% of the API cost compared to the uncompressed baseline. We detail further findings in our scaling experiments in Appendix~\ref{sec:appendix-deep-research-analysis}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/deepres_main.pdf}
    \caption{\textbf{Deep Research Scaling Results. (Left)
    } RACE score versus average task cost when using \textsc{GPT-4o} as a predictor with \textsc{Qwen-2.5} compressors of varying sizes. Costs are based on \textsc{GPT-4o} API rates (Aug 2025: \$2.50/1M input tokens, \$10.00/1M output tokens). Larger compressors improve performance with minimal cost increases. For reference, we include \textsc{GPT-4o} results without compression and also for the \textsc{GPT-4o-search-preview} model.
    \textbf{(Right)} RACE scores for different \textsc{Qwen-2.5} compressor sizes (0.5B–14B) under three \textsc{Llama} predictors (8B, 70B, 405B). 
    %Larger predictors consistently yield higher scores across compressor scales.
    }
    \label{fig:deepres_main}
\end{figure}

\section{Discussion}
\label{sec:discussion}
We establish an information-theoretic framework for compressor-predictor systems to determine how model selection and scaling affect compute and communication efficiency. Our findings come with important limitations: our MI estimator relies on proxy models and log probabilities, introducing potential variance and biases. Furthermore, we focus on \textsc{GPT}-style non-reasoning models with single-round communication, limiting generalizability to reasoning-augmented models or iterative multi-agent workflows.

Several research directions warrant investigation. Mutual information estimation for LM outputs remains challenging, though alternative estimators like \textsc{InfoNCE} \citep{aitchison2021infonce} offer promising solutions. Information-theoretic principles could guide compressor routing strategies and fallback decisions for remote full-context processing. Training objectives based on rate-distortion analysis represent another avenue for optimizing compressor-predictor communication. Finally, mixture-of-experts (MoE) models \citep{fedus2022switch} may exhibit different scaling behaviors since their compute cost depends on activated experts rather than total parameter count.

Overall, we distill our findings into four principles for agentic system design:

\begin{exampleboxprinciples}[Principles for Agentic System Design]
\small
\begin{itemize}[leftmargin=*]
    \item \textbf{Compressors can be scaled at a sublinear computational cost.} Larger compressor LMs are more accurate, concise, and information-efficient. Since larger models are more concise, FLOPs-per-generation scale sublinearly as a function of model size.
    \item \textbf{``Front-load" compute into local compressors to reduce remote costs.} Scaling compressors is more effective than scaling predictors. By running larger compressors on-device, we can reduce predictor serving costs on the cloud.
    \item \textbf{Optimize for information density.} Mutual information serves as a task-agnostic indicator of compression quality and is tightly linked to downstream performance and perplexity. 
    \item \textbf{Expect model family to differ in scaling trends.} Choice of compressor and predictor model family yields offsets in rate-distortion curves and scaling effects. \textsc{Qwen-2.5} compressors scale more compute-efficiently than \textsc{Llama} and \textsc{Gemma-3}. \textsc{Qwen-2.5} predictors yield higher accuracies than \textsc{Llama}.
\end{itemize}
\end{exampleboxprinciples}

%By placing compressor and predictor model scaling in an information-theoretic framing, we hope to provide a first step toward principled design of agentic systems. 
%In future works, we hope to serve as a foundation for more principled designs and discussions of agentic architectures that balance compute and communication efficiency. 

% As agentic pipelines become more complex, intentional principle-based design will become increasingly important. Our work seeks to establish principles on how the selection and scaling of compressor and predictor models affect compute and communication efficiency. 
%We model each compressor-predictor interaction as an information bottleneck, which allows us to evaluate empirical scaling laws as approximations of classic information-theory.

%Across model families and datasets, we find consistent scaling laws in asymmetric agentic systems. Larger compressors (a) achieve higher downstream accuracy, (b) produce shorter, more concise summaries, and (c) transmit more mutual information per token. We further observe that scaling compressor LMs improves accuracy–cost tradeoffs more effectively than scaling predictor LMs. In some compressor families (e.g., Qwen-2.5), these gains come at nearly constant compute cost, showing that larger compressors can trade computation for communication efficiency.

\newpage
We utilized AI tools to assist with code implementation and manuscript proofreading.

\section*{Ethics statement}
Regarding fairness and accessibility, our recommendation to ``front-load" computation into local compressors may create barriers for researchers with limited hardware resources, potentially exacerbating inequalities in AI access despite reducing cloud API costs by 74\%. The compression techniques process documents through multiple model stages, raising privacy concerns about information retention in compressed representations, especially when handling sensitive data. Our efficiency improvements could accelerate broader deployment of agentic systems with both beneficial and harmful applications, while the environmental impact of encouraging larger local model deployment (up to 27B parameters) requires careful consideration against potential increases in aggregate energy consumption. We encourage practitioners to implement appropriate privacy safeguards and consider the dual-use implications of these compression-prediction architectures as they become more prevalent.

\section*{Reproducibility statement}
We provide comprehensive implementation details and experimental specifications throughout the paper and appendices. Section~\ref{sec:info-theoretic-setup} contains the complete derivation and implementation of our mutual information estimator, while Appendix~\ref{sec:appendix-compute-cost} details the FLOPs computation methodology for dense transformer models. All experimental configurations, including model selections, hyperparameters, and prompt templates, are specified in Appendices~\ref{appendix-methods} and \ref{sec:appendix-exp-setup}. Dataset construction procedures are documented in Appendix~\ref{sec:appendix-dataset}, with specific sampling criteria for each of the four datasets, as well as prompt templates to construct synthetic QA and generation tasks. All models used to generate synthetic tasks and answers are detailed in Appendix~\ref{sec:appendix-dataset}. The Deep Research experimental setup is fully described in Appendix~\ref{sec:appendix-deepresearch-setup}, including the complete workflow implementation and evaluation framework, as well as prompt templates for compressor and predictor LMs. All experiments run on $S=5$ random seeds with reported standard errors, and we specify the exact model versions, inference parameters, and evaluation protocols used across all experiments. Rate-distortion analysis parameters and fitting procedures are detailed in Appendix~\ref{sec:appendix-rate-distortion}, while the generalized linear model specifications for meta-analysis are provided in Appendix~\ref{sec:appendix-compressor-model}.

\newpage
% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.

\newpage
\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\appendix
%\section{Appendix}
\newpage

\section{Extended Related Work}
\label{sec:appendix-related-work}

%\textcolor{red}{I think we need a section about alternative estimators (MINE, InfoNCE) and how difficult it is to estimate MI in high continuous dimensions, but that deserves its own section IMO (perhaps it's better to move this into the main text)}

\paragraph*{Deep Research} Over the past year, large-scale Deep Research systems have been popularized and adopted by frontier labs in industry such as OpenAI, Anthropic, and xAI. These systems commonly have an asymmetric setup, where a high-capacity \textit{predictor} LM decomposes the user query into subtasks that are executed by \textit{compressor} models in parallel \citep{schluntz2025building}. The results of these subtasks are synthesized into one answer that is commonly presented to the user as a comprehensive \textit{research report}. In practice, compressor models can range from frontier models \citep{hadfield2025multiagent, schluntz2025building} to local small LMs \citep{narayan2025minions}. Recent works have been centered around establishing evaluation benchmarks to compare different agentic \textit{Deep Research} systems \citep{du2025deepresearch}. These benchmarks focus on measuring the output quality and downstream utility of generated research reports. 
%Our own experiments on DeepResearch Bench similarly evaluate end-to-end scaling of predictors and compressors. We view these results as a complement to our bottleneck analysis, showing how design principles extend into realistic multi-agent workflows.
% We focus on communication between agents, which carries different objectives as compared to textual output intended for human understanding.
%We build a basic understanding of the trade-off between communication rate and compute cost as agentic systems are scaled. 

\section{Extended Description of Methods}
\label{appendix-methods}

In this section, we provide a more in-depth explanation and derivation of our information-theoretic approach and problem setup.

\subsection{Compute Cost of Dense LMs}
\label{sec:appendix-compute-cost}
We measure compute cost of each compressor/predictor LM call by the number of FLOPs per token in each forward pass through our dense transformer-based LMs as
\begin{align*}
    C_{\mathrm{dense}} &\approx 2N_{\mathrm{params}} + 2n_{\mathrm{layer}}n_{\mathrm{ctx}}d_{\mathrm{attn}},
\end{align*}
with model size \(N_{\mathrm{params}}\), number of input context tokens \(n_{\mathrm{ctx}}\), number of layers \(n_{\mathrm{layer}}\), and number of attention heads per layer \(d_{\mathrm{attn}}\) \citep{scalinglaws}. We observe that FLOPs-per-token-generated for dense models grows roughly linearly with model size.

%\subsection{Lower Bound on $I(X;Y)$ via Fano's Inequality}
%\label{sec:appendix-mi-lowerbound}

%In addition to estimating $I(X;Z)$, we provide a lower bound on $I(X;Y)$ for categorical $Y$ variables (e.g., multiple-choice QA in \textsc{LongHealth}). Using Fano's inequality, we are able to connect downstream prediction accuracy to $I(X;Y)$ without the need for any LM logits.  

%Let $|\mathcal{Y}|$ denote the number of possible answer choices. For classification tasks, Fano’s inequality says that
%\begin{align*}
%    H(Y \mid X) &\leq H_b(e) + P(e) + \log(|\mathcal{Y}| - 1),
%\end{align*}
%where $P(e) = 1 - \text{accuracy}$ is the probability of error, and $H_b(e)$ is the binary entropy function:
%\begin{align*}
%    H_b(e) = -P(e) \log P(e) - (1 - P(e)) \log (1-P(e))
%\end{align*}
%Since
%\begin{align*}
%    I(X;Y) &= H(Y) - H(Y \mid X),
%\end{align*}
%we find the lower bound:
%\begin{align*}
%    I(X;Y) &\geq H(Y) - H_b(e) - P(e) \log(|\mathcal{Y}| - 1),
%\end{align*}
%If the ground-truth multiple-choice answers are uniformly distributed, we can approximate
%\begin{align*}
%    H(Y) \approx \log|\mathcal{Y}|.
%\end{align*}

%This estimator allows us to compute the lower bound to $I(X;Y)$ for categorical $Y$ without needing to access LM logits, offering a more practical approach to evaluating $I(X;Y)$ given only the error rate. 

\subsection{Rate-Distortion-Theory}
\label{sec:appendix-rate-distortion}

Assume $X$ to be an independent Gaussian random variable, we model the rate-distortion function as 
\begin{align*}
    R(D) \;=\; \begin{cases}
\displaystyle \tfrac{1}{2}\log\bigl( \tfrac{\sigma^{2}(X)}{D} \bigr), & 0 \le D \le \sigma^{2}(X)\\[6pt]
0, & D > \sigma^{2}(X).
\end{cases}
\end{align*}
We illustrate rate-distortion curves as distortion \(D\) (how much accuracy is lost in communication) versus rate \(R\) (how many bits spent encoding data). Inverting the expression for the rate-distortion function gives \citep{wiley2005ratedistortion}
\begin{align*}
    D_{Gaussian}(R) &= \sigma^{2}\,2^{-2R} \\
         &= \sigma^{2}\,e^{-\,2\ln 2\,R} \\
         &= C\,e^{-\beta R}, \qquad\text{with}\quad
C=\sigma^{2},\; \beta=2\ln(2).
\end{align*}
In practice, we treat \(C\) and \(\beta\) as function parameters to account for unknown variance and modeling noise. LM compression-prediction systems often exhibit a non-zero distortion floor (e.g.\ imperfect LM judge, label noise, predictor expressive power), which we account for through offset \(D_{0}\). \(D_{0}\) is a lower bound of the distortion in the system as rate (bit efficiency) increases,
\begin{align*}
    D(R) &= C\,e^{-\beta R} + D_0.
\end{align*}
We fit exponential decay functions to the rate-distortion curves based on the least-squares estimates \((\hat C,\hat\beta,\hat D_{0})\).


\section{Prompts}
\label{sec:appendix-prompts}

\subsection{Compressor Model Prompts}
\label{sec:appendix-prompts-compression}
We use the following prompt templates to compress the raw context documents on \textsc{LongHealth}, \textsc{FinanceBench}, \textsc{FineWeb}, and each chat conversation on \textsc{WildChat}:

\begin{examplebox}[Query-Specific Compression Prompt Template (LongHealth/FinanceBench/FineWeb)]
    \ttfamily
Summarize the following text to include ONLY information needed to answer the question.\\
Extract the key points relevant to the question.\\
DO NOT ANSWER THE QUESTION DIRECTLY.\\
\\
Question: \\
\{query\} \\
\\
Text: \\
\{text\}\\
\\
Your summary (make sure to include all important details / background information related to the *question*. **DO NOT ANSWER THE QUESTION**)
\end{examplebox}

\begin{examplebox}[Memory Construction/Compression Prompt Template (WildChat)]
    \ttfamily
You are a memory compression assistant, tasked with summarizing a chat conversation. \\
Produce a summary that preserves all details that could be useful as memory for a language model. DO NOT invent any information. \\
\\
CHAT: \\ 
\{conversation\}\\
\\
Your summary (Just plain text, no formatting.)
\end{examplebox}

\begin{examplebox}[Query-Agnostic Compression Prompt Template (FineWeb)]
    \ttfamily
Summarize the following text and produce a summary that preserves all details that could be needed to answer likely questions about the text. Do NOT invent facts.\\
\\
Do NOT answer any question; just summarize potential answer-bearing info. \\
\\
Text: \\
\{text\}\\
\\
Your summary (make sure to include all important details / background information related. Just plain text, no formatting.)
\end{examplebox}

\subsection{Predictor Model Prompts}
\label{sec:appendix-prompts-prediction}
We used the following prompt templates to answer extractive QA tasks on \textsc{LongHealth}, \textsc{FinanceBench}, \textsc{WildChat}, and \textsc{FineWeb}, and creative tasks on \textsc{FineWeb}.

compress the raw context documents on \textsc{LongHealth}, \textsc{FinanceBench}, \textsc{FineWeb}, and each chat conversation on \textsc{WildChat}:

\begin{examplebox}[LongHealth/FinanceBench Prediction Prompt Template]
    \ttfamily
Please answer the following question based on the provided summary.\\
Question:\\
\{query\}\\
\\
Summary:\\
\{summary\}\\
\\
Please respond in the following JSON format:
<briefly think about the information you have and the question you need to answer>\\
\\
\{\{ \\
\tab "explanation": "<brief explanation of the answer. explain how you arrived at the answer. 1-2 sentences>", \\
\tab "answer": "<your final answer>" \\
\}\}\\
\\
Your answer (YOU MUST ONLY RESPOND WITH THE JSON OBJECT):
\end{examplebox}

\begin{examplebox}[WildChat Prediction Prompt Template]
    \ttfamily
Please answer the following question based on the provided chat memory.\\
\\
Question:\\
\{query\}\\
\\
Memory:\\
\{memory\}\\
\\
Please respond in the following JSON format:
<briefly think about the information you have and the question you need to answer>\\
\\
\{\{\\
\tab "answer": "<your final answer>"
\}\}\\
\\
Your answer (YOU MUST ONLY RESPOND WITH THE JSON OBJECT):
\end{examplebox}

\begin{examplebox}[FineWeb Prediction Prompt Template (Extractive)]
    \ttfamily
Please answer the following question based on the provided chat memory.\\
\\
Question:\\
\{query\}\\
\\
Chat Memory:\\
\{summary\}\\
\\
Please respond in the following JSON format:
<briefly think about the information you have and the question you need to answer>\\
\\
\{\{\\
\tab "answer": "<your final answer>" \\
\}\} \\
\\
Your answer (YOU MUST ONLY RESPOND WITH THE JSON OBJECT):
\end{examplebox}

\begin{examplebox}[FineWeb Prediction Prompt Template (Creative)]
    \ttfamily
Please do the following based on the provided chat memory.\\
\\
Task:
\{query\}\\
\\
Chat Memory:\\
\{summary\}\\
\\
Please respond in the following JSON format:
<briefly think about the information you have and the question you need to answer>\\
\\
\{\{\\
\tab "answer": "<your final answer>"\\
\}\}\\
\\
Your answer (YOU MUST ONLY RESPOND WITH THE JSON OBJECT):
\end{examplebox}

\subsection{DeepResearch Prompts}
The following prompt templates were used sequentially as the backbone for our compressor-predictor Deep Research workflow. We indicate in parentheses if the prompt was used for the predictor LM or compressor LM.

\begin{examplebox}[DeepResearch Query Generation Prompt Template (Predictor)]
\ttfamily
You are a research supervisor tasked with comprehensively exploring a research topic. Use a strategic, top-down approach to design your research.\\ \\ Research Topic: \{query\}\\ \\ **PHASE 1: RESEARCH PLANNING**\\ First, analyze this research topic and create a comprehensive research plan. Consider:\\ - What are the key areas that must be investigated to fully understand this topic?\\ - What specific objectives will guide your research?\\ - How do different aspects of this topic relate to each other?\\ - What types of information will be most valuable for a complete analysis?\\ - What is the logical flow for presenting findings?\\
\\
**PHASE 2: STRATEGIC QUERY GENERATION**\\
Based on your research plan, generate EXACTLY 8 different search queries that together will provide comprehensive coverage of this topic. Each query should serve a specific strategic purpose in your overall research architecture.\\ \\ For each search query, provide a specific sub-task/question that explains how it serves your research plan.\\
\\
Return your response in this exact JSON format:\\
\{\{ \\
\mlblock[2em]{"research\_plan": "Your comprehensive research architecture and strategic objectives for investigating this topic. Explain the key areas to investigate, how they relate, and the logical structure for analysis.",}

\mlblock[2em]{"queries": [}
\mlblock[4em]{
\{\{ \\
\tab "search\_query": "specific search terms optimized for Google",\\
\tab "sub\_task": "What specific question does this query address and how does it serve the research \tab plan?"\\
\}\},}

\mlblock[4em]{
\{\{ \\
\tab "search\_query": "second strategic search query",\\
\tab "sub\_task": "What does this query aim to discover and how does it fit the research \tab architecture?"\\
\}\},}

\mlblock[4em]{
\{\{ \\
\tab "search\_query": "third targeted search query",\\
\tab "sub\_task": "What aspect does this explore and why is it essential to the research plan?"\\
\}\},}

\mlblock[4em]{
\{\{ \\
\tab "search\_query": "fourth strategic search query",\\
\tab "sub\_task": "What question does this answer and how does it complement other queries?"\\
\}\},}

\mlblock[4em]{
\{\{ \\
\tab "search\_query": "fifth focused search query",\\
\tab "sub\_task": "What aspect does this cover and how does it build on previous queries?"\\
\}\},}

\mlblock[4em]{
\{\{ \\
\tab "search\_query": "sixth comprehensive search query",\\
\tab "sub\_task": "What additional dimension does this explore and why is it crucial?"\\
\}\},}

\mlblock[4em]{
\{\{ \\
\tab "search\_query": "seventh strategic search query",\\
\tab "sub\_task": "What specific gap does this fill in the research architecture?"\\
\}\},}

\mlblock[4em]{
\{\{ \\
\tab "search\_query": "eighth concluding search query",\\
\tab "sub\_task": "What final aspect does this cover and how does it complete the comprehensive \tab research?"\\
\}\}}
\mlblock[2em]{],} \\

\mlblock[2em]{"synthesis\_strategy": "Detailed strategy for combining findings from all 8 queries based on your research plan. Explain how the information will be structured, what relationships will be highlighted, and how the final analysis will be organized to maximize comprehensiveness and insight."} 
\}\} \\
\\
**Strategic Guidelines:**\\
1. Each search query should be 3-8 well-chosen keywords targeted for your specific research objectives\\
2. Design queries to serve complementary roles in your research architecture (not just generic dimensions)\\
3. Ensure queries are strategically coordinated to provide comprehensive topic coverage\\
4. Each sub-task should explain how the query serves your overall research plan\\
5. Create a synthesis strategy that reflects your planned research structure\\ \\
**Research Focus Areas to Consider:**\\
- Foundational understanding and current state\\
- Key challenges, problems, or limitations\\
- Solutions, methodologies, and best practices\\
- Evidence, data, and empirical findings\\
- Future trends, developments, and implications\\
- Multiple perspectives and stakeholder viewpoints\\
\\
CRITICAL: You must return ONLY the JSON object. Do NOT format it as a code block with ```json``` or any other markdown formatting. Return the raw JSON object directly. \end{examplebox}

\begin{examplebox}[DeepResearch Source Summarization Prompt Template (Compressor)]
    \ttfamily
Your job is to extract detailed, specific information from the following content to support comprehensive research analysis.\\
\\
**Main Research Query:** \{query\} \\

**Specific Sub-task/Question:** \{sub\_task\}\\
\\
\#\# Content\\
\{content\}\\
\\
**EXTRACTION REQUIREMENTS: Provide a detailed and comprehensive extraction that captures:**\\
\\
**Factual Information:**\\
- Specific numbers, statistics, percentages, and quantitative data\\
- Dates, timelines, and chronological information\\
- Names of people, organizations, companies, and institutions\\
- Geographic locations, regions, and jurisdictions\\
- Technical specifications, measurements, and benchmarks\\
\\
**Detailed Examples and Evidence:**\\
- Concrete case studies and real-world examples\\
- Specific research findings and study results\\
- Direct quotes and expert opinions\\
- Policy details and regulatory information\\
- Implementation details and methodologies\\
\\
**Comprehensive Coverage:**\\
- Key facts directly relevant to both the main query AND the specific sub-task\\
- Important concepts, definitions, and explanations\\
- Cause-and-effect relationships and underlying mechanisms\\
- Trends, patterns, and developments over time\\
- Challenges, limitations, and problem areas identified\\
\\
**Analytical Insights:**\\
- Implications and significance of the information\\
- Relationships between different data points\\
- Comparative information and benchmarks\\
- Future projections and forecasted trends\\
- Expert assessments and professional evaluations\\
\\
Focus on depth and specificity while maintaining clarity. Extract comprehensive, specific information with extensive detail, numbers, examples, and evidence. Do not provide brief summaries - ensure your extraction is thorough and substantial. Extract information that would be valuable for creating a comprehensive research report. Pay special attention to information that directly addresses the sub-task question.\\
\\
Return your extraction in JSON format with these fields:\\
- "explanation": Your detailed extraction of specific information, facts, data, examples, and evidence with extensive detail\\
- "answer": "relevant" if this content contains information relevant to the query and sub-task, "not relevant" otherwise\\
\\
CRITICAL JSON FORMATTING RULES:\\
- Replace all double quotes (") inside text with single quotes (')\\
- Replace all newlines with spaces\\
- Ensure the JSON is valid and parseable\\
- Do NOT use line breaks within the JSON fields\\
\\
Example format: \\
\{\{"explanation": "Your detailed extraction with specific facts, numbers, examples, and evidence using single quotes for any nested quotes", "answer": "relevant"\}\}\\
\\
CRITICAL: You must return ONLY the JSON object. Do NOT format it as a code block with ```json``` or any other markdown formatting. Return the raw JSON object directly.
\end{examplebox}

\begin{examplebox}[DeepResearch Synthesis Prompt Template (Predictor)]
    \ttfamily
You are tasked with creating a comprehensive, high-quality research report for a DeepResearch task. You have extensive research findings below - use ALL of them to create a detailed, thorough analysis.\\
\\
**Original Research Task:** \{original\_task\}\\
\\
**Research Plan:** \{research\_plan\}\\
\\
**Research Findings:**\\
\{qa\_pairs\}\\
\\
**Synthesis Strategy:** {synthesis\_strategy}\\
\\
**COMPREHENSIVE INFORMATION UTILIZATION - ALL SOURCES REQUIRED:**\\
You must systematically work through ALL the provided research findings above. Do not selectively use only some information - your report must demonstrate that you have reviewed and integrated ALL relevant details, data points, examples, and perspectives from every query and source provided.\\
\\
**REPORT STRUCTURE AND REQUIREMENTS:**\\
1. **Detailed Background Context** - Provide extensive background and context\\
2. **Comprehensive Analysis** - Multiple detailed sections covering all aspects\\
3. **Extensive Evidence Integration** - Use specific examples, data, quotes from ALL sources\\
4. **Thorough Implications Discussion** - Detailed analysis of implications and significance\\
5. **Complete Conclusions** - Comprehensive conclusions and future research directions\\
\\
**WRITING REQUIREMENTS FOR HIGH QUALITY:**\\
- Write detailed explanations, not brief summaries\\
- Include extensive examples and case studies from the research\\
- Provide comprehensive background and context for every major point\\
- Use all statistical data, quotes, and specific details from the research findings\\
- Elaborate on implications, significance, and broader connections\\
- Include detailed analysis of methodologies, approaches, and frameworks mentioned\\
- Discuss limitations, challenges, and areas for further research extensively\\
\\
Create a thorough academic research report that:\\
- Uses extensive detail and comprehensive analysis throughout\\
- Integrates ALL findings with detailed explanations and context\\
- Provides comprehensive coverage with extensive supporting evidence\\
- Includes detailed discussion of all relevant aspects and implications\\
- Demonstrates mastery of the subject through thorough, detailed analysis\\
\\
**FINAL REQUIREMENT:**\\
Your response must be substantial and comprehensive. Write extensively with exhaustive detail, comprehensive analysis, and complete utilization of all research findings. Provide truly comprehensive coverage of the topic that demonstrates thorough understanding and integration of all available research.
\end{examplebox}

\section{Extended Experimental Setup}
\label{sec:appendix-exp-setup}

Here, we further explain the construction of our datasets, choice of compressor and predictor models, and Deep Research experimental setup.

\subsection{Datasets}
\label{sec:appendix-dataset}

\subsubsection{LongHealth}
\label{sec:appendix-longhealth} 
\textsc{LongHealth} is a QA benchmark composed of $20$ patient cases and clinical documents. Each of the $20$ patients has a set of $20$ multiple-choice questions about their personal records each ranging from 5{,}090 to 6{,}754 words \citep{adams2024longhealth}. The original \textsc{LongHealth} benchmark is a multiple-choice QA task. To more closely mirror our QA setups in the remaining three datasets, we remove the multiple-choice options in the prediction step. We subsample $N=20$ documents and queries and generate $M=20$ compressions for each of the problem contexts.

\subsubsection{FinanceBench}
\label{sec:appendix-financebench}
\textsc{FinanceBench} is a long-context QA benchmark on $150$ financial reports. Each financial report ranges from 1{,}923 to 517{,}224 tokens, with an average length of 119{,}968 tokens \citep{islam2023financebench}. We filter the original \textsc{FinanceBench} dataset to only include samples with answer evidence at one location in the text. We slice a text segment of 21{,}500 tokens centered around the evidence as the raw document context. We subsample $N=20$ problems and generate $M=20$ compressions for each of the problem contexts.

\subsubsection{WildChat}
\label{sec:appendix-wildchat} 

Our motivation in constructing a chat memory dataset is to simulate real-world memory systems that require models to integrate information across multiple previous interactions. Queries could build upon multiple previous exchanges, or individual isolated chats. In the original \textsc{WildChat} dataset consisting of 837{,}989 multi-turn ChatGPT chats, each chat conversation exists as a standalone sample. We subsample $D=1000$ chat conversations with between $4$ and $8$ turns to construct our dataset. The dataset construction process is as follows:
\begin{enumerate}[leftmargin=*]
    \item \textbf{User Construction:} We construct synthetic users by grouping $10$ chat samples to each user (total $N=100$ users).
    \item \textbf{QA Generation:} We format each of the $10$ chat conversations and provide \textsc{GPT-4o-mini} with all full chat conversations along with the QA prompt to generate a question unique to each user that has not appeared in its chat history. 
\end{enumerate}

\begin{examplebox}[QA Prompt Template]
    \ttfamily
You are a data generation assistant, tasked with building a benchmark that evaluates the memory capabilities of a language model. \\
You will be provided a list of previous chat conversations. Your goal is to generate a new synthetic query that has not appeared in previous chats, but nevertheless benefits from the information in previous chats.\\
\\
CHATS:\\
\{chats\} \\
\\
Generate a new synthetic query that has not appeared in previous chats, but nevertheless benefits from the information that has appeared in previous chats. \\
Do not generate a RAG query about existing data in the chats, but rather a new query that could leverage existing chat information as **memory**. \\
\\
Please respond in the following JSON format:
<briefly think about the information you have and the question you can generate from it> \\
\\
\{\{ \\
\tab "question": "<question>", \\
\tab "answer": "<answer>", \\
\}\} \\
Your answer (YOU MUST ONLY RESPOND WITH THE JSON OBJECT):
\end{examplebox}

\subsubsection{FineWeb}
\label{sec:appendix-fineweb}
The \textsc{FineWeb} dataset contains an extensive set of web pages since 2013. At the time of writing, the dataset includes $25.9$ billion entries spanning from 2013 to 2025. To construct our subset of document and QA pairings, we collect $N=100$ samples with between 15{,}000 and 28{,}000 tokens, and ask \textsc{GPT-4o-mini} to synthetically generate $2$ extractive and $3$ creative QAs based on the cleaned web data and QA prompt.

\begin{examplebox}[QA Prompt Template]
    \ttfamily
You are generating synthetic question–answer (QA) pairs from a source text.
\\\\
SOURCE\_TEXT:\\
\{context\}
\\\\
Use only information from SOURCE\_TEXT. No hallucinated facts. \\
Generate five questions and answers: \\
- Question 1: What is \{\{topic\}\} and why is it important? (type = "qa") \\
- Question 2: What is \{\{topic\}\} and how does it work? (type = "qa") \\
- Question 3: Write an email to a colleague summarizing the findings and take-aways. (type = "generation") \\
- Question 4: Generate rap lyrics that teach the core concepts. (type = "generation") \\
- Question 5: Generate a poem about the topic. (type = "generation")
\\\\
Please respond in the following JSON format:
<briefly think about the information you have and questions you can generate from it>
\\\\
\{\{ \\
\tab "questions": [ \\
\tab \tab \{\{ \\
\tab \tab \tab"topic": "<topic 1>", \\
\tab \tab \tab"question": "<question 1>", \\
\tab \tab \tab"answer": "<answer 1>", \\
\tab \tab \tab"type": "qa" \\
\tab \tab \}\}, \\
\tab \tab \{\{ \\
\tab \tab \tab"topic": "<topic 2>", \\
\tab \tab \tab"question": "<question 2>", \\
\tab \tab \tab"answer": "<answer 2>", \\
\tab \tab \tab"type": "qa" \\
\tab \tab \}\}, \\
\tab \tab \{\{ \\
\tab \tab \tab"topic": "<topic 3>", \\
\tab \tab \tab"question": "<question 3>", \\
\tab \tab \tab"answer": "<answer 3>", \\
\tab \tab \tab"type": "generation" \\
\tab \tab \}\}, \\
\tab \tab \{\{ \\
\tab \tab \tab "topic": "<topic 4>", \\
\tab \tab \tab "question": "<question 4>", \\
\tab \tab \tab "answer": "<answer 4>", \\
\tab \tab \tab "type": "generation" \\
\tab \tab \}\}, \\
\tab \tab \{\{ \\
\tab \tab \tab "topic": "<topic 5>", \\
\tab \tab \tab "question": "<question 5>", \\
\tab \tab \tab "answer": "<answer 5>", \\
\tab \tab \tab "type": "generation" \\
\tab \tab \}\} \\
\tab ] \\
\}\}
\\\\
Your answer (YOU MUST ONLY RESPOND WITH THE JSON OBJECT):
\end{examplebox}



\subsection{Compressor Model Details}
\label{sec:appendix-compressor-model}

For the \textsc{Llama-3} family, we use the models \textsc{Llama-3.2-1B-Instruct}, \textsc{Llama-3.2-3B-Instruct}, \textsc{Llama-3.1-8B-Instruct}. For the \textsc{Qwen-2.5} family, we use the models \textsc{Qwen-2.5-1.5B-Instruct}, \textsc{Qwen-2.5-3B-Instruct}, \textsc{Qwen-2.5-7B-Instruct}. For the \textsc{Gemma-3} family, we use the models \textsc{Gemma-3-1B-IT}, \textsc{Gemma-3-4B-IT}, and \textsc{Gemma-3-12B-IT}. Additionally, we evaluate \textsc{Qwen-2.5-14B-Instruct} as compressor LM on \textsc{WildChat} and \textsc{FineWeb}. 

All three compressor model families are fine-tuned for instruction following. Compression outputs of at most $4096$ tokens are generated with temperature of $0.7$ for \textsc{Llama-3} and \textsc{Qwen-2.5}, and $1.0$ for \textsc{Gemma-3}. 

\subsection{Predictor Model Details}
\label{sec:appendix-supervisor-model}
As predictor models we use \textsc{GPT-4o}, \textsc{Llama-3.1-8B-Instruct}, \textsc{Llama-3.3-70B-Instruct}, and \textsc{Llama-3.1-405B-Instruct}. Predictor models generate with a temperature of $0.6$ across all benchmarks and experiments.

\subsection{Generalized Linear Model Analysis Setup}
\label{sec:appendix-glm-analysis}

We fit a logistic regression that predicts binary correctness of a compression-prediction output on: 
\begin{itemize}[leftmargin=*]
    \item Z-score normalized lengths of the input document, prediction output, and compression output,
    \item Z-score normalized predictor and compressor model size,
    \item Indicator \(\mathbf{1}\{\text{Compressor=Qwen}\}\) for the compressor model family,
\end{itemize}
where the predictors are \textsc{Llama-3} models of sizes 1B, 8B, 70B, and 405B.

%ä\subsection{Mutual information proxy model details}
%In practice, we approximate $p$ with a proxy model $q$, which is an LM of a different compressor family (\textsc{Qwen-2.5}/\textsc{Llama}/\textsc{Gemma-3}) than the generating model $p$ when evaluating mutual information \(I(X;Z \mid Q)\) for a compressor model. 
%We do this such that the proxy model does not prefer compressions generated by itself as a compressor model. For instance, \textsc{Llama-3.1-8B} is the proxy model when evaluating compressions generated by \textsc{Qwen-2.5} and \textsc{Gemma-3} models.


\subsection{Deep Research Setup Details}
\label{sec:appendix-deepresearch-setup}
For our experiments, we randomly sample $N=20$ English research tasks from the \textsc{DeepResearch Bench} test set to ensure a representative evaluation across diverse research domains. We conduct $5$ independent runs for each experimental configuration. This allows us to report mean performance with standard error bars, providing a robust assessment.
  
\subsubsection{Full Deep Research Workflow Setup}
\label{sec:appendix-deepresearch-workflow-setup}
In our Deep Research system setting, a predictor LM decomposes each research task into a collection of \emph{(Query, Subtask)} pairs. Each pair consists of a targeted web search query with a natural language instruction that specifies how the retrieved evidence should be analyzed. The predictor then distributes these pairs to compressor LMs, which independently perform the searches in parallel. Compressor LMs process the retrieved content according to the subtask, and compress the results into summaries. The predictor then aggregates these summaries into a comprehensive research report. This setup is illustrated in Figure~\ref{fig:deepres_setup}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/deepres_setup.png}
    \caption{%
    \textbf{Deep Research workflow.}
    A predictor LM decomposes a Deep Research task into \emph{(Query, Subtask)} pairs, where each pair specifies a targeted web search and an associated analysis instruction. Compressor LMs work in parallel to retrieve evidence, process it according to the subtask, and compress the findings into concise summaries, which the predictor then aggregates into a final report.}
    \label{fig:deepres_setup}
\end{figure}


We evaluate our system using the \textsc{DeepResearch Bench} framework \citep{du2025deepresearch}, which assesses agent performance through four dimensions: Comprehensiveness, Depth, Instruction-following, and Readability. More specifically, we use \textit{RACE} (Reference-based Adaptive Criteria Evaluation) scores to study the impact of model scale. The costs used in Figure~\ref{fig:deepres_main} are based on \textsc{GPT-4o} API rates (Aug 2025: \$2.50/1M input tokens, \$10.00/1M output tokens). In addition, there is a constant SerpAPI web search cost of \$0.12 for every task, which is not included in the figure.

\subsubsection{Deep Research Compressor Model Details}
\label{sec:appendix-deepresearch-compressors}
We employ the \textsc{Qwen-2.5-Instruct} family of models as compressor LMs, ranging from 0.5B to 14B parameters. These models are hosted on Modal Labs using the SGLang inference framework, enabling free, high-throughput parallel inference. All compressor models use a temperature of 0.7 and a maximum output token limit of 2,000 tokens per response. The specific compressor models used are: 
\begin{itemize}[leftmargin=0.2in]
    \item \textsc{Qwen-2.5-0.5B-Instruct}: Smallest model for minimal compression overhead
    \item \textsc{Qwen-2.5-1.5B-Instruct}: Balance between efficiency and capability
    \item \textsc{Qwen-2.5-3B-Instruct}: Mid-range compression quality
    \item \textsc{Qwen-2.5-7B-Instruct}: Strong comprehension with moderate compute
    \item \textsc{Qwen-2.5-14B-Instruct}: Highest quality compression in our experiments
\end{itemize}
Each compressor independently processes the search results for its assigned \emph{(Query, Subtask)} pair, extracting and compressing the relevant information according to the predictor's instructions. The compressed summaries from all compressors are then aggregated by the predictor into the final research report.

\subsubsection{Deep Research Predictor Model Details}
\label{sec:appendix-deepresearch-predictors}
We evaluate four predictor models spanning different scales and providers:

\begin{itemize}[leftmargin=0.2in]
    \item \textsc{Llama-3.1-8B-Instruct}: Entry-level predictor with basic task decomposition capabilities. Temperature set to 0.6, maximum output tokens of 4,000.
    \item \textsc{Llama-3.1-70B-Instruct}: Mid-tier predictor with improved reasoning and task planning. Temperature set to 0.6, maximum output tokens of 4,000.
    \item \textsc{Llama-3.1-405B-Instruct}: Large-scale predictor with advanced multi-step reasoning capabilities. Temperature set to 0.6, maximum output tokens of 4,000.
    \item \textsc{GPT-4o}: State-of-the-art commercial predictor serving as our performance upper bound. Temperature set to 0.6, maximum output tokens of 16,000 to accommodate comprehensive report generation.
\end{itemize}

All predictors use a slightly lower temperature (0.6) compared to compressors to ensure more consistent and structured task decomposition and report synthesis. The predictor is responsible for: (1) decomposing the research question into targeted queries, (2) formulating specific subtasks for each query, (3) distributing work to the compressor pool, and (4) synthesizing compressor outputs into a coherent final report.

A notable limitation is that the Llama family predictors are constrained to 4,000 output tokens, which can limit the comprehensiveness of their final research reports compared to GPT-4o's 16,000 token capacity. This constraint particularly affects the synthesis phase where the predictor must compile information from multiple compressor summaries into a cohesive report.

\subsection{Compression Failure Modes}
\label{sec:appendix-failure-modes-compression}

We attempt to understand how scaling compressor size qualitatively increases downstream QA accuracy. To do so, we categorize compressor errors into one of the three recurring failure modes.
%\begin{table}[h]
%\centering
%\caption{Compressor error categories in the compression-prediction workflow on the LongHealth QA task.}
%\label{tab:error-distribution-longhealth}
%\begin{tabular}{lcc}
%\toprule
%\textbf{Error Category} & \textbf{Count} & \textbf{Percentage} \\
%\midrule
%Incorrect Answer                 & 62 & 36.3\% \\
%No Answer                        & 57 & 33.3\% \\
%Omission of Information/Over-Compression  & 52 & 30.4\% \\
%\bottomrule
%\end{tabular}

%\vspace{0.5em}
%\scriptsize{Based on 171 compressor errors that were manually reviewed and labeled. Percentages are approximate.\\Less than 10\% of errors occurring were predictor errors.}
%\end{table}

\paragraph*{Category 1: Incorrect answer.}   
The compression contains an erroneous number or finding in an attempt to answer the question, often due to confusion between reports at different time points. Larger compressors reduce this due to their capacity to better model temporal dependencies.

\begin{exampleboxmaroon}[Error Category 1: Incorrect Answer]
\textbf{Query:} When was Mr. Alan Fisher first diagnosed with a heart disease?  

\textbf{Summary:} Mr. Alan Fisher was first diagnosed with coronary artery disease on 10/1995, during a routine renal transplant biopsy due to progressive deterioration of kidney function. The initial biopsy showed isometric tubular epithelial vacuolization, which was associated with isometric microvesicular cytoplasmic transformation, exceeding 80\%. This finding was highly suggestive of Chronic Allograft Nephropathy (CAN).  

\textbf{Prediction:} 10/1995

\textbf{Correct Answer:} 1989

\textbf{Evidence in context:} **Other Diagnoses** [...] Coronary artery disease, CAD-3: - Previous anterior wall infarction in 1989, treated with [...]

\textbf{Error diagnosis:} Other diagnoses were made in 10/1995, but not coronary artery disease. 
\end{exampleboxmaroon}

\paragraph*{Category 2: No answer.}  
%\textbf{Category 2: No answer.} 
The compression contains no information relevant to the query. All facts and findings in the compression are unrelated, leaving the question unaddressed. The compressor model fails to include any answer-bearing content.

\begin{exampleboxmaroon}[Error Category 2:  No answer]
\textbf{Query:} What significant cardiac intervention did Mr. Fisher undergo in May 1995?

\textbf{Summary:} Mr. Alan Fisher was admitted on March 14, 2020, and underwent a urethrotomy interna and calculated intravenous antibiotic therapy with Meropenem. He was discharged with a urinary catheter inserted. However, he still experienced recurrent urinary tract infections. An additional treatment plan was developed to include antifungal therapy with Fluconazole. The patient was discharged on March 19, 2020.

\textbf{Prediction:} None  

\textbf{Correct Answer:} PTCA + stent in RIVA

\textbf{Evidence in context:} **Other Diagnoses** [...] PTCA + stent in the right coronary artery (RIVA) in 05/1995

\textbf{Error diagnosis:} The cardiac intervention in May 1995 was embedded as background information in the section ``Other Diagnoses" and not directly in the main medical findings. The model failed to understand the importance of historical interventions to answering the query.

\end{exampleboxmaroon}

\paragraph*{Category 3: Details or parts of the findings are omitted from compression.} 
%\textbf{Category 3: Details or parts of the findings are omitted from compression.} 
The compression may correctly identify all medical findings or records required to answer the query, however not in enough detail to adequately answer the question. The compression may also capture only part of the medical findings required to answer the query, while omitting certain facts/information that are crucial to answering the full question. We often observe this when the compression contains many different patient/treatment facts that are irrelevant for answering the query, such that the critical findings are glossed over and "overly compressed".

\begin{exampleboxmaroon}[Error Category 3: Omission of information/over-compression]
\textbf{Query:} Where did Mrs. Done show metastases?

\textbf{Summary:} Mrs. Jane Done, born on 01.01.1966, had a history of metastatic malignant melanoma, with the most recent metastases being detected on 11/01/2020. Her treatment included a microsurgical resection of a right frontal tumor in 2018, a 2020 radiation therapy course, and immunotherapy with Nivolumab and Ipilimumab. In 2021, she was diagnosed with cervical lymph node metastasis, and underwent surgery followed by adjuvant therapy with pembrolizumab. In 2022, she presented with multiple metastases in her CT scan, which were confirmed on PET-CT. Her most recent treatments include six cycles of Vemurafenib, Cobimetinib, and Pembrolizumab.

\textbf{Prediction:} Right frontal tumor, cervical lymph nodes, and multiple unspecified locations  

\textbf{Correct Answer:} Brain, lungs, liver, heart, lymph nodes, muscles, bone  

\textbf{Evidence in context:} Microsurgical resection right frontal tumor [...] hemorrhaged right frontal metastasis from previously diagnosed malignant melanoma [...] multiple roundish subsolid nodules found bipulmonary [...] multiple hypodense lesions throughout both lobes, indicative of metastatic spread [...] concerning 2 cm mass abutting the lateral wall of the left ventricle raising the suspicion for cardiac metastasis [...] Cervical lymph node metastasis [...] a 2.5 cm mass identified within the left psoas muscle, consistent with muscular metastasis [...] lytic lesions involving the sternum and right 4th rib, consistent with osseous metastatic disease

\textbf{Error diagnosis:} The compressor selectively included only frequent metastasis mentions explicitly (brain, lymph nodes) in its summary while compressing numerous organ-specific findings in other parts of the context (lungs, liver, heart, muscles, bone) as "multiple metastasis".  

This suggests that the compressor model was successful in identifying further metastasis. However, the compressor model did not provide all details necessary for answer completeness and was overly aggressive in compressing sites mentioned less frequently in the context.
\end{exampleboxmaroon}

\section{Extended Results}

In this section, we present extended results and ablations of key design choices in our compression-prediction setup.

\subsection{Extended Results on Scaling Laws of Compressor Models}
\label{sec:scaling-laws-appendix}

We extend our analysis by constructing synthetic QA tasks on two further datasets and evaluate the perplexity of the compressions across different compressor model sizes. We measure perplexity by evaluating the log probabilities of a \textsc{Llama-3.1-8B} model on the target answer. %given the chat context/memories generated by \textsc{Gemma-3} and \textsc{Qwen-2.5} compressor models, and the log probabilities of a \textsc{Qwen-2.5-7B} model for \textsc{Llama} compressor models.

%\subsubsection{WildChat: Chat Memory}
\subsubsection{Constructing Chat Memory on Wildchat}
\label{sec:wildchat-results-appendix}

In our experiments, a compressor model summarizes long contexts with regards to context-specific questions. In practice, long context lengths also pose a major challenge in recalling information from past LM chat conversations \citep{eyuboglu2025cartridges}. Modern LLM chatbots construct internal memory about a user's past chat histories, which serve as context for future conversations. Instead of generating query-specific summaries, we generate chat memories for each user by summarizing each chat interaction of a user using a compressor LM. The predictor then attempts to answer synthetic queries posed by the user based on the chat memory. Again, we vary the compressor model size and examine its effects on downstream perplexity, compression size, and compute cost in FLOPs-per-generation.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/figure 13.pdf}
    \caption{\textbf{Perplexity and inference cost scale with compressor model size when generating memory for LMs (\textsc{WildChat}).} The $y$-axis shows the \textbf{(Left)} perplexity on, with the black dashed line showing the baseline perplexity given all $10$ full uncompressed chat conversations, \textbf{(Middle)} Number of tokens in compression of each chat conversation, \textbf{(Right)} GFLOPs-per-generation of each chat conversation. Larger models display lower perplexity and similar behavior in compute scaling as on \textsc{LongHealth}, \textsc{FinanceBench}, and \textsc{FineWeb}.}
    \label{fig:figure-13-wildchat}
\end{figure}

\paragraph*{Larger compressors yield lower perplexity.} As expected, we find in Figure~\ref{fig:figure-13-wildchat} that chat memories generated by larger compressor models yield lower perplexity across model families. Query-agnostic summaries of chat conversations output by the largest compressor model of each model family yield up to \(1.14\times\) lower log probabilities as compared to the \(1\)B model sizes. 

\paragraph*{FLOPs-per-generation scaling holds on \textsc{Wildchat}.} 
The scaling of compression output length on holds for \textsc{Qwen-2.5} and \textsc{Gemma-3} compressors, resulting in FLOPs-per-generation scaling sublinearly with model size (Figure~\ref{fig:figure-13-wildchat}). In contrast, larger \textsc{Llama} compressors generate longer compressions, resulting in steeper scaling of FLOPs-per-generation.  
%The scaling of compression/memory size on \textsc{Wildchat} (Figure~\ref{fig:figure-13-wildchat}) is less clear than for the question-specific compressions generated on \textsc{LongHealth} and \textsc{FinanceBench} (Figure~\ref{fig:figure-3-properties}). 
However, we observe consistent trends in scaling of FLOPs-per-generation between compressor model families: It is significantly more compute-efficient to scale \textsc{Qwen-2.5} and \textsc{Gemma-3} compressors than \textsc{Llama} compressors.

\subsubsection{Varying Task Type on FineWeb}
\label{sec:fineweb-results-appendix}

We extend our analysis of scaling compressor model size to a fourth dataset. On \textsc{FineWeb}, we further ablate by task type: \textbf{extractive} tasks, which require the predictor model to identify and reproduce information explicit in the context---e.g. factual QA--and \textbf{creative} tasks which require the predictor model to generate longer, open-ended outputs that is not verbatim in the context---e.g., paraphrasing, format-change).
We examine the compressor scaling behavior of both query-specific (Figure~\ref{fig:figure-12-fineweb-question-specific}) and query-agnostic (Figure~\ref{fig:figure-12-fineweb-general}) summaries. 

\paragraph*{Larger compressors yield lower perplexity.} 
As expected, we find that increasing compressor size consistently reduces perplexity for both extractive and creative tasks, on both query-specific and query-agnostic summaries. Larger compressors approach the baseline performance of giving the predictor direct access to the full uncompressed context, rather than a lossy compression thereof (Figures~\ref{fig:figure-12-fineweb-question-specific}, \ref{fig:figure-12-fineweb-general}). 

We observe that perplexity differs in magnitude for different task types. Extractive tasks show lower perplexity values, as answers are explicitly present in the context, while creative tasks are more challenging. Query-agnostic summaries tend to achieve lower perplexity on creative tasks than query-specific summaries, which suggests that broader, more general compressions capture stylistic and semantic cues that are key to creative, open-ended generation tasks.

\paragraph*{FLOPs-per-generation scaling holds on \textsc{FineWeb}.} We continue to observe predictable scaling of compute cost. As we increase compressor size, the amount of FLOPs-per-generation increases at different rates consistent with our findings on \textsc{LongHealth}, \textsc{FinanceBench}, and \textsc{WildChat}. Scaling \textsc{Qwen-2.5} and \textsc{Gemma-3} compressor model size comes at a cheaper compute cost than for \textsc{Llama} compressors. We find identical trends across different natures of the task (extractive vs. creative) and types of summary (query-specific vs. query-agnostic). 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/figure 11.pdf}
    \caption{\textbf{Perplexity, length of summary, and inference cost scale with compressor model size on \textsc{FineWeb} for query-specific summaries.} The $y$-axis shows the \textbf{(Left)} perplexity, with the black dashed line showing the baseline perplexity given the full uncompressed context, \textbf{(Middle)} number compression output tokens, \textbf{(Right)} GFLOPs-per-generation on: \textbf{(Top)} extractive tasks, \textbf{(Bottom)} creative tasks. Larger models are more token-efficient, trading off perplexity for compute.}
    \label{fig:figure-12-fineweb-question-specific}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/figure 12.pdf}
    \caption{\textbf{Perplexity, length of summary, and inference cost scale with compressor model size on \textsc{FineWeb} for "general" query-agnostic summaries.} The $y$-axis shows the \textbf{(Top)} perplexity on \textbf{(Top Left)} extractive and \textbf{(Top Right)} creative tasks, with the black dashed line showing the baseline perplexity given the full uncompressed context. \textbf{(Middle)} Number of compression output tokens, \textbf{(Bottom)} GFLOPs-per-generation. Larger models are more token-efficient, trading off perplexity for compute.}
    \label{fig:figure-12-fineweb-general}
\end{figure}

\subsubsection{Scaling of Mutual Information and Bit Efficiency on FinanceBench}
\label{sec:financebench-mi-appendix}
To understand whether our information-theoretic findings generalize beyond \textsc{LongHealth}, we examine mutual information and bit efficiency scaling on \textsc{FinanceBench}. Figure~\ref{fig:figure-4-appendix} shows that the scaling behavior remains consistent: larger compressors retain more information about the original document while compressing more efficiently.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/figure 4 appendix.pdf}
    \caption{\textbf{Larger compressors have outputs that carry more information about their inputs (conditioned on the query) on \textsc{FinanceBench}.} Larger compressor model sizes compress documents with higher \textbf{(Left)} mutual information, \textbf{(Right)} bit efficiency (bits of mutual information per token). The black dotted line \textbf{(Left)} represents the theoretical maximum of MI at the natural logarithm $\log(N)$, where $N$ is the number of documents MI is computed across.}
    \label{fig:figure-4-appendix}
\end{figure}

\subsubsection{What Are the Effects of Conciseness Instructions?}
\label{sec:financebench-prompting-appendix}
Naturally, we ask whether explicitly instructing the compressor to different levels of conciseness changes the scaling behaviors that we observe. 
We vary the prompt to instruct the compressor LM to be \textit{concise} (3 sentences), \textit{normal} (6 sentences), and \textit{elaborate} (9 sentences). 
%\begin{figure}[h]
%    \centering
%    \includegraphics[width=1\linewidth]{figures/figure 5.pdf}
%    \caption{\textbf{Scaling behaviors of compressor model size hold across instructed conciseness (\textsc{Compressor = Qwen-2.5}).} 
%    \textbf{(Left)} Accuracy \textbf{(Middle)} GFLOPs per compression \textbf{(Right)} MI versus compressor model size. 
%    Accuracy and MI are largely unaffected by conciseness instructions. Compressors instructed to be more concise are more token-efficient, and thus compute-efficient. Trends in accuracy, compute cost, and MI as we scale compressor hold across conciseness constraints. Appendix~\ref{sec:financebench-prompting-appendix} shows analogous results on \textsc{FinanceBench}.}
%    \label{fig:figure-5-prompting}
%\end{figure}
We find in Figure~\ref{fig:figure-5-prompting} and \ref{fig:figure-5-appendix} that accuracy and MI are unaffected by instructed conciseness on both \textsc{LongHealth} and \textsc{FinanceBench}.
While prompting shifts compression output size and compute cost by an absolute offset, the compressor scaling trends hold across different conciseness constraints, showing that our scaling results are driven by compressor capacity.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/figure 5 appendix.pdf}
    \caption{\textbf{Scaling behaviors of compressor model size hold across instructed conciseness (\textsc{Compressor = Llama}).} 
    \textbf{(Left)} Accuracy \textbf{(Middle)} GFLOPs-per-generation \textbf{(Right)} MI versus compressor model size. 
    Accuracy and MI are largely unaffected by conciseness instructions. Compressors instructed to be more concise are more token-efficient, and thus compute-efficient. Trends in accuracy, compute cost, and MI as we scale compressor hold across conciseness constraints.}
    \label{fig:figure-5-appendix}
\end{figure}

%\subsubsection{Scaling of $I(Z;Y)$ on LongHealth}
%\label{sec:appendix-longhealth-dpi}
%We evaluate compressor model scaling on the \textsc{LongHealth} dataset as a multiple-choice QA task. We ask a \textsc{Llama-3.1-8B} predictor model to select one of the $K=5$ answer options given the compression $Z$. Since we frame the prediction task as a classification problem, we are able to use our lower bound estimator in Section~\ref{sec:appendix-mi-lowerbound} to measure how much information compressions $Z$ contain about target predictions $Y$. We find in Figure~\ref{fig:figure-14-lowerbound} that $I(Z;Y)$ increases with compressor model size. Larger compressors output summaries that preserve more QA-relevant information for downstream prediction. 

%We compute $I(X;Y)$ using the same lower bound estimator by prompting the predictor with the full document context. Consistent with the data processing inequality for $X \to Z \to Y$, we find that $I(Z;Y)$ lies strictly below $I(X;Y)$ across all compressor model sizes, which aligns with the data processing inequality \(I(X;Z)\ge I(Z;Y)\).
%We confirm that the compressor acts as the information bottleneck by limiting the amount of information the predictor is able to access.

%\begin{figure}[h!]
%    \centering
%    \includegraphics[width=0.35\linewidth]{figures/figure 14 dpi longhealth.png}
%    \caption{\textbf{Larger compressor models have outputs that carry more information about the ground truth prediction on \textsc{LongHealth} multiple-choice QA tasks.} Larger compressor model sizes compress documents with higher mutual information $I(Z;Y)$ between compression $Z$ and prediction $Y$. The black dotted line represents the lower bound to $I(X;Y)$, and the gray area around the line denotes the standard error. This confirms that the data processing inequality theorem holds for our system.}
%    \label{fig:figure-14-lowerbound}
%\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/figure 10.pdf}
    \caption{\textbf{Generalized Linear Model (GLM) Coefficients.} Regression analysis on a GLM predicting QA correctness (0/1) on \textbf{(Blue)} \textsc{LongHealth}, \textbf{(Red)} \textsc{FinanceBench}. The y-axis shows coefficient estimates for each variable, horizontal bars are 95\% confidence intervals, asterisks mark variables that are significant at $p < 0.05$ on both datasets. For more details on our GLM setup, refer to Appendix~\ref{sec:appendix-glm-analysis}.}
    \label{fig:figure-10-glm}
\end{figure}

\subsection{Extended Results of Rate-Distortion Analysis}
\label{sec:appendix-rate-distortion-results}
We aim to establish rules of thumb for design decisions around choice of compressor and predictor models based on rate-distortion theoretic concepts introduced in Section~\ref{sec:info-theoretic-setup} and Appendix~\ref{sec:appendix-rate-distortion}. 
We further investigate the fidelity, compute cost, and communication efficiency of different compressor-predictor pairings. 
We examine light-weight \textsc{Qwen-2.5} and \textsc{Llama} compressor models, and \textsc{Qwen-2.5} and \textsc{Llama} predictor models stretching from 1B to 405B parameters. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/figure 6 combined.pdf}
    \caption{\textbf{Exploring the trade-off between compression and fidelity loss: rate-distortion curve.} y-axis depicts distortion and x-axis shows rate.
    Markers indicate compressor sizes (1B, 3B, 8B) in the \textsc{Llama-3} compressor model family; vertical and horizontal bars denote standard errors. 
    \textbf{(Left)} We examine a single compressor-predictor LM pairing, \textsc{compressor=Llama-3} and \textsc{predictor=Llama-3.3-70B}. 
    \textbf{(Right)} We compare different compressor-predictor LM pairings, where the predictor model is \textsc{Qwen-2.5-72B} ("Qwen-2.5") or \textsc{Llama-3.3-70B} ("Llama"). See Appendix~\ref{sec:appendix-rate-distortion} for results on further compressor-predictor LM pairings.}
    \label{fig:figure-6/7-combined}
\end{figure}

\subsubsection{Scaling Predictor Model Size} 
\label{sec:appendix-scaling-predictor-size}

In a compression-prediction system, the compressor model acts as a bottleneck on information about the document $X$. If we fix that information bottleneck and the amount of information passed through, how does the capacity to generate predictions affect downstream QA performance? We fix the compressor model to be either \textsc{Qwen-2.5} or \textsc{Llama}, and vary the predictor to be \textsc{Llama} models sizes 1B, 8B, 70B, 405B on the datasets \textsc{LongHealth} and \textsc{FinanceBench} (Figure~\ref{fig:figure-6-appendix}). By plotting accuracy against bit efficiency in an alternative representation of rate-distortion analysis, downstream QA accuracy improves when moving from small predictors at 1B to larger 8--70B predictors, but then saturates with further scaling to a massive 405B predictor (Figure~\ref{fig:figure-6-appendix-2}). 
%We fit exponential decay functions to the rate-distortion data points in Figure~\ref{fig:figure-8-supervisorscaling}:
%\begin{align*}
%    D(R) &= \hat C\,e^{-\hat \beta R} + \hat D_0.
%\end{align*}
%We optimize the parameters $\hat C$ for initial value, $\hat \beta$ for decay rate, and $\hat D_0$ for distortion floor. See Appendix~\ref{sec:appendix-rate-distortion} for further information.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/figure 6 appendix.pdf}
    \caption{\textbf{Exploring the trade-off between compression and fidelity loss: rate-distortion curve.} The y-axis depicts distortion and the x-axis shows rate. \textit{Distortion} measures how far our decompression is from the original data, which is quantified by 1 - accuracy. \textit{Rate} measures the amount of information (as quantified by mutual information) encoded in each compression token. Markers indicate compressor sizes in the \textsc{Qwen-2.5} (1.5B, 3B, 7B) and \textsc{Llama} (1B, 3B, 8B)  compressor model family; vertical and horizontal bars denote standard errors.}
    \label{fig:figure-6-appendix}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/figure 6 appendix 2.pdf}
    \caption{\textbf{Exploring the relationship between compression and accuracy.} The y-axis depicts accuracy and the x-axis shows bit efficiency. \textit{Bit efficiency} is defined as the bits of mutual information encoded in each compression token. Markers indicate compressor sizes in the \textsc{Qwen-2.5} (1.5B, 3B, 7B) and \textsc{Llama} (1B, 3B, 8B)  compressor model family; vertical and horizontal bars denote standard errors.}
    \label{fig:figure-6-appendix-2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/figure 9 appendix.pdf}
    \caption{\textbf{QA Accuracy versus total compute cost on \textsc{FinanceBench}.} In each panel, the y-axis shows the accuracy and the x-axis plots total compute cost in FLOPs-per-generation on a log-scale for \textbf{(Left)} \textsc{Qwen-2.5}, \textbf{(Right)} \textsc{Llama-3} compressor LMs. Markers indicate compressor sizes in the \textsc{Qwen-2.5} (1.5B, 3B, 7B) and \textsc{Llama-3} (1B, 3B, 8B)  compressor model family; vertical and horizontal bars denote standard errors.}
    \label{fig:figure-9-financebench}
\end{figure}

\subsection{Extended Results of Deep Research Analysis}
\label{sec:appendix-deep-research-analysis}

In our Deep Research scaling experiments, we additionally ablate the effect of search result quantity by providing \textsc{GPT-4o} with the top $k={1, 3, 6}$ search results from each of the 8 predictor queries directly, bypassing the compression step. This analysis in Figure~\ref{fig:deepres_main} reveals that even with maximal context utilization ($k=6$, totaling 48 sources), the uncompressed approach achieves a similar level of \textit{RACE} scores at significantly higher API costs compared to our compression-based strategy.

We also compute the compute cost in FLOPs-per-generation of the system, as shown in Figure~\ref{fig:deepres-flops-tokens}. We observe an increase in the number of total FLOPs-per-generation used by the Deep Research system as predictor size increases. We generally observe that larger compressors extract and provide more tokens of information in their compressions.

\begin{figure}[htbp]
    \centering
\includegraphics[width=0.85\linewidth]{figures/deepres_flops_tokens.pdf}
    \caption{%
    \textbf{Compressor model size versus compute and token usage.} 
    In each panel, the x-axis shows the Qwen compressor size (in billions of parameters). 
    \textbf{(Left)} Total GFLOPs per task grows with compressor model size, with larger predictors (Llama 405B, 70B, 8B) amplifying compute cost. 
    \textbf{(Right)} Compressor output tokens per task, which remain relatively stable across predictors (GPT-4o, Llama 405B, 70B, 8B), increase moderately with larger compressors.}
    \label{fig:deepres-flops-tokens}
\end{figure}

%\subsection{Learning Synthetic Compression through Small-Scale Transformers}

%We try to understand in more detail the relationship between compute and communication efficiency in LMs. In our previous experiments, we found that larger compressor LMs produce more concise summaries (Figure~\ref{fig:figure-3-properties}) with higher mutual information (Figure~\ref{fig:figure-4-mi}). Larger compressor LMs appear to be able to compress semantic information relevant to downstream queries more effectively, trading additional computation for reduced communication cost. We ask whether this is a consequence of the raw model capacity of larger models, or from the greater amount of information absorbed during LM pretraining.

%To investigate these questions systematically, we design a controlled experimental framework that isolates the compression-prediction pipeline. Rather than studying the complex, high-dimensional space of natural language, we examine how neural architectures learn to compress binary sequences for downstream prediction tasks. 

\end{document}
