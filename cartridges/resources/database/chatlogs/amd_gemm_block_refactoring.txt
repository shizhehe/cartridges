

User: I am thinking of splitting up REG_BLOCK into smaller pieces, like BLOCK_SIZE/8. Can you help me refactor this kernel to fit that logic. Make minimal changes.


Assistant: 

I'd be happy to help you refactor the kernel to split REG_BLOCK into smaller pieces of BLOCK_SIZE/8. However, I don't see the kernel code in your message. Could you please share the kernel code you'd like me to refactor? Once I can see the current implementation, I'll be able to make the minimal changes needed to implement your BLOCK_SIZE/8 splitting logic.


User: 

#include "kittens.cuh"
#include "pyutils/pyutils.cuh"
#include "utils.cpp"
using namespace kittens;

constexpr int BLOCK_SIZE       = 128;  
constexpr int K_STEP           = 256;
constexpr int DOT_SLICE_SHARED = 64;
constexpr int REG_BLOCK        = BLOCK_SIZE / 4; 
constexpr int DOT_SLICE        = 16;

#define NUM_WARPS 8
#define NUM_THREADS (kittens::WARP_THREADS * NUM_WARPS)

#define M 8192
#define K 8192
#define N 8192

using _gl_A = gl<bf16, -1, -1, -1, -1>;
using _gl_B = gl<bf16, -1, -1, -1, -1>;
using _gl_C = gl<float, -1, -1, -1, -1>;

using G = kittens::group<8>;

struct micro_globals {
    _gl_A a;
    _gl_B b;
    _gl_C c;
    dim3 grid()  { return dim3(N / BLOCK_SIZE, M / BLOCK_SIZE); } 
    dim3 block() { return dim3(NUM_THREADS); } 
    size_t dynamic_shared_memory() { return 65536; }
};

__global__ __launch_bounds__(NUM_THREADS, 1)
void micro_tk(const micro_globals g) {
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);
    st_bf<BLOCK_SIZE, DOT_SLICE_SHARED> (&As) = al.allocate<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>();
    st_bf<BLOCK_SIZE, DOT_SLICE_SHARED> (&Bs) = al.allocate<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>();

    rt_bf<REG_BLOCK, DOT_SLICE> a_reg_0, b_reg_0, b_reg_1;
    rt_fl<REG_BLOCK, REG_BLOCK, ducks::rt_layout::col> C_accum[2];
    for (int i = 0; i < 2; i++) { zero(C_accum[i]); }

    // Small register buffers for pipelining
    constexpr int BUFFER_SIZE = 64;
    float4 a_buffer_next[BUFFER_SIZE];
    float4 b_buffer_next[BUFFER_SIZE];
    int a_metadata[3], b_metadata[3];

    const int row = blockIdx.y;
    const int col = blockIdx.x;

    const int warp_id = kittens::warpid();
    const int warp_row = warp_id / 2;
    const int warp_col = warp_id % 2;

    const int num_tiles = K / K_STEP;
    const int num_shared_slices = K_STEP / DOT_SLICE_SHARED;
    const int num_slices = DOT_SLICE_SHARED / DOT_SLICE;

    // Load first tile into shared memory
    G::load(As, g.a, {0, 0, row, 0});
    G::load(Bs, g.b, {0, 0, col, 0});


    // Pre-load second tile into register bufers 
    load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_A, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
        a_buffer_next, BUFFER_SIZE, g.a, {0, 0, row, 1}, As, a_metadata);
    load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_B, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
        b_buffer_next, BUFFER_SIZE, g.b, {0, 0, col, 1}, Bs, b_metadata);

    if (warp_col / 4 == 0) {
        __builtin_amdgcn_s_barrier();
    }
    asm volatile("s_waitcnt vmcnt(0)");


    // #pragma unroll 2
    for (int tile = 0; tile < num_tiles; ++tile) {

        for (int shared_slice = 0; shared_slice < num_shared_slices; ++shared_slice) {
            const int next_k_offset = tile * num_shared_slices + shared_slice + 1;
            const bool should_load = (next_k_offset < num_tiles * num_shared_slices);

            // Start loading NEXT data to registers 
            if (should_load) {
                load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_A, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
                    a_buffer_next, BUFFER_SIZE, g.a, {0, 0, row, next_k_offset}, As, a_metadata);
                
                load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_B, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
                    b_buffer_next, BUFFER_SIZE, g.b, {0, 0, col, next_k_offset}, Bs, b_metadata);
            }

            __builtin_amdgcn_s_barrier();

            
            // Compute on CURRENT data in shared memory
            #pragma unroll 
            for (int slice = 0; slice < num_slices; ++slice) {
                kittens::load_async(b_reg_1, subtile_inplace<REG_BLOCK, DOT_SLICE>(Bs, {warp_col + 2, slice}));
                kittens::load(a_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(As, {warp_row, slice}));
                kittens::load(b_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(Bs, {warp_col, slice}));
                mma_ABt(C_accum[0], a_reg_0, b_reg_0, C_accum[0]);
                mma_ABt(C_accum[1], a_reg_0, b_reg_1, C_accum[1]);
            }
            asm volatile("s_waitcnt vmcnt(1)");

            // Now wait for loads and write to shared memory
            if (should_load) {
                store_registers_to_shared<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, NUM_THREADS>(
                    a_buffer_next, As, a_metadata);
                store_registers_to_shared<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, NUM_THREADS>(
                    b_buffer_next, Bs, b_metadata);
            }
        }
    }

    if (warp_col / 4 == 1) {
        __builtin_amdgcn_s_barrier();
    }

    store(g.c, C_accum[0], {0, 0, row * 4 + warp_row, col * 4 + warp_col});
    store(g.c, C_accum[1], {0, 0, row * 4 + warp_row, col * 4 + warp_col + 2});
}

void dispatch_micro(micro_globals g) {
    unsigned long mem_size = g.dynamic_shared_memory();
    hipFuncSetAttribute((void*)micro_tk, hipFuncAttributeMaxDynamicSharedMemorySize, mem_size);
    micro_tk<<<g.grid(), g.block(), mem_size>>>(g);
    hipDeviceSynchronize();
}

PYBIND11_MODULE(tk_kernel, m) {
    m.doc() = "tk_kernel python module";
    py::bind_kernel<micro_tk>(m, "micro_tk", &micro_globals::a, &micro_globals::b, &micro_globals::c); 
    py::bind_function<dispatch_micro>(m, "dispatch_micro", &micro_globals::a, &micro_globals::b, &micro_globals::c);
}


Looking at your kernel code, I can see that REG_BLOCK is currently defined as BLOCK_SIZE / 4 (which equals 32), and you want to split it into smaller pieces of BLOCK_SIZE / 8 (which would be 16).
Here's the refactored kernel with minimal changes to implement your BLOCK_SIZE/8 logic:

#include "kittens.cuh"
#include "pyutils/pyutils.cuh"
#include "utils.cpp"
using namespace kittens;

constexpr int BLOCK_SIZE       = 128;  
constexpr int K_STEP           = 256;
constexpr int DOT_SLICE_SHARED = 64;
constexpr int REG_BLOCK        = BLOCK_SIZE / 8;  // Changed from /4 to /8 (now 16 instead of 32)
constexpr int DOT_SLICE        = 16;

#define NUM_WARPS 8
#define NUM_THREADS (kittens::WARP_THREADS * NUM_WARPS)

#define M 8192
#define K 8192
#define N 8192

using _gl_A = gl<bf16, -1, -1, -1, -1>;
using _gl_B = gl<bf16, -1, -1, -1, -1>;
using _gl_C = gl<float, -1, -1, -1, -1>;

using G = kittens::group<8>;

struct micro_globals {
    _gl_A a;
    _gl_B b;
    _gl_C c;
    dim3 grid()  { return dim3(N / BLOCK_SIZE, M / BLOCK_SIZE); } 
    dim3 block() { return dim3(NUM_THREADS); } 
    size_t dynamic_shared_memory() { return 65536; }
};

__global__ __launch_bounds__(NUM_THREADS, 1)
void micro_tk(const micro_globals g) {
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);
    st_bf<BLOCK_SIZE, DOT_SLICE_SHARED> (&As) = al.allocate<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>();
    st_bf<BLOCK_SIZE, DOT_SLICE_SHARED> (&Bs) = al.allocate<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>();

    rt_bf<REG_BLOCK, DOT_SLICE> a_reg_0, b_reg_0, b_reg_1;
    rt_fl<REG_BLOCK, REG_BLOCK, ducks::rt_layout::col> C_accum[4];  // Doubled from 2 to 4
    for (int i = 0; i < 4; i++) { zero(C_accum[i]); }  // Updated loop bound

    // Small register buffers for pipelining
    constexpr int BUFFER_SIZE = 64;
    float4 a_buffer_next[BUFFER_SIZE];
    float4 b_buffer_next[BUFFER_SIZE];
    int a_metadata[3], b_metadata[3];

    const int row = blockIdx.y;
    const int col = blockIdx.x;

    const int warp_id = kittens::warpid();
    const int warp_row = warp_id / 2;
    const int warp_col = warp_id % 2;

    const int num_tiles = K / K_STEP;
    const int num_shared_slices = K_STEP / DOT_SLICE_SHARED;
    const int num_slices = DOT_SLICE_SHARED / DOT_SLICE;

    // Load first tile into shared memory
    G::load(As, g.a, {0, 0, row, 0});
    G::load(Bs, g.b, {0, 0, col, 0});

    // Pre-load second tile into register bufers 
    load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_A, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
        a_buffer_next, BUFFER_SIZE, g.a, {0, 0, row, 1}, As, a_metadata);
    load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_B, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
        b_buffer_next, BUFFER_SIZE, g.b, {0, 0, col, 1}, Bs, b_metadata);

    if (warp_col / 4 == 0) {
        __builtin_amdgcn_s_barrier();
    }
    asm volatile("s_waitcnt vmcnt(0)");

    // #pragma unroll 2
    for (int tile = 0; tile < num_tiles; ++tile) {

        for (int shared_slice = 0; shared_slice < num_shared_slices; ++shared_slice) {
            const int next_k_offset = tile * num_shared_slices + shared_slice + 1;
            const bool should_load = (next_k_offset < num_tiles * num_shared_slices);

            // Start loading NEXT data to registers 
            if (should_load) {
                load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_A, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
                    a_buffer_next, BUFFER_SIZE, g.a, {0, 0, row, next_k_offset}, As, a_metadata);
                
                load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_B, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
                    b_buffer_next, BUFFER_SIZE, g.b, {0, 0, col, next_k_offset}, Bs, b_metadata);
            }

            __builtin_amdgcn_s_barrier();

            // Compute on CURRENT data in shared memory
            #pragma unroll 
            for (int slice = 0; slice < num_slices; ++slice) {
                // Load data for all 4 sub-blocks
                kittens::load(a_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(As, {warp_row, slice}));
                kittens::load(b_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(Bs, {warp_col, slice}));
                kittens::load_async(b_reg_1, subtile_inplace<REG_BLOCK, DOT_SLICE>(Bs, {warp_col + 2, slice}));
                
                // Compute for sub-blocks [0] and [1] (same as before)
                mma_ABt(C_accum[0], a_reg_0, b_reg_0, C_accum[0]);
                mma_ABt(C_accum[1], a_reg_0, b_reg_1, C_accum[1]);
                
                // Load and compute for additional sub-blocks [2] and [3]
                kittens::load(a_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(As, {warp_row + 4, slice}));
                mma_ABt(C_accum[2], a_reg_0, b_reg_0, C_accum[2]);
                mma_ABt(C_accum[3], a_reg_0, b_reg_1, C_accum[3]);
            }
            asm volatile("s_waitcnt vmcnt(1)");

            // Now wait for loads and write to shared memory
            if (should_load) {
                store_registers_to_shared<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, NUM_THREADS>(
                    a_buffer_next, As, a_metadata);
                store_registers_to_shared<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, NUM_THREADS>(
                    b_buffer_next, Bs, b_metadata);
            }
        }
    }

    if (warp_col / 4 == 1) {
        __builtin_amdgcn_s_barrier();
    }

    // Store all 4 sub-blocks
    store(g.c, C_accum[0], {0, 0, row * 8 + warp_row, col * 4 + warp_col});          // Changed row multiplier from 4 to 8
    store(g.c, C_accum[1], {0, 0, row * 8 + warp_row, col * 4 + warp_col + 2});      // Changed row multiplier from 4 to 8
    store(g.c, C_accum[2], {0, 0, row * 8 + warp_row + 4, col * 4 + warp_col});      // New sub-block
    store(g.c, C_accum[3], {0, 0, row * 8 + warp_row + 4, col * 4 + warp_col + 2});  // New sub-block
}

void dispatch_micro(micro_globals g) {
    unsigned long mem_size = g.dynamic_shared_memory();
    hipFuncSetAttribute((void*)micro_tk, hipFuncAttributeMaxDynamicSharedMemorySize, mem_size);
    micro_tk<<<g.grid(), g.block(), mem_size>>>(g);
    hipDeviceSynchronize();
}

PYBIND11_MODULE(tk_kernel, m) {
    m.doc() = "tk_kernel python module";
    py::bind_kernel<micro_tk>(m, "micro_tk", &micro_globals::a, &micro_globals::b, &micro_globals::c); 
    py::bind_function<dispatch_micro>(m, "dispatch_micro", &micro_globals::a, &micro_globals::b, &micro_globals::c);
}

The key changes made to split REG_BLOCK into BLOCK_SIZE/8 pieces:

Changed REG_BLOCK definition: From BLOCK_SIZE / 4 (32) to BLOCK_SIZE / 8 (16)
Doubled the accumulator array: C_accum[4] instead of C_accum[2] to handle twice as many sub-blocks
Updated the compute loop: Added computation for the additional sub-blocks by loading data from warp_row + 4 and computing C_accum[2] and C_accum[3]
Updated the store operations: Added stores for the new sub-blocks with appropriate row offsets (row * 8 instead of row * 4 and additional + 4 offset for the new sub-blocks)

These minimal changes maintain the same overall structure and logic while accommodating the smaller register block size. The kernel now processes 4 sub-blocks instead of 2, each of size 16x16 instead of 32x32.


User: If we halve REG_BLOCK, wouldn't we need to quadruple the number of C_accum?


Assistant: 

You're absolutely right! If we halve REG_BLOCK from 32 to 16, we're effectively splitting each 32x32 block into four 16x16 blocks, so we need to quadruple the number of accumulators, not just double them.
Let me fix that:

#include "kittens.cuh"
#include "pyutils/pyutils.cuh"
#include "utils.cpp"
using namespace kittens;

constexpr int BLOCK_SIZE       = 128;  
constexpr int K_STEP           = 256;
constexpr int DOT_SLICE_SHARED = 64;
constexpr int REG_BLOCK        = BLOCK_SIZE / 8;  // Changed from /4 to /8 (now 16 instead of 32)
constexpr int DOT_SLICE        = 16;

#define NUM_WARPS 8
#define NUM_THREADS (kittens::WARP_THREADS * NUM_WARPS)

#define M 8192
#define K 8192
#define N 8192

using _gl_A = gl<bf16, -1, -1, -1, -1>;
using _gl_B = gl<bf16, -1, -1, -1, -1>;
using _gl_C = gl<float, -1, -1, -1, -1>;

using G = kittens::group<8>;

struct micro_globals {
    _gl_A a;
    _gl_B b;
    _gl_C c;
    dim3 grid()  { return dim3(N / BLOCK_SIZE, M / BLOCK_SIZE); } 
    dim3 block() { return dim3(NUM_THREADS); } 
    size_t dynamic_shared_memory() { return 65536; }
};

__global__ __launch_bounds__(NUM_THREADS, 1)
void micro_tk(const micro_globals g) {
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);
    st_bf<BLOCK_SIZE, DOT_SLICE_SHARED> (&As) = al.allocate<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>();
    st_bf<BLOCK_SIZE, DOT_SLICE_SHARED> (&Bs) = al.allocate<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>();

    rt_bf<REG_BLOCK, DOT_SLICE> a_reg_0, b_reg_0, b_reg_1;
    rt_fl<REG_BLOCK, REG_BLOCK, ducks::rt_layout::col> C_accum[8];  // Quadrupled from 2 to 8
    for (int i = 0; i < 8; i++) { zero(C_accum[i]); }  // Updated loop bound

    // Small register buffers for pipelining
    constexpr int BUFFER_SIZE = 64;
    float4 a_buffer_next[BUFFER_SIZE];
    float4 b_buffer_next[BUFFER_SIZE];
    int a_metadata[3], b_metadata[3];

    const int row = blockIdx.y;
    const int col = blockIdx.x;

    const int warp_id = kittens::warpid();
    const int warp_row = warp_id / 2;
    const int warp_col = warp_id % 2;

    const int num_tiles = K / K_STEP;
    const int num_shared_slices = K_STEP / DOT_SLICE_SHARED;
    const int num_slices = DOT_SLICE_SHARED / DOT_SLICE;

    // Load first tile into shared memory
    G::load(As, g.a, {0, 0, row, 0});
    G::load(Bs, g.b, {0, 0, col, 0});

    // Pre-load second tile into register bufers 
    load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_A, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
        a_buffer_next, BUFFER_SIZE, g.a, {0, 0, row, 1}, As, a_metadata);
    load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_B, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
        b_buffer_next, BUFFER_SIZE, g.b, {0, 0, col, 1}, Bs, b_metadata);

    if (warp_col / 4 == 0) {
        __builtin_amdgcn_s_barrier();
    }
    asm volatile("s_waitcnt vmcnt(0)");

    // #pragma unroll 2
    for (int tile = 0; tile < num_tiles; ++tile) {

        for (int shared_slice = 0; shared_slice < num_shared_slices; ++shared_slice) {
            const int next_k_offset = tile * num_shared_slices + shared_slice + 1;
            const bool should_load = (next_k_offset < num_tiles * num_shared_slices);

            // Start loading NEXT data to registers 
            if (should_load) {
                load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_A, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
                    a_buffer_next, BUFFER_SIZE, g.a, {0, 0, row, next_k_offset}, As, a_metadata);
                
                load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_B, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
                    b_buffer_next, BUFFER_SIZE, g.b, {0, 0, col, next_k_offset}, Bs, b_metadata);
            }

            __builtin_amdgcn_s_barrier();

            // Compute on CURRENT data in shared memory
            #pragma unroll 
            for (int slice = 0; slice < num_slices; ++slice) {
                // Load data for computation
                kittens::load(a_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(As, {warp_row * 2, slice}));
                kittens::load(b_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(Bs, {warp_col * 2, slice}));
                kittens::load_async(b_reg_1, subtile_inplace<REG_BLOCK, DOT_SLICE>(Bs, {warp_col * 2 + 1, slice}));
                
                // Compute for first row of sub-blocks
                mma_ABt(C_accum[0], a_reg_0, b_reg_0, C_accum[0]);  // [0,0]
                mma_ABt(C_accum[1], a_reg_0, b_reg_1, C_accum[1]);  // [0,1]
                
                // Load second A block and compute
                kittens::load(a_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(As, {warp_row * 2 + 1, slice}));
                mma_ABt(C_accum[2], a_reg_0, b_reg_0, C_accum[2]);  // [1,0]
                mma_ABt(C_accum[3], a_reg_0, b_reg_1, C_accum[3]);  // [1,1]
                
                // Load B blocks for second column pair
                kittens::load(b_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(Bs, {warp_col * 2 + 4, slice}));
                kittens::load_async(b_reg_1, subtile_inplace<REG_BLOCK, DOT_SLICE>(Bs, {warp_col * 2 + 5, slice}));
                
                // Load first A block again and compute for third column pair
                kittens::load(a_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(As, {warp_row * 2, slice}));
                mma_ABt(C_accum[4], a_reg_0, b_reg_0, C_accum[4]);  // [0,2]
                mma_ABt(C_accum[5], a_reg_0, b_reg_1, C_accum[5]);  // [0,3]
                
                // Load second A block and compute for fourth column pair
                kittens::load(a_reg_0, subtile_inplace<REG_BLOCK, DOT_SLICE>(As, {warp_row * 2 + 1, slice}));
                mma_ABt(C_accum[6], a_reg_0, b_reg_0, C_accum[6]);  // [1,2]
                mma_ABt(C_accum[7], a_reg_0, b_reg_1, C_accum[7]);  // [1,3]
            }
            asm volatile("s_waitcnt vmcnt(1)");

            // Now wait for loads and write to shared memory
            if (should_load) {
                store_registers_to_shared<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, NUM_THREADS>(
                    a_buffer_next, As, a_metadata);
                store_registers_to_shared<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, NUM_THREADS>(
                    b_buffer_next, Bs, b_metadata);
            }
        }
    }

    if (warp_col / 4 == 1) {
        __builtin_amdgcn_s_barrier();
    }

    // Store all 8 sub-blocks (2x2 grid of the original 2 blocks becomes 4x2 grid)
    store(g.c, C_accum[0], {0, 0, row * 8 + warp_row * 2, col * 8 + warp_col * 2});         // [0,0]
    store(g.c, C_accum[1], {0, 0, row * 8 + warp_row * 2, col * 8 + warp_col * 2 + 1});     // [0,1]
    store(g.c, C_accum[2], {0, 0, row * 8 + warp_row * 2 + 1, col * 8 + warp_col * 2});     // [1,0]
    store(g.c, C_accum[3], {0, 0, row * 8 + warp_row * 2 + 1, col * 8 + warp_col * 2 + 1}); // [1,1]
    store(g.c, C_accum[4], {0, 0, row * 8 + warp_row * 2, col * 8 + warp_col * 2 + 4});     // [0,2]
    store(g.c, C_accum[5], {0, 0, row * 8 + warp_row * 2, col * 8 + warp_col * 2 + 5});     // [0,3]
    store(g.c, C_accum[6], {0, 0, row * 8 + warp_row * 2 + 1, col * 8 + warp_col * 2 + 4}); // [1,2]
    store(g.c, C_accum[7], {0, 0, row * 8 + warp_row * 2 + 1, col * 8 + warp_col * 2 + 5}); // [1,3]
}

void dispatch_micro(micro_globals g) {
    unsigned long mem_size = g.dynamic_shared_memory();
    hipFuncSetAttribute((void*)micro_tk, hipFuncAttributeMaxDynamicSharedMemorySize, mem_size);
    micro_tk<<<g.grid(), g.block(), mem_size>>>(g);
    hipDeviceSynchronize();
}

PYBIND11_MODULE(tk_kernel, m) {
    m.doc() = "tk_kernel python module";
    py::bind_kernel<micro_tk>(m, "micro_tk", &micro_globals::a, &micro_globals::b, &micro_globals::c); 
    py::bind_function<dispatch_micro>(m, "dispatch_micro", &micro_globals::a, &micro_globals::b, &micro_globals::c);
}

You're absolutely correct! Now we have:

8 accumulators instead of 2 (quadrupled as you pointed out)
Proper indexing that accounts for splitting each 32x32 block into four 16x16 sub-blocks
Updated compute logic that loads the appropriate sub-tiles and computes all 8 combinations
Correct store operations that place the results in the right locations

The logic now properly handles the 2x2 subdivision of each original register block, giving us 8 total sub-blocks per warp (2 original blocks Ã— 4 sub-blocks each).


User: 

#include "kittens.cuh"
#include "pyutils/pyutils.cuh"
#include "utils.cpp"
using namespace kittens;

#define NUM_WARPS 8
#define NUM_THREADS (kittens::WARP_THREADS * NUM_WARPS)

constexpr int BLOCK_SIZE       = 128;  
constexpr int K_STEP           = 256;
constexpr int DOT_SLICE_SHARED = 64;
constexpr int REG_BLOCK_M      = BLOCK_SIZE / 8; 
constexpr int REG_BLOCK_N      = BLOCK_SIZE / 8;
constexpr int DOT_SLICE        = 16;

#define M 8192
#define K 8192
#define N 8192

using _gl_A = gl<bf16, -1, -1, -1, -1>;
using _gl_B = gl<bf16, -1, -1, -1, -1>;
using _gl_C = gl<float, -1, -1, -1, -1>;

using G = kittens::group<8>;

struct micro_globals {
    _gl_A a;
    _gl_B b;
    _gl_C c;
    dim3 grid()  { return dim3(N / BLOCK_SIZE, M / BLOCK_SIZE); } 
    dim3 block() { return dim3(NUM_THREADS); } 
    size_t dynamic_shared_memory() { return 65536; }
};

__global__ __launch_bounds__(NUM_THREADS, 1)
void micro_tk(const micro_globals g) {
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);
    st_bf<BLOCK_SIZE, DOT_SLICE_SHARED> (&As) = al.allocate<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>();
    st_bf<BLOCK_SIZE, DOT_SLICE_SHARED> (&Bs) = al.allocate<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>();

    rt_bf<REG_BLOCK_M, DOT_SLICE> a_reg_0, b_reg_0, b_reg_1;
    rt_fl<REG_BLOCK_M, REG_BLOCK_N, ducks::rt_layout::col> C_accum[8];
    for (int i = 0; i < 8; i++) { zero(C_accum[i]); }

    // Small register buffers for pipelining
    constexpr int BUFFER_SIZE = 64;
    float4 a_buffer_next[BUFFER_SIZE];
    float4 b_buffer_next[BUFFER_SIZE];
    int a_metadata[3], b_metadata[3];

    const int row = blockIdx.y;
    const int col = blockIdx.x;

    const int warp_id = kittens::warpid();
    const int warp_row = warp_id / 2;
    const int warp_col = warp_id % 2;

    const int num_tiles = K / K_STEP;
    const int num_shared_slices = K_STEP / DOT_SLICE_SHARED;
    const int num_slices = DOT_SLICE_SHARED / DOT_SLICE;

    // Load first tile into shared memory
    G::load(As, g.a, {0, 0, row, 0});
    G::load(Bs, g.b, {0, 0, col, 0});


    // Pre-load second tile into register bufers 
    load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_A, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
        a_buffer_next, BUFFER_SIZE, g.a, {0, 0, row, 1}, As, a_metadata);
    load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_B, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
        b_buffer_next, BUFFER_SIZE, g.b, {0, 0, col, 1}, Bs, b_metadata);

    if (warp_col / 4 == 0) {
        __builtin_amdgcn_s_barrier();
    }
    asm volatile("s_waitcnt vmcnt(0)");


    // #pragma unroll 2
    for (int tile = 0; tile < num_tiles; ++tile) {

        for (int shared_slice = 0; shared_slice < num_shared_slices; ++shared_slice) {
            const int next_k_offset = tile * num_shared_slices + shared_slice + 1;
            const bool should_load = (next_k_offset < num_tiles * num_shared_slices);

            // Start loading NEXT data to registers 
            if (should_load) {
                load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_A, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
                    a_buffer_next, BUFFER_SIZE, g.a, {0, 0, row, next_k_offset}, As, a_metadata);
                
                load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_B, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
                    b_buffer_next, BUFFER_SIZE, g.b, {0, 0, col, next_k_offset}, Bs, b_metadata);
            }

            __builtin_amdgcn_s_barrier();

            
            // Compute on CURRENT data in shared memory
            #pragma unroll 
            for (int slice = 0; slice < num_slices; ++slice) {
                kittens::load_async(b_reg_1, subtile_inplace<REG_BLOCK_M, DOT_SLICE>(Bs, {warp_col + 2, slice}));
                kittens::load(a_reg_0, subtile_inplace<REG_BLOCK_M, DOT_SLICE>(As, {warp_row, slice}));
                kittens::load(b_reg_0, subtile_inplace<REG_BLOCK_N, DOT_SLICE>(Bs, {warp_col, slice}));
                mma_ABt(C_accum[0], a_reg_0, b_reg_0, C_accum[0]);
                mma_ABt(C_accum[1], a_reg_0, b_reg_1, C_accum[1]);
            }
            asm volatile("s_waitcnt vmcnt(1)");

            // Now wait for loads and write to shared memory
            if (should_load) {
                store_registers_to_shared<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, NUM_THREADS>(
                    a_buffer_next, As, a_metadata);
                store_registers_to_shared<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, NUM_THREADS>(
                    b_buffer_next, Bs, b_metadata);
            }
        }
    }

    if (warp_col / 4 == 1) {
        __builtin_amdgcn_s_barrier();
    }

    store(g.c, C_accum[0], {0, 0, row * 4 + warp_row, col * 4 + warp_col});
    store(g.c, C_accum[1], {0, 0, row * 4 + warp_row, col * 4 + warp_col + 2});
}

void dispatch_micro(micro_globals g) {
    unsigned long mem_size = g.dynamic_shared_memory();
    hipFuncSetAttribute((void*)micro_tk, hipFuncAttributeMaxDynamicSharedMemorySize, mem_size);
    micro_tk<<<g.grid(), g.block(), mem_size>>>(g);
    hipDeviceSynchronize();
}

PYBIND11_MODULE(tk_kernel, m) {
    m.doc() = "tk_kernel python module";
    py::bind_kernel<micro_tk>(m, "micro_tk", &micro_globals::a, &micro_globals::b, &micro_globals::c); 
    py::bind_function<dispatch_micro>(m, "dispatch_micro", &micro_globals::a, &micro_globals::b, &micro_globals::c);
}


This is my starting point



Assistant: 

I see you've already made some changes from the original - you've separated REG_BLOCK into REG_BLOCK_M and REG_BLOCK_N (both BLOCK_SIZE/8 = 16), and you have 8 accumulators allocated, but the compute loop is still only using the first 2 accumulators. Let me help you complete the refactoring with minimal changes to make use of all 8 accumulators:

#include "kittens.cuh"
#include "pyutils/pyutils.cuh"
#include "utils.cpp"
using namespace kittens;

#define NUM_WARPS 8
#define NUM_THREADS (kittens::WARP_THREADS * NUM_WARPS)

constexpr int BLOCK_SIZE       = 128;  
constexpr int K_STEP           = 256;
constexpr int DOT_SLICE_SHARED = 64;
constexpr int REG_BLOCK_M      = BLOCK_SIZE / 8; 
constexpr int REG_BLOCK_N      = BLOCK_SIZE / 8;
constexpr int DOT_SLICE        = 16;

#define M 8192
#define K 8192
#define N 8192

using _gl_A = gl<bf16, -1, -1, -1, -1>;
using _gl_B = gl<bf16, -1, -1, -1, -1>;
using _gl_C = gl<float, -1, -1, -1, -1>;

using G = kittens::group<8>;

struct micro_globals {
    _gl_A a;
    _gl_B b;
    _gl_C c;
    dim3 grid()  { return dim3(N / BLOCK_SIZE, M / BLOCK_SIZE); } 
    dim3 block() { return dim3(NUM_THREADS); } 
    size_t dynamic_shared_memory() { return 65536; }
};

__global__ __launch_bounds__(NUM_THREADS, 1)
void micro_tk(const micro_globals g) {
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);
    st_bf<BLOCK_SIZE, DOT_SLICE_SHARED> (&As) = al.allocate<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>();
    st_bf<BLOCK_SIZE, DOT_SLICE_SHARED> (&Bs) = al.allocate<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>();

    rt_bf<REG_BLOCK_M, DOT_SLICE> a_reg_0, a_reg_1, b_reg_0, b_reg_1;
    rt_fl<REG_BLOCK_M, REG_BLOCK_N, ducks::rt_layout::col> C_accum[8];
    for (int i = 0; i < 8; i++) { zero(C_accum[i]); }

    // Small register buffers for pipelining
    constexpr int BUFFER_SIZE = 64;
    float4 a_buffer_next[BUFFER_SIZE];
    float4 b_buffer_next[BUFFER_SIZE];
    int a_metadata[3], b_metadata[3];

    const int row = blockIdx.y;
    const int col = blockIdx.x;

    const int warp_id = kittens::warpid();
    const int warp_row = warp_id / 2;
    const int warp_col = warp_id % 2;

    const int num_tiles = K / K_STEP;
    const int num_shared_slices = K_STEP / DOT_SLICE_SHARED;
    const int num_slices = DOT_SLICE_SHARED / DOT_SLICE;

    // Load first tile into shared memory
    G::load(As, g.a, {0, 0, row, 0});
    G::load(Bs, g.b, {0, 0, col, 0});

    // Pre-load second tile into register bufers 
    load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_A, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
        a_buffer_next, BUFFER_SIZE, g.a, {0, 0, row, 1}, As, a_metadata);
    load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_B, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
        b_buffer_next, BUFFER_SIZE, g.b, {0, 0, col, 1}, Bs, b_metadata);

    if (warp_col / 4 == 0) {
        __builtin_amdgcn_s_barrier();
    }
    asm volatile("s_waitcnt vmcnt(0)");

    // #pragma unroll 2
    for (int tile = 0; tile < num_tiles; ++tile) {

        for (int shared_slice = 0; shared_slice < num_shared_slices; ++shared_slice) {
            const int next_k_offset = tile * num_shared_slices + shared_slice + 1;
            const bool should_load = (next_k_offset < num_tiles * num_shared_slices);

            // Start loading NEXT data to registers 
            if (should_load) {
                load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_A, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
                    a_buffer_next, BUFFER_SIZE, g.a, {0, 0, row, next_k_offset}, As, a_metadata);
                
                load_global_to_registers<2, false, st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, _gl_B, coord<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>>, NUM_THREADS>(
                    b_buffer_next, BUFFER_SIZE, g.b, {0, 0, col, next_k_offset}, Bs, b_metadata);
            }

            __builtin_amdgcn_s_barrier();

            // Compute on CURRENT data in shared memory
            #pragma unroll 
            for (int slice = 0; slice < num_slices; ++slice) {
                // Load A blocks (2x2 grid of 16x16 blocks)
                kittens::load(a_reg_0, subtile_inplace<REG_BLOCK_M, DOT_SLICE>(As, {warp_row * 2, slice}));
                kittens::load(a_reg_1, subtile_inplace<REG_BLOCK_M, DOT_SLICE>(As, {warp_row * 2 + 1, slice}));
                
                // Load first pair of B blocks
                kittens::load(b_reg_0, subtile_inplace<REG_BLOCK_N, DOT_SLICE>(Bs, {warp_col * 2, slice}));
                kittens::load_async(b_reg_1, subtile_inplace<REG_BLOCK_N, DOT_SLICE>(Bs, {warp_col * 2 + 1, slice}));
                
                // First 4 accumulations: a_reg_0 with both b blocks, a_reg_1 with both b blocks
                mma_ABt(C_accum[0], a_reg_0, b_reg_0, C_accum[0]);  // [0,0]
                mma_ABt(C_accum[1], a_reg_0, b_reg_1, C_accum[1]);  // [0,1]
                mma_ABt(C_accum[2], a_reg_1, b_reg_0, C_accum[2]);  // [1,0]
                mma_ABt(C_accum[3], a_reg_1, b_reg_1, C_accum[3]);  // [1,1]
                
                // Load second pair of B blocks (offset by +4 for the second column pair)
                kittens::load(b_reg_0, subtile_inplace<REG_BLOCK_N, DOT_SLICE>(Bs, {warp_col * 2 + 4, slice}));
                kittens::load_async(b_reg_1, subtile_inplace<REG_BLOCK_N, DOT_SLICE>(Bs, {warp_col * 2 + 5, slice}));
                
                // Next 4 accumulations: same A blocks with second pair of B blocks
                mma_ABt(C_accum[4], a_reg_0, b_reg_0, C_accum[4]);  // [0,2]
                mma_ABt(C_accum[5], a_reg_0, b_reg_1, C_accum[5]);  // [0,3]
                mma_ABt(C_accum[6], a_reg_1, b_reg_0, C_accum[6]);  // [1,2]
                mma_ABt(C_accum[7], a_reg_1, b_reg_1, C_accum[7]);  // [1,3]
            }
            asm volatile("s_waitcnt vmcnt(1)");

            // Now wait for loads and write to shared memory
            if (should_load) {
                store_registers_to_shared<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, NUM_THREADS>(
                    a_buffer_next, As, a_metadata);
                store_registers_to_shared<st_bf<BLOCK_SIZE, DOT_SLICE_SHARED>, NUM_THREADS>(
                    b_buffer_next, Bs, b_metadata);
            }
        }
    }

    if (warp_col / 4 == 1) {
        __builtin_amdgcn_s_barrier();
    }

    // Store all 8 sub-blocks with proper indexing
    store(g.c, C_accum[0], {0, 0, row * 8 + warp_row * 2, col * 8 + warp_col * 2});         // [0,0]
    store(g.c, C_accum[1], {0, 0, row * 8 + warp_row * 2, col * 8 + warp_col * 2 + 1});     // [0,1]
    store(g.c, C_accum[2], {0, 0, row * 8 + warp_row * 2 + 1, col * 8 + warp_col * 2});     // [1,0]
    store(g.c, C_accum[3], {0, 0, row * 8 + warp_row * 2 + 1, col * 8 + warp_col * 2 + 1}); // [1,1]
    store(g.c, C_accum[4], {0, 0, row * 8 + warp_row * 2, col * 8 + warp_col * 2 + 4});     // [0,2]
    store(g.c, C_accum[5], {0, 0, row * 8 + warp_row * 2, col * 8 + warp_col * 2 + 5});     // [0,3]
    store(g.c, C_accum[6], {0, 0, row * 8 + warp_row * 2 + 1, col * 8 + warp_col * 2 + 4}); // [1,2]
    store(g.c, C_accum[7], {0, 0, row * 8 + warp_row * 2 + 1, col * 8 + warp_col * 2 + 5}); // [1,3]
}

void dispatch_micro(micro_globals g) {
    unsigned long mem_size = g.dynamic_shared_memory();
    hipFuncSetAttribute((void*)micro_tk, hipFuncAttributeMaxDynamicSharedMemorySize, mem_size);
    micro_tk<<<g.grid(), g.block(), mem_size>>>(g);
    hipDeviceSynchronize();
}

PYBIND11_MODULE(tk_kernel, m) {
    m.doc() = "tk_kernel python module";
    py::bind_kernel<micro_tk>(m, "micro_tk", &micro_globals::a, &micro_globals::b, &micro_globals::c); 
    py::bind_function<dispatch_micro>(m, "dispatch_micro", &micro_globals::a, &micro_globals::b, &micro_globals::c);
}

The key changes I made to complete your refactoring:

Added a_reg_1: Now we have two A register blocks to handle the 2x2 subdivision
Updated the compute loop:

Load 2 A blocks (a_reg_0, a_reg_1) for the current warp's row pair
Load 2 B blocks at a time for each column pair
Compute all 8 combinations systematically


Fixed the indexing:

A blocks: warp_row * 2 and warp_row * 2 + 1
B blocks: warp_col * 2, warp_col * 2 + 1 for first pair, then warp_col * 2 + 4, warp_col * 2 + 5 for second pair


Updated store operations:

Changed from row * 4 to row * 8 and col * 4 to col * 8 to account for the finer subdivision
Added proper offsets for all 8 sub-blocks



This now properly utilizes all 8 accumulators and handles the 2x2 subdivision of each original 32x32 block into four 16x16 sub-blocks.RetryClaude can make mistakes. Please double-check responses.

User: 

This was not helpful.






