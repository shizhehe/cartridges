You’ll be notified for every new message in this conversation. Change this setting


simran
  2:48 PM
hi!! making a gc with will who's leading the port
2:49
@jmonsalv
 sent a bunch of resources to me -- forwarded them to will
:thankyou:
1

2:49
we can use this chat
:+1:
1



jmonsalv
  3:34 PM
Sounds good
3:34
I also got some new resources I was gonna share in the other channel, but I am trying to go through them and organize them in a better way
:thankyou:
1

3:35
Still, I do not know 
@William Hu
 question from today. Maybe Tom will be able to provide more info on it soon
3:36
And it seems like Will is going in the right direction. He did mention some issues with some intrinsics. My guess is that the compiler hipcc is using may not have all the intrinsics in the version you're running on. Under the hood these are all LLVM compilers, but there are differences between them
3:38
My gh is josemonsalve2


William Hu
  3:38 PM
yeah, that's my initial guess as well! I assumed you should be able to just pass in the right architecture gfx942 and the compiler itself would be able to have the right intrinsic definitions. Maybe it's an issue with which version of hipcc?


jmonsalv
  3:38 PM
Btw, Tom mentioned today CK:
https://github.com/ROCm/composable_kernel/blob/develop/example/01_gemm/gemm_wmma_bf16.cpp
gemm_wmma_bf16.cpp
// SPDX-License-Identifier: MIT
// Copyright (c) 2018-2024, Advanced Micro Devices, Inc. All rights reserved.

#include "common.hpp"

Show more
<https://github.com/ROCm/composable_kernel|ROCm/composable_kernel>ROCm/composable_kernel | Added by https://scalingintelligence.slack.com/services/B05NUUFG5GA
:+1:
1



William Hu
  3:38 PM
will look into this!!


jmonsalv
  3:38 PM
That's just an example, but that seems to be another way of creating kernels. The reality is that it all goes down to LLVM back end, so you can even generate kernels directly with clang if you want
3:39
The dispatch would be easier to do via HSA in that case, because the HIP runtime adds extra arguments that are not generated if you do it directly
:+1:
1



jmonsalv
  3:47 PM
image.png
 
image.png
3:47
Can you guys see this canvas


William Hu
  3:56 PM
yep!


jmonsalv
  7:08 PM
The CK docs is horrible, so I am trying to find if this is a good resource to learn how MM has been created before.
Here is a video about it. This is better than tryin to read the docs
7:08
https://www.youtube.com/watch?v=-732zELVbpU
YouTubeYouTube | GPU MODE
Lecture 25: Speaking Composable Kernel (CK) 

:thankyou:
1

7:11
It seems like this folder is quite relevant.
https://github.com/ROCm/composable_kernel/blob/65f182d617b0d2889b778b4d8773b99ea4d[…]include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma_impl.hpp
warp_gemm_attribute_mfma_impl.hpp
#define DISPATCH_MFMA_(mfma_, dmod_, amod_, bmod_, cmod_)       \
    if constexpr(post_nop_)                                     \
    {                                                           \
        asm volatile(mfma_ " %0, %1, %2, %3 ; yyy\n"            \
                           "s_nop 3"                            \
Show more
<https://github.com/ROCm/composable_kernel|ROCm/composable_kernel>ROCm/composable_kernel | Added by https://scalingintelligence.slack.com/services/B05NUUFG5GA
:thankyou:
1



William Hu
  8:01 PM
Thanks! I'll take a look.


eddier
  1:14 PM
was added to the conversation by jmonsalv
.


jmonsalv
  1:14 PM
All, I am adding Eddie. He's been helping me get more resources
1:15
My take with CK is:
There are some similarities to how TK manages tiling and stuff like that.
It could be useful to review as reference
Documentation is bad, and project is not that easy to navigate. So it may also be confusing
(edited)
:thankyou:
1

1:16
Let us know if it is useful


jmonsalv
  4:48 PM
How is it going guys? I know you're busy with the other deadline, but just checking in case there are any additional thoughts or things that are showstoppers


William Hu
  5:13 PM
Thanks for checking in! I've mostly been writing hip kernels the last week or so (was able to get a gemm close to triton performance using amd mfma intrinsics). Currently working on exposing that in TK.


jmonsalv
  7:12 PM
That's awesome to hear! Great progress. Looking forward to hearing from this on Monday
:+1:
1



jmonsalv
  4:12 PM
Hi guys!
4:16
I was a bit busy late May, but I'd love to restart the conversation now that your deadline is over


simran
  4:17 PM
@William Hu
 and I are having a hack session on Thursday! We could all zoom/hack together as well.


jmonsalv
  4:17 PM
I'd love to set up a sync call to understand the TK challenges for AMD right now?


simran
  4:17 PM
We'd like to get out the TK repo ASAP!


jmonsalv
  4:17 PM
Yes! let's schedule something up?


simran
  4:17 PM
I think it's mainly been a lack of prioritization as the bottleneck, but we want to get this out as soon as next week.


jmonsalv
  4:18 PM
We may not be able to stay for the full day hacking session, but would love to see what's going on
:+1:
1

4:19
What time would you think is best?


jmonsalv
  10:49 AM
@simran
 what time will you want me to join tomorrow?
10:49
I mean Thursday


William Hu
  10:51 AM
^ I’ll be available to hack for most of the work day


jmonsalv
  10:52 AM
I don't want to interrupt too much, since you're the TK experts, I doubt I will be able to provide much on that front (I am trying to catch up before the meeting). But I want to get the current blockers, and see if I can answer questions that may come up during the day, or direct them to the appropriate people
10:53
I want to also understand the general strategy you're following. My read of the TK source code tells me there's a lot of PTX dependencies, and I am not sure how to overcome the lack of TMAs in our current hardware. So I want to see how that's being done


jmonsalv
  10:54 AM
If you have any source code I can take a look at before the meeting, that will also be really useful


6 replies
Last reply 10 days agoView thread


simran
  5:03 PM
apologies was doing my defense yesterday and missed this - yes tomorrow am flexible


jmonsalv
  5:03 PM
WOW!!! Congrats!!
5:03
How did it go?
5:03
:tada:


simran
  5:03 PM
done passed


simran
  5:03 PM
@William Hu
 you're coming into hack right?

2 replies
Last reply 9 days agoView thread


simran
  5:04 PM
and we can zoom you in at some point 
@jmonsalv
:white_check_mark:
1



simran
  5:04 PM
also what do folks here think of mojo?

1 reply
9 days agoView thread


simran
  5:05 PM
and one more question for the amd experts -- do you see any hardware-level bottlenecks towards implementing this type of idea on your chips: https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles
or opportunities to do even better?


jmonsalv
  5:05 PM
Chris Lattner's Mojo?


simran
  5:05 PM
yes


jmonsalv
  5:05 PM
I've not dive deep but it's an interesting idea. I just don't know if it will gain enough traction (edited) 


simran
  5:06 PM
they've released some new features i think for gpu recently


jmonsalv
  5:06 PM
So far I've not seen strong users
5:07
I read the Megakernel stuff. It's really interesting. I am taking my research right now more towards device side dispatches and improvements to the hardware scheduler, but these ideas are really good
5:07
Many of which could be implemented via LLVM-IR. So there should be opportunities also in products like IREE or Triton
5:08
I believe the beauty of TK is in the runtime aspect itself. But I would love to hear your take


simran
  11:38 AM
does amd have dedicated memory load/store instructions?
11:38
similar to memcpy_async, tma, etc.
11:38
for asynchronous ops


jmonsalv
  11:39 AM
My understanding is that we don't have a tma for the current Gen
11:39
NDA info: (Comming up)


simran
  11:39 AM
even ampere pre-hopper had some async instructions though right


jmonsalv
  11:39 AM
Let me see if we have anything for DMA


simran
  11:39 AM
even if no dedicated tma-style circuit
11:40
so maybe something like that?


jmonsalv
  11:40 AM
That's right. Let me check what the name is


simran
  11:44 AM
thank you!


jmonsalv
  11:52 AM
Yup, I think I was correct.
There is a DMA engine on the device, but that's mostly for device-device device-host communication. Not so much for device-sharedmem. I am trying to see if there is something that's not documented well about it.
Typical types of transfers that a DMA may control are:
Memory to memory transfers (internal to external)
Transfers from I/O devices to memory
Transfers from memory to I/O devices
Transfers from/to communications ports and memory
Transfers from/to serial ports and memory
I also asked Mohammad about this:
We dont have async instructions in MI300 or 350/355, we have them in <next gen>.
:thankyou:
2

11:53
This sucks, I know
11:53
That's what my biggest concern is for TK on our devices


simran
  11:54 AM
Ok thank you this is very helpful!


simran
  12:00 PM
Could we also get this for the mi325 (not sure how much is public)
12:00
Screenshot 2025-06-12 at 12.00.39 PM.png
 
Screenshot 2025-06-12 at 12.00.39 PM.png
12:01
Sizes/speeds of the cache hierarchy?
Last Level Cache (LLC)
256 MB
Dedicated Memory Size
256 GB
Dedicated Memory Type
HBM3E
Infinity Cache
Yes
Memory Interface
8192-bit
Memory Clock
6 GHz
Peak Memory Bandwidth
6 TB/s
Memory ECC Support
Yes (Full-Chip
^what i found online
https://www.amd.com/en/products/accelerators/instinct/mi300/mi325x.html (edited) 

AMDAMD
AMD Instinct™ MI325X Accelerators
AMD Instinct™ MI325X GPU accelerators set new AI performance standards, delivering incredible performance and efficiency for training and inference. (92 kB)
https://www.amd.com/en/products/accelerators/instinct/mi300/mi325x.html



jmonsalv
  12:06 PM
Yup, let me confirm


jmonsalv
  12:20 PM
I'm asking the product side, because I do not find any docs in the internal docs I have access to


simran
  12:21 PM
Thank you


jmonsalv
  12:25 PM
I have requested access to more docs (there is an access control company wide, and products tend to have their own protection) to see if I can get more info myself


simran
  2:15 PM
Thank you!
2:15
We're working on a matmul rn in tK so if you guys have input on the strategies used for the pytorch impl, we're working on closing the gap to that


simran
  4:22 PM
Also on the front of num_streaming_multiprocessors, etc.


simran
  12:59 PM
could you guys confirm:
what is the memory hierarchy
what sorts of asynchronous support exists
you mentioned a next version of the gpu might have load-async. does mi355 count as a next version, or do you mean a gpu after that even?
are there mbarrier equivalents / synchronization primitives?
A huddle happened  4:05 PM
You, William Hu, and jmonsalv were in the huddle for 14m.


jmonsalv
  4:14 PM
https://github.com/ROCm/hipBLAS/blob/develop/library/src/amd_detail/hipblas.cpp
hipblas.cpp
<https://github.com/ROCm/hipBLAS|ROCm/hipBLAS>ROCm/hipBLAS | Added by https://scalingintelligence.slack.com/services/B05NUUFG5GA
4:16
https://github.com/ROCm/rocBLAS
ROCm/rocBLAS
Next generation BLAS implementation for ROCm platform
Website
https://rocm.docs.amd.com/projects/rocBLAS/en/latest/
Stars
382
Added by https://scalingintelligence.slack.com/services/B05NUUFG5GA
4:16
https://github.com/ROCm/rocBLAS/tree/develop/library/src/blas3
4:17
https://github.com/ROCm/rocBLAS/blob/develop/library/src/blas3/rocblas_dgmm_kernels.cpp
4:20
Tasks:
I set up meeting with Muhammad and/or Ryan
You send me how to reproduce what you have so I can think of possibilities
I run what you send me and if I can think of anything I can report
(edited)


jmonsalv
  4:36 PM
MI325X
HBM 256 GB HBM3e
6.0 TB/s
Peak GF16 1307
Peak FP8 2615
4:36
Trying to get the LDS stuff
4:38
MI350X
HBM 2889 GB HBM3e
Theoretical Peak BW 8 TB/s
IDK if I can share the Peak performance for this one
4:39
MI355X
Same memory
4:43
MI355X
32 Compute Units per XCD
Total CU 256
L1 32KB
L2 (Shared by all CUs in the XCD) 4MB
MALL LLC 256 MB Per HBM stack (2XCDs)
LDS 160 KB (edited) 


jmonsalv
  4:50 PM
MI325X
38 CUs per XCD
Total CUs 304 (across XCDs)
Memory 256 GB HBM3e
6TB/s Mem BW
L1 32KB
L2 4 MB (Per xcd)
MALL LLC 256 MB (Pair of XCDs)
LDS 64 KB (edited) 
4:52
The BW and latency between memories should be the same across gens. The main difference is memory density


simran
  4:52 PM
Thanks so much! Whats register file size again?


jmonsalv
  4:56 PM
"VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is not required to be equal numbers of both types."
This is from the MI300 ISA spec
https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-se[…]tures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf
4:56
Let me know if that's enough


William Hu
  6:43 PM
Screenshot 2025-06-13 at 6.42.27 PM.png
 
Screenshot 2025-06-13 at 6.42.27 PM.png
6:44
We profiled our Triton kernel on the left (400 TFLOPS) against our TK on the right one (200 TFLOPS). Triton gets much higher read bandwidth and remote read traffic is non-zero. Was wondering if yall had any ideas what this points to/suggests. Thanks! (edited) 


William Hu
  11:23 AM
Hi again, was wondering if there was another profiling tool (something like the Radeon gpu profiler) that exposes profiled ISA on cdna3
11:27
^ or if there’s a compiler flag we can set to have hipcc emit the ISA


jmonsalv
  9:59 AM
Hey sorry, I was busy over the weekend
10:01
Let me see if I understand
10:01
you want to see the assembly?
10:02
Let me send you the flag (edited) 
10:02
From the top of my head I think it is --save-temps
10:02
But I am confirming


jmonsalv
  10:10 AM
HIP-clang (hipcc with HIP_PLATFORM=clang) accepts clang options, include a -save-temps flag (e.g. hipcc -save-temps foo.cpp)
:thankyou:
1

10:13
The llvm-ir is also quite useful there


simran
  10:15 AM
Can we meet with the people you told us about
10:15
On kernel logic


jmonsalv
  10:15 AM
Yes, I'm going to set up a meeting this week. They said they're good
10:16
give ma  sec


simran
  10:16 AM
Is it possible to do it soon, like today?


jmonsalv
  10:16 AM
@eddier
 sent me this
amd_challenge_solutions/fp8_gemm/fp8_gemm.md at main · seb-v/amd_challenge_solutions
This is pretty darn simple but this won the AMD GEMM challenge.
fp8_gemm.md
# FP8 GEMM Problem Solution
## Main idea
I used a classical 2-level tiling approach. Check my blog post for more details, but here are the main steps:
1. Loop over the K dimension:  
  • Load a tile of matrix A and matrix B into LDS, using all threads in the workgroup.  
  • Load a sub-tile into VGPRs and perform smaller matrix multiplications at the wave level.
2. Accumulate results into local VGPRs.
[](picture0.jpg)
## LDS tiling
My solution uses a single tiling configuration across all test cases: 128×128 for each matrix. This choice was driven by:
• 128 stride matches the scaling tile size of the problem (As & Bs).
• Two FP8 tiles of 128×128 occupy 32KB of LDS, which is exactly half of the 64KB avai… Show more
<https://github.com/seb-v/amd_challenge_solutions|seb-v/amd_challenge_solutions>seb-v/amd_challenge_solutions | Added by https://scalingintelligence.slack.com/services/B05NUUFG5GA


2 replies
Last reply 4 days agoView thread


jmonsalv
  10:18 AM
Muhammad is OoO today. He's on vacation I think
10:19
The next open in their calendar is Friday 10 AM


simran
  10:20 AM
Maybe we can meet with just one of them?


jmonsalv
  10:22 AM
image.png
 
image.png
10:22
IDK if they have a deliverable or something. They work together
10:22
But this is the view
10:22
I'm going to lock friday still


simran
  10:34 AM
thanks!


jmonsalv
  3:02 PM
I'm still digging to see if I can get some other person to talk


jmonsalv
  12:24 PM
Guys a couple of questions.
Have you compared TK to Composable Kernels? We were asked what the differences are, and I believe TK has a more bottom up approach with a strong emphasis on the memory abstraction. But do you have more info on this? If not that's ok


jmonsalv
  12:25 PM
The second question is, did you write something to be able to reproduce your comparison experiments? I wanted to give it a try


3 replies
Last reply 3 days agoView thread


jmonsalv
  12:26 PM
We continue to get more involvement from other people


simran
  12:27 PM
We haven't compared to composable kernels


jmonsalv
  12:27 PM
Not in results, but in approach


simran
  12:30 PM
Currently our GEMM is 415 TFLOPs vs. 700 for PyTorch.
We need to improve pipelining
We need to reduce bank conflicts
If you are spending cycles/have teammembers who can help with these aspects, this is the major place to improve
:heart:
1



jmonsalv
  12:33 PM
I can get info on the bank config. I've seen those docs. But for pipelining, I am not the one person. I will think of more people


simran
  12:34 PM
Amazing that would be very helpful


jmonsalv
  12:41 PM
Are these LDS conflicts or Cache conflicts?


jmonsalv
  12:41 PM
I'd assume LDS

1 reply
3 days agoView thread


jmonsalv
  12:43 PM
So the needed info would be how is this mapped in hw?

2 replies
Last reply 3 days agoView thread


jmonsalv
  12:44 PM
There's also VGPR bank conflicts, just in case :laughing:

1 reply
3 days agoView thread


William Hu
  12:45 PM
If there are any good resources/examples on how to use buffer_load or global_load instructions please let us know too!
:white_check_mark:
1


1 reply
3 days agoView thread


jmonsalv
  12:50 PM
I'm trying to find the arch docs on this, to see if I can share something
12:51
I assume your info is what is in the MI300 ISA doc


jmonsalv
  12:51 PM
image.png
 
image.png
:white_check_mark:
1


1 reply
3 days agoView thread


simran
  12:54 PM
yeah we've already seen that
12:54
we were looking for lower-level insights
12:54
but nw


jmonsalv
  12:54 PM
Yup, on it
12:54
Just making sure


stwinata
  3:33 PM
was added to the conversation by jmonsalv
.


jmonsalv
  3:33 PM
Guys, Adding Stanley to the group
:heart:
1

3:33
He may have some feedback for your questions better than me


simran
  3:33 PM
hey thanks! our other labmate connected us as well!


jmonsalv
  3:34 PM
Oh nice!
3:34
It seems like we're having the right discussions internally then (edited) 


simran
  3:57 PM
Awesome!! We're getting closer. We're roughly at 490 TFLOPs and no bank conflicts anymore, better cache reuse
3:57
We think we're just missing something on the algorithm side
3:59
We've tried dot-slicing (cc/
@stwinata
): https://github.com/willhu-jpg/AMD-benchmarking-harness/blob/main/kernels/TK/bf16fp32/transpose_matmul/kernel_v3.cpp
provided a lift over no-dot slicing, but haven't combined it with register pre-fetching as yet
We are trying to refactor our current best kernel (https://github.com/willhu-jpg/AMD-benchmarking-harness/blob/main/kernels/TK/bf16fp32/transpose_matmul/kernel_v2.cpp)
we want to use 8 waves like your amd reference
we want to set it up so that 4 waves are loading while 4 waves are computing, and then swap their roles.
@William Hu
 maybe you can send a thread trace of our best attempt so far when you get a chance. Feel free to add anything I missed above.
:+1:
2



jmonsalv
  4:04 PM
image.png
 
image.png
4:05
Can you guys have a few slides on TK so they understand the features


William Hu
  4:17 PM
Zip
 

kernel_v2_trace.zip
Zip


muhosama
  9:01 PM
was added to the conversation by jmonsalv
.


jmonsalv
  9:02 PM
@channel Adding 
@muhosama
 to this chat. He's gonna help out with the performance issues
:thankyou:
2

9:03
See you all tomorrow


muhosama
  9:27 PM
Hi all! I am a fan of TK, I work on GPU-GEMMs and everything linear algebra. Looking forward to tomorrow's meeting! Will use this as a follow-up chat if you have any questions. :relaxed:
:thankyou:
2

9:28
Thanks 
@jmonsalv
 for connecting!


jmonsalv
  11:02 AM
NExt steps seem to be:
You're going to try to adapt these scheduling ideas to TK
More questions go through this channel
I will try to share the video. Let me post some of the messages from teams here
No follow up meeting scheduled yet
:+1:
1

11:03
From Ryan:
tensilelite Code :
https://github.com/ROCm/hipBLASLt/tree/develop/tensilelite
Example of a handwritten hipblaslt kernel (We don't always use the handwritten ones) : https://github.com/ROCm/hipBLASLt/blob/develop/tensilelite/Tensile/CustomKernels/Custom_Cijk_Alik_Bljk_BBS_BH_MT256x256x64_MI16x16x1_UserArgs_shortname0_gfx950.s
Custom_Cijk_Alik_Bljk_BBS_BH_MT256x256x64_MI16x16x1_UserArgs_shortname0_gfx950.s
<https://github.com/ROCm/hipBLASLt|ROCm/hipBLASLt>ROCm/hipBLASLt | Added by https://scalingintelligence.slack.com/services/B05NUUFG5GA
11:03
I need to hop on my flight, but I think chiplet swizzling is probably what Muhammad is covering. There is a triton example of that index transformation here (hopefully you have access to it): https://github.com/ROCm/triton/blob/ba25d4d5bdb4f42886f8c43b9ab4718c6abe7061/python/perf-kernels/streamk/persistent_gemm.py#L34-L35
persistent_gemm.py
    if NUM_XCDS != 1:
        pid = (pid % NUM_XCDS) * (NUM_SMS // NUM_XCDS) + (pid // NUM_XCDS)
<https://github.com/ROCm/triton|ROCm/triton>ROCm/triton | Added by https://scalingintelligence.slack.com/services/B05NUUFG5GA
11:03
(this also happens in hipblasLT as the parameter WorkGroupMappingXCC iirc)
11:04
This is the blog shared by Muhammad:
rocm.blogs.amd.com/software-tools-optimization/compute-memory-modes/README.html


jmonsalv
  11:25 AM
[ThunderKittens] Support session on MM-20250620_100959-Meeting Recording.mp4
 
At a low low level as well as any sorts of. You know, basic hex for zeros ones, if we want to say like, you know, tiled up ones or something, and then… View transcript


William Hu
  11:40 AM
@stwinata
 Thanks so much for taking the time today! I was a bit curious about how best to partition work between 8 warps (say we have 2 subgroups each with 4 warps). What portion of the C tile should each subgroup be responsible for? Do we split work across the K-dimension and do a reduction at the end? Do we split across the N or M dimension and have memory be duplicated across the subgroups?
:heart:
1



stwinata
  1:37 PM
Just N and M dimension across the subgroups so they are all parallel to each other
1:38
4 in M and 2 in N, but should be easy to flip the ordering


William Hu
  1:39 PM
I see!
1:40
does it ever make sense to do 2 in M and 2 in N?


William Hu
  1:40 PM
so workgroup 1 loads in A[0:1] and B[0:1]

1 reply
Today at 2:06 PMView thread


William Hu
  1:40 PM
while workgroup 2 loads in A[2:3] and B[2:3]


stwinata
  1:40 PM
Yes if we are not doing ping-pong style (2 wave/warp in a workgroup) gemm. that would come out to 4 wave/warp per workgroup however
1:41
ah workgroup/thread block 2x2 is OK (edited) 


William Hu
  1:43 PM
I'm currently thinking of doing something like this
wg 1 does a global to shared load of A[0:1] and B[0:1] while wg2 waits
When wg1 is done loading, wg2 kicks of a global to shared load of A[2:3] and B[2:3] and wg1 computes the upper left quadrant of C.
When wg2 is done loading, wg2 computes the lower right quadrant of C while wg1 computes the upper right quadrant of C
wg2 then computes the lower left quadrant of C while wg1 loads in A[0:1] and B[2:3]
1:44
or is that too convoluted?
1:45
C is broken into 4 quadrants. wg1 is always responsible for the upper two quadrants while wg2 is always responsible for the lower two (edited) 


stwinata
  1:48 PM
here, is wg here wave-group or workgroup?


William Hu
  1:49 PM
wave group
1:49
8 waves split into 2 wave groups each with 4 waves
New


William Hu
  2:00 PM
If it helps to visualize ^
IMG_5846
 
IMG_5846


William Hu
  2:49 PM
yeah this implementation is incredibly slow unfortunately ^. too much scratch space used I think


