This is the very beginning of your direct message history with 
@Dan Fu
, 
@Neil Movva
, and 
@William Hu
You’ll be notified for every new message in this conversation. Change this setting


Dan Fu
  1:26 PM
Hey jokers!
1:26
I think we only get gpu-10 on the AMD node
:+1:
1

1:26
So let’s coordinate
1:26
I think 
@William Hu
 currently has it reserved
1:26
I think we can use slurm to log into it and share the GPUs though


William Hu
  2:19 PM
For profiling: https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/install/core-install.html#rocprofiler
rocprof-compute profile -n even_faster_matmul --no-roof -- python scripts/eval.py kernel_type=THUNDERKITTEN kernel=even_faster_matmul AB_type=BF16 .correctness

rocprof-compute analyze -p workloads/even_faster_matmul/MI300/ --gui
rocm.docs.amd.comrocm.docs.amd.com
Installing and deploying ROCm Compute Profiler — ROCm Compute Profiler 3.1.0 documentation
ROCm Compute Profiler installation and deployment
2:20
There's a bug in
vim /opt/rocm-6.4.1/libexec/rocprofiler-compute/rocprof_compute_analyze/analysis_webui.py
line:355 change self.app.run_... to self.app.run


William Hu
  7:03 PM



Neil Movva
re: fine grained sync options on AMD, here is my current understanding
__syncthreads() == S_BARRIER
__syncwarp() == s_waitcnt 0
s_waitcnt supports finer conditions too, e.g. wait on global memory, or only on shared mem (LDS)
from section 4.3 and 4.4 of the CDNA3 ISA doc
From a private conversation | Jun 11th
7:05
is this equivalent to m_barriers then? We're looking at how to do inter-warp synchronization right now
7:05
had a call with ben and tldr is bc shared memory is so small, it's better to load straight to registers and use shared memory to communicate between warps


Dan Fu
  7:27 PM
Is that what the example does?
7:27
I am skeptical of strong claims without evidence!
7:27
Let’s build and benchmark


William Hu
  7:30 PM
The example uses shared memory as an intermediary. Threads then pull from shared memory to compute.


Simran
  7:49 PM
what example


William Hu
  7:49 PM
https://github.com/seb-v/amd_challenge_solutions/blob/main/fp8_gemm/fp8_gemm.md
GitHubGitHub
amd_challenge_solutions/fp8_gemm/fp8_gemm.md at main · seb-v/amd_challenge_solutions
Contribute to seb-v/amd_challenge_solutions development by creating an account on GitHub. (33 kB)
https://github.com/seb-v/amd_challenge_solutions/blob/main/fp8_gemm/fp8_gemm.md



Dan Fu
  9:26 PM
This seems like a counter example to the theory :)
9:27
Let’s implement this? We have the answer key!
:white_check_mark:
1



Simran
  10:42 PM
we got up to 320 tflops so far... more tomorrow (edited) 
:party-dinosaur:
2



Dan Fu
  10:46 PM
Do we need the async stuff or no?
10:46
I have another opportunity to bother them on Monday


Simran
  10:58 PM
ideally if they have it
10:59
but if they don't then nothing we can do
10:59
we think there's something algorithmic


Dan Fu
  10:21 AM
How’s our kernel?



36 replies
Last reply 4 days agoView thread


Dan Fu
  10:34 AM
Have we implemented this? https://github.com/seb-v/amd_challenge_solutions/blob/main/fp8_gemm/fp8_gemm.md
GitHubGitHub
amd_challenge_solutions/fp8_gemm/fp8_gemm.md at main · seb-v/amd_challenge_solutions
Contribute to seb-v/amd_challenge_solutions development by creating an account on GitHub. (33 kB)
https://github.com/seb-v/amd_challenge_solutions/blob/main/fp8_gemm/fp8_gemm.md

10:34
Have we benchmarked it?
10:34
Is it real?
10:35
What are the specific questions?


Dan Fu
  10:44 AM
replied to a thread:
How’s our kernel?
Ok it seems like we should just clone this kernel
View newer replies


Dan Fu
  11:36 AM
Check out “rocprof compute viewer”
:+1:
2

11:36
SIMD-level visualization of stalls


Simran
  12:00 PM
So far we're seeing that our L1 cache hits are far lower than PyTorch which tells me that I think we're not coalescing our global loads effectively


Dan Fu
  12:03 PM
How are you loading? This AMD guy just said all the loads are global -> register -> shared -> register
12:03
Is there some indirection that TK is hiding?
12:03
Or are we only doing the global -> register


Simran
  12:03 PM
We're doing global --> shared --> register on the way in


Dan Fu
  12:03 PM
Ah


Simran
  12:03 PM
register --> global on the way out


Dan Fu
  12:04 PM
I am told that global -> shared does not exist as an instruction
12:04
So probably it is going to register in between


Simran
  12:04 PM
Oh sure here's our instruction you can show them:https://github.com/willhu-jpg/ThunderKittens-HIP/blob/609c63b1235c5d231ff5497bcec97c49a1d6f601/include/ops/warp/memory/tile/global_to_shared.cuh#L19

GitHubGitHub
ThunderKittens-HIP/include/ops/warp/memory/tile/global_to_shared.cuh at 609c63b1235c5d231ff5497bcec97c49a1d6f601 · willhu-jpg/ThunderKittens-HIP
Tile primitives for speedy kernels. Contribute to willhu-jpg/ThunderKittens-HIP development by creating an account on GitHub. (102 kB)
https://github.com/willhu-jpg/ThunderKittens-HIP/blob/609c63b1235c5d231ff5497bcec97c49a1d6f601/include/ops/warp/memory/tile/global_to_shared.cuh#L19

12:05
it quickly puts it in register then shared


Simran
  12:21 PM
Did they approve this?


Dan Fu
  12:25 PM
Sorry we did a quick break for lunch
12:25
This function is running in a loop right?
12:25
What you want to do is issue a bunch of loads and then have one wait count


Simran
  12:26 PM
what do you mean by wait count


Dan Fu
  12:26 PM
s_waitcnt vmcnt(0)
12:26
This is the equivalent of async_load_wait
12:26
The wait that this one is happening it would be quite slow


Simran
  12:26 PM
had a syncthreads: https://github.com/willhu-jpg/AMD-benchmarking-harness/blob/9c33ed6ca13a67e7a29abfc2fe73bda2ceff2d7a/kernels/TK/bf16fp32/transpose_matmul/kernel.cpp#L60


Dan Fu
  12:27 PM
sync threads is a block-level sync


Simran
  12:27 PM
and no sync/wait in the main load function
12:27
true


Dan Fu
  12:27 PM
s_waitcnt vmcnt(0)\n is doing a wait


Simran
  12:27 PM
but we're having the warps load together as a group in that load function call so we need cross-warp sync


Dan Fu
  12:29 PM
The way you want to do that is:
All warps issue all their loads
All warps issue s_waitcnt (this is like a warpsync), now each warp has its data in registers
block-level sync, now all the warps have done their loads
Warps put their stuff into shared memory
(edited)


5 replies
Last reply 4 days agoView thread


Dan Fu
  12:29 PM
You want to minimize the calls to s_waitcnt


Simran
  12:29 PM
ok looking into this


Dan Fu
  12:37 PM
green is matmul
Orange is read from sram
waves are scheduled round robin 64 threads at a time
(edited)
IMG_1153
 
IMG_1153


Dan Fu
  12:37 PM
To do a pipeine, you create 8 waves


10 replies
Last reply 4 days agoView thread


Dan Fu
  12:38 PM
And then ahve wave 0 do matmul while wave 1 is loading


Simran
  12:38 PM
what tool is this and what are you profiling


Dan Fu
  12:38 PM
This is a gemm
12:38
I’m not sure what kind
12:38
I think this is the rocm profiler visualizer
12:38
I’ll try to see if we can run the actual kernel


Simran
  12:41 PM
Ok lol can we borrow that gemm as reference
12:41
or is it theirs/not yours?


Dan Fu
  12:41 PM
It’s not mine :joy:
:white_check_mark:
1

12:42
I’ll try to see if we can get it
12:42
It might be a triton kernel


Simran
  12:42 PM
also ask them how to download that visualizer!!
12:42
Screenshot 2025-06-16 at 12.42.24 PM.png
 
Screenshot 2025-06-16 at 12.42.24 PM.png


12:42
we've been using one that looks like this ^


Dan Fu
  12:43 PM
Link incoming hopefully :crossed_fingers:


Simran
  2:50 PM
Do you understand how banking works here? My understanding is CDNA has 32 banks but 64 threads/wave
2:51
So I'm not sure what pattern would lead to 0 bank conflicts?
2:52
The pytorch kernel shows 0 conflicts/access, while our kernel shows 0.5 conflicts/access which is something that would happen if we were using our old nvidia bank swizzling but now ran it with 64 threads


Dan Fu
  2:53 PM
In theory, everything is answered here: https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-se[…]tures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf
2:54
Each compute unit has a 64 kB memory space that enables low-latency communication between work-items within a work-group, or the work-items within a wavefront; this is the local data share (LDS). This memory is configured with 32 banks, each with 512 entries of 4 bytes. The shared memory contains 32 integer atomic units designed to enable fast, unordered atomic operations. This memory can be used as a software cache for predictable re-use of data, a data exchange machine for the work-items of a work-group, or as a cooperative way to enable efficient access to off-chip memory.


Simran
  2:54 PM
True I was reading "This memory is
configured with 32 banks, each with 512 entries of 4 bytes."
2:55
Yeah but figuring out how to avoid conflicts with 64 threads


Dan Fu
  2:57 PM
Can you find the source code in hipblaslt?
:white_check_mark:
1

2:57
It’s in theory all open source


Dan Fu
  7:30 PM
https://github.com/ROCm/rocprof-compute-viewer
GitHubGitHub
GitHub - ROCm/rocprof-compute-viewer
Contribute to ROCm/rocprof-compute-viewer development by creating an account on GitHub. (38 kB)
https://github.com/ROCm/rocprof-compute-viewer

7:30
This is the magic visualizer
7:31
The keys to the kingdom


William Hu
  9:29 PM
some setup instructions to setup rocprofv3:
rocprof-compute-setup.sh
 
mkdir -p rocprofiler-setup
cd rocprofiler-setup
​
git clone https://github.com/ROCm/rocprofiler-sdk.git rocprofiler-sdk-source
apt-get install libdw-dev
Click to expand inline (20 lines)
9:29
usage to get traces:
rocprofv3 --att -d <output_dir> -- <application_path>


Dan Fu
  9:13 AM
The trace of the ping-pong matmul kernel, let’s see what we can learn
Zip
 

triton_256m256n64k_pingpong
Zip
:+1:
1


1 reply
3 days agoView thread


Dan Fu
  9:13 AM
There’s also a way to build the viewer on macOS
9:14
https://rocm.docs.amd.com/projects/rocprof-compute-viewer/en/latest/install/installation.html#install-viewer
rocm.docs.amd.comrocm.docs.amd.com
Building ROCprof Compute Viewer from source — ROCprof Compute Viewer 0.1.0 documentation
ROCprof Compute Viewer is a tool for visualizing and analyzing GPU thread trace data collected with rocprofv3.


Dan Fu
  2:22 PM
Zip
 

484n8_25625664_PP.zip
Zip




Dan Fu
  9:58 AM
Instructions for taking out own traces
9:58
https://gist.github.com/raikonenfnu/e698bd2b125a823011aaad8ad5243832


Dan Fu
  9:58 AM
Should we trace our kernel?
:white_check_mark:
1


1 reply
2 days agoView thread


William Hu
  11:36 AM
The kernel's sitting around 400 TFLOPS now thanks to some optimizations Simran made on the global->shared loading side. We're looking at fixing up the swizzling pattern to remove the bank conflicts now.


Simran
  11:36 AM
+1, 415 TFLOPs


Dan Fu
  11:41 AM
@Simran
 I’ll introduce you to the AMD guy via DM on together slack
11:41
If you send him you trace he can probably help
11:41
He’s very eager


Simran
  11:41 AM
Perfect!


Simran
  3:48 PM
tech_ops_amd_xqh@aac11-slurm-controller-p:~/simran$ bash launch_docker.sh
launch_docker.sh: line 1: podman: command not found


2 replies
Last reply 2 days agoView thread


Simran
  3:48 PM
has anyone come across this recently
3:48
i don't have sudo to install podman


Simran
  4:32 PM
Update! 480 TFLOPS
:party-dinosaur:
1



Dan Fu
  4:45 PM
Nice!
4:46
What’s the hipblas target?


Simran
  4:55 PM
unfortunately like 700 tflops
4:55
getting there


Dan Fu
  4:59 PM
Cool
4:59
We’ll get there :slightly_smiling_face:
5:00
We can profile it to get the visualization right?


Simran
  5:01 PM
yes!
i added in register prefetching/overlaps with mma to get this win
but it led to a few register spills - we'll be faster if we can eliminate
i'm still using 4 waves, not 8 as the amd guy said - so hopefully that refactor gets us there


Dan Fu
  5:04 PM
Oh ya!
5:05
With only 4 then there’s no pingponging


Simran
  5:05 PM
https://github.com/willhu-jpg/AMD-benchmarking-harness/blob/main/kernels/TK/bf16fp32/transpose_matmul/kernel_v2.cpp

GitHubGitHub
AMD-benchmarking-harness/kernels/TK/bf16fp32/transpose_matmul/kernel_v2.cpp at main · willhu-jpg/AMD-benchmarking-harness
Contribute to willhu-jpg/AMD-benchmarking-harness development by creating an account on GitHub. (103 kB)
https://github.com/willhu-jpg/AMD-benchmarking-harness/blob/main/kernels/TK/bf16fp32/transpose_matmul/kernel_v2.cpp

5:05
current logic ^


Dan Fu
  5:05 PM
So it’s serializing smem -> reg -> mma?


Simran
  5:05 PM
what's your feel on what to change
5:05
yeah


Dan Fu
  5:05 PM
Oh ya
5:05
Easy


Simran
  5:05 PM
but there's double buffering on shared to register though


Dan Fu
  5:05 PM
Just change 4 -> 8
5:06
The chip can be doing twice as many things
5:06
The warp is basically serial
5:06
But the hardware can do compute at the same time as it does sram -> reg (edited) 
5:07
But the only way to do it is to have two warps scheduled at the same time
5:07
https://github.com/willhu-jpg/AMD-benchmarking-harness/blob/main/kernels/TK/bf16fp32/transpose_matmul/kernel_v2.cpp#L111-L121 so this does not happen in parallel right now
GitHubGitHub
AMD-benchmarking-harness/kernels/TK/bf16fp32/transpose_matmul/kernel_v2.cpp at main · willhu-jpg/AMD-benchmarking-harness
Contribute to willhu-jpg/AMD-benchmarking-harness development by creating an account on GitHub. (103 kB)
https://github.com/willhu-jpg/AMD-benchmarking-harness/blob/main/kernels/TK/bf16fp32/transpose_matmul/kernel_v2.cpp#L111-L121



Simran
  5:08 PM
ah ok i see
5:08
so one half of warps doing the reg loads while the other does the mma
5:08
will switch


Dan Fu
  5:08 PM
Yep


Simran
  7:43 PM
is it possible for us to get sudo access outside of the docker?


Dan Fu
  7:45 PM
No


Simran
  7:45 PM
ok cool nw
7:46
we'ere just seeing funny stuff where the same kernel is 40 tflops faster outside docker / might be diff rocm version though
7:46
all good


