(0:02 - 0:43)
at a low level as well as any sorts of, you know, basic hex for zeros, ones, if we want to say like, you know, tile.ones or something. And then that's about it, maybe some basic like relu or these types of operations that are just so you can apply them uniformly to a tile. Then you have piutils, this is literally just for binding, very, very simple, you don't touch it. 

You have types, so this just defines a shared memory tile. There's also complex versions or a shared vector. This just defines what's going on in the file.

(0:43 - 2:18)
Do these types also support like mixed precision or is it complex and just like the same for? There's no like, a tile has multiple types of D types, but a tile can be FP8, BF16, FP32, FP16. And you can separately instantiate like A for A and B, like different? Yeah, totally. And it's all designed in a way that takes care of all the packing for you. 

So if it's FP8, it'll be four-packed. If it's BF16, it'll be two-packed, that gets all taken care of. And the swizzling similarly is set up in a way where the user can pretend that they're indexing into a Torch tensor row and column and under the hood, we will take care of the address indexing for them and the swizzling patterns for them based off of the D type of the tile that they've requested and the size of the tile they've requested. 

We use either like, you know, 64 byte, 128 byte, 32 byte swizzling. And we did not need to change much of that for the AMD to NVIDIA side, except for some of the byte offsets, which we'll figure it out where because we're operating with 64 threads here, we wanted to change some of the logic and strategy there. So these are the only lines of code that changed going from NVIDIA to AMD.

(2:18 - 10:58)
I have a quick question about the structure. Is there a reason why there isn't just like a pip and CUDA backend using the same sort of repo? It was more just temporarily just to make it easier. Yeah, we'll put it all together at the end. 

Yeah. And so the similar thing is for registers, complex versions of it. And then the core register layout is defined in a short file here. 

And yeah, top level, you have complex or register tile versions. Globals are just pointers to the global memory, obviously, but like the nice thing about them is that we've set up the syntax in a way where it's extremely similar to indexing into a PyTorch tensor. So you just have a global memory pointer and some shared memory tile that you want to load to. 

And then you just say, this is my batch element, my head element, my row and my column. And on the NVIDIA side, we pre-configure all the TMA descriptors for the user, et cetera. And the striding is managed on the Thunderkitten's library side for the user to be compatible with that setup. 

And we could preserve the same thing on the AMD side. The last folder, so this is just basically the includes folder. This is just defining operations that are done at a wave or warp scope or at a warp group or on the AMD side, we just configured ourselves like a group of four waves is going to be able to say load big tiles together and share the effort there into shared memory. 

So in the groups here, um, we have operations over, um, uh, tiles to basically load from global memory to register global to shared, shared to register. Each of these files is pretty simple. It just has a function to load and a function to store. 

So if we open up, um, we were looking previously at global to, uh, shared. So let's start there. Okay. 

So here on the NVIDIA side, we're wrapping basically their, um, LDS and STS, um, load intrinsics and store intrinsics in this way. Um, so just to show you, um, this is what a load looks like. It's partitioning work over the thread. 

It's very quickly, um, loading into registers and storing down to shared memory. And so that's going to be very similar here where we have the load into registers and quickly write down to shared memory. So it's a very similar, um, you know, function. 

If we look apples to apples, the only thing is that we wanted to, um, you know, account for the, the different sorts of instructions here on the AMD versus NVIDIA side, but you can see that it's just wrapping the same kind of structure of intrinsics. Um, shared to register on the two sides is going to go from a shared memory tile to a register tile. There's tons and tons of like just template meta programming so that everything just reconfigures based off of tile sizes or D type. 

Um, so it's a pretty concise library, but this is the load, um, function for, um, the NVIDIA side. Um, you can see that we kind of, um, are also wrapping the intrinsics in here. Um, but notice that when we index into shared memory, we're using that dot index function that I kind of showed you existed in the, um, shared memory tile constructor. 

And this is the thing that basically takes in a simple row and column and computes the swizzled location of that logical, um, address, uh, with accounting for swizzling. Um, do you do higher level tiling than like shared memory? Do you do like L2 tiling? No, we don't do L2. Like the L2 cache is not programmable, at least on the NVIDIA side, but we have what's called like a task ID, um, where we use that to decide which thread blocks load different tiles, especially for the gems in a way that like encourages better L2 reuse. 

Um, but we implicit tiling in L2, right? Like instead of explicitly saying it, you're trying to load it, um, in such a way that you end up forming some sort of, um, space filling curves or some patterns in it. Exactly. Yeah. 

Yeah. So this is our entire Matmul kernel in BF16. It goes up to 91 lines, including all the boilerplate. 

Um, it's just doing some setup of this, as I said, task ID and super grouping so that we get good reuse on the B tiles of the, um, uh, of the, uh, tensors that we end up loading in. Persistent grid structure, um, producers are continuously going and using TMA load async to pull into a buffer. Um, and the buffer is configured, uh, configurable in the, um, template. 

So you just make with any pipeline stage you want, make with any number of consumer producer, um, warps that you'd want. And then the consumer just grabs from the shared buffer and runs the warp group MMA, and also takes care of storing down. We reallocate, you know, registers from producers to consumers, um, in a way, you know, so we can just use larger titles and that's the end of the story there. 

So you can see it's kind of just. Yeah. You mentioned earlier that there were, uh, Python bindings. 

Could you effectively reproduce this example also in Python? Um, like you, you mean like write the code in Python? Yeah. Like, uh, like kind of like how we could do it with Triton, uh, essentially. Oh, I see. 

We, I think we could, we've gotten the question a lot. We haven't taken like the past to, to really do that, um, super aggressively yet by Python bindings. I just met, like, you just make this file and you call it from. 

Okay. I see. So like the map mall itself becomes like a callable thing. 

Yeah. So it's just like import the TK kernel and it takes, you know, three seconds to, to compile. And then you just run the TK kernel. 

Oh, I see. I see. I think I misunderstood when you said like you had Python binding support because with Qt, I think with Qt, the latest Qt DSL stuff, uh, and Triton, you can essentially like you build these kernels within Python. 

Yeah. So I think we, we noticed, so at the time that we were doing this, the new Qt version obviously didn't exist, but like Triton existed. And I think, um, I think Qt is basically just taking what Cutlass has and like wrapping everything in a Python context, but like, so it's not actually changing the simplicity level, but I think for some of the kernels that we have on the, um, TK side, they're like, so our lab is more of a machine learning lab. 

We work on new styles of ML architectures. And for some of them we have, we need like slightly bespoke, um, like custom CUDA, like very rarely, but we wanted to be able to fail gracefully and like allow the user to take advantage of any like new instruction without waiting for people to add it in for them. So like we, we were able to port Thunder Kittens to Blackwell in literally, uh, like a day or two days, just kind of because it's all like, we can just wrap the PTX and we have full flexibility.

(10:58 - 11:46)
So we, we kind of wanted to preserve that, picked a different point in the trade-off space, but I you could like wrap it in, um, Python if you wanted. Yeah. Yeah. 

I think I was just, um, mentioning this because it's, uh, even though it's just a wrapper, I think it, it is, uh, an abstraction choice, right? Like Qt is essentially saying we're going to allow thread level programming, uh, much like you do in CUDA, but as a Python DSL, whereas Triton extracts all of that away and says, we're going to do everything for you. And you just program it at work, uh, at a thread block level. Yeah, for sure. 

I think that, that Mojo, there are a ton of tools and yeah, also trying to figure out what's easiest for different types of people. But yeah, I think that's a super, um, you know, valid approach too and worth considering. Yeah.

(11:47 - 14:40)
So what was the, um, I, I, I'm gonna speak for Ryan as well, but we kind of understand a lot of this. Um, what was the context behind, uh, what you would, uh, like how can we help you essentially? Yeah. Maybe we could switch a little bit to that. 

Yeah. So we've been trying to get up to, uh, so our understanding on this hardware is that we've been, so we've been comparing to the PyTorch version of the kernel so far. I know there's like Hiploss LT and so on, and getting roughly like 700 teraflops out of the, um, MI325. 

Um, we have been struggling to get past, um, somewhat like 500 teraflops, um, with our strategy, uh, so far. And, um, part of getting here was like, yeah, kind of, we, we see in the traces, no more bank conflicts, pretty decent, like L1 cache reuse and so on. Um, but kind of just wanted to run through the strategy we're using on the actual like kernel side and see if, um, you guys had any sort of low level suggestions on how to improve it. 

Um, so, uh, can I, can I, uh, say one thing before we move on? So PyTorch, um, NATML actually calls into Hiploss LT. Okay. Yeah. 

Okay. So, so our, our main gen driver is, um, is called tensile light or tensile. Um, it is the code gen code generator for Hiploss LT. 

Uh, so if you notice like the CIJK, whatever calls that, uh, PyTorch makes, um, it's actually calling the kernels generated, generated within Hiploss LT for, um, uh, through, through, uh, tensile light. So, and that is close to handwritten code, almost, uh, handwritten assembly basically. Um, but it's a Pythonic way to generate that handwritten assembly based on, uh, the kind of optimization strategies that you would pick. 

Um, and just by looking at the performance you're getting, um, I might have a quick idea of like what that could be. And I just want to first ask before you like explain all the stuff that you've tried, because I'd love to in general, learn all the things you've tried. Um, because this, I love this project actually, like I have a intern that is starting next week and, um, I'm going to, um, have him take a closer look at, at it as well, uh, from a different perspective, which we can talk about in the future. 

So, um, MI300, um, is a desegregated L2 architecture. So it's like a chiplet design. Um, and it's, uh, most people program it like an H100 or like an NVIDIA machine.

(14:41 - 21:28)
Uh, but inherently it's actually quite different from it. Um, uh, and I will, if it's okay, I can pull up a blog and then you can tell me if you have tried this or not. Um, and then we can switch back to like Thunderkittens and see, uh, some of the differences. 

One sec. Okay. This, um, is a little bit relevant, but, uh, not exactly what, uh, just the optimization would look like, but it's good to look at pictures. 

Um, so, uh, what I mean by desegregated architecture is that, um, it is not like a monolithic GPU. Um, each of these like compute units or each of these XCDs is like a bunch of compute units. So you can think of, uh, from NVIDIA terminology, each single XCD has a bunch of, um, SMs inside it. 

Um, in MI300X there's 38, uh, SMs in each one of these XCD. Um, but the core part of it is that each XCD actually has its private L2. So L2 isn't necessarily shared across XCDs. 

Um, and when we program our like monolithic traditional GPUs, like MI200 or H100 or even B100 to an extent, um, and we do sort of swizzling patterns, uh, like to, to better take advantage of, uh, L2 locality, because L2 locality is like a huge part of, uh, um, like the initial sort of performance, right? It's like a good 10, 10, 15%. If you are not doing it right, it can be really terrible because you get no reuse out of it. Um, how we program it is like, we assume there's a giant L2. 

Um, yeah, I think, I think that's it. So like Ryan linked to the exact piece of code that you will probably play around with. Um, so it's a, uh, well, how we program this is we think of it as a giant L2 and then, uh, we schedule things. 

So in this case, these are going to be your task IDs. Um, and then you try to fetch your tile based on your task ID, whether that corresponds to maybe like the thread block ID or whatever that abstraction looks like, or maybe a cluster ID. Um, so is there some notion of, uh, some processing elements ID being, uh, used to fetch the resultant tile ID from the, from the global memory? Um, however, when you do it that way in monolithic GPUs, you can, uh, do like space filling curves and everything works fine because you have an L2 that's shared across all XCDs, right? Um, on, uh, AMD GPUs, that's not, uh, the case. 

Every XCD will have its own L2. Um, and even more so if you're launching a persistent kernel across all XCDs, um, so let's say you have 304 compute units or SMs, um, and you set your grid size to 304 and you launched a persistent kernel that loops over, uh, segments of tiles, uh, like processes 304 tiles in, uh, every like wave of work. Um, what's going to happen is, uh, the tiles are going to, uh, or the work groups or thread blocks are going to be assigned by the scheduler like this. 

So the first one is going to go to XCD zero. The second one is going to go to XCD one. Third one is going to go to XCD three, four, five, six, seven, eight. 

And then it's going to come back around to assign, uh, nine, 10, 11, 12, 13, 14, 15, 16. And it's filling the GPU or XCDs first, like it's spreading the work across XCDs first before internally spreading the work onto the SMs per XCD. Um, I hope that makes sense. 

That's what the scheduler is doing. Um, so for gems, we have to actually undo that because we want all the results tiles that are close together on filling the first XCDs SMs first before moving on to the next one before moving on to the next one, because that's how you get the reuse, right? Like that's how like all of, if your neighboring tiles are in the same XCD and XCD here, like again, group of SMs, right? Um, that's, that's when your neighboring tiles would, would get reused from other neighboring tiles that got assigned to the same XCD. That makes a lot of sense. 

So just to make sure I understand how many, um, SMs are in an XCD and are there two like blocks of these XCDs that share L2 then? Uh, no. So, uh, uh, there's each XCD in MI300, uh, series has its own individual L2. Yeah. 

Uh, I can talk about the, um, memory hierarchy in a second, but each XCD has its own individual L2. So each of these will have like a block inside it. Yeah. 

Let me see if I have a, I definitely have a better presentation for this. Um, so yeah. Uh, uh, so each, each XCD will have its own, um, L2, but, um, the, uh, what was your other question? Sorry, I'm kind of doing a lot of Or I just wanted to understand, uh, where the reuse opportunity, make sure I understood where the reuse opportunity is. 

I know. So each XCD has its own L2, but does that mean that the like strip of XCDs that you were showing in the figure can also benefit from any form of use through? Yeah, yeah, yeah. Okay. 

Yes. Yeah. So it's kind of like this, this is a much better picture, right? So they share an L2 and then we have a last level cache actually. 

So pairs of XCDs share a last level cache. So you can form two levels of tiling. That's why, uh, Brian asked that initial question. 

Um, so there's could be one level of implicit tiling. This is not like actually you're not tiling inside L2, like explicitly because the caches are not programmable, but, um, you have a implicit tiling with how you access the, the, the tiles, uh, in the L2. And then we have an another implicit level of tiling in our last level cache, which is, which in this case is shown as like four separate LLCs, but you can think of it as a giant LLC, a memory site cache that is, uh, for all of these purple blocks and each purple block is a pair of XCD.

(21:29 - 22:27)
So purple blocks are called, uh, Iodized or AIDs. Um, these Iodized are the network of this giant chiplet, uh, based GPU. And it's connecting things across, uh, like these boundaries. 

These interconnects, um, are the, are like the AIDs have these interconnects, um, allowing you to, allowing this XCD to access this XCDs data or this XCDs to access this XCDs data. And when I say this XCDs, I really mean the HBM next to that XCD or the global memory next to the XCD. So you're forming two levels of tiling. 

First one is in L2, the second one is in LLC, but most of your performance comes from L2. You don't really need to, even for your kernel, the initial step, you wouldn't even worry about the last level cache. As long as you can organize, um, in a single XCD first and then, um, uh, get the reuse from, from that, you should be already like a good bump of performance up.

(22:27 - 27:02)
Um, and, uh, your other question was how many, uh, SMs are in each XCD. Uh, so in MI300x, there are 38 compute units. Uh, so just take the total compute units and divide it by eight because there's eight XCDs. 

So I think in MI325, it's, oh, it's 32, uh, SMs per XCD or CU, we call it compute units. Um, yeah, just, uh, you could probably just look up how many, uh, compute units I have, or this might be also publicly available. I just don't work on MI325 that often, so I'm not sure. 

Um, yeah, but MI350, our like latest, latest GPU that just got announced is, um, 32 compute units or SMs per XCD. Um, and there's total 256 of them. So, um, eight XCDs. 

Uh, and then the pattern, the, the, the tiling pattern looks kind of like this. I, I hope you can see this picture. Um, but, uh, you're trying to create these like space filling curves, right? So A, B, um, you, you may, you may want to do, uh, one sec. 

Uh, you may want to do 0, 1, 2, 3, instead of 0, uh, 1, 2, 3, 4, 5, 6, like instead of going linearly where you only get reuse out of, uh, one side, you might want to form these like nice, uh, like Z order curves, right? Um, and this is what you do in like a monolithic GPU because you have a single L2, all CUs share that L2. Uh, but in MI300x, what happens is you have to separate XCDs. Um, so you, if you don't, uh, fix that like scheduling thing, uh, you get like really random patterns where neighboring tiles end up in, uh, different XCDs. 

So the color purple means this lives in purple L2, this lives in green L2, and they will never be able to share data. They can, these L2s are not connected. They're only connected through last level cache. 

Um, so your L2 level hit rate just drops significantly because each XCD is just not, uh, processing like neighboring tiles. Um, yeah, these figures are supposed to illustrate like how the scheduler, uh, should schedule it. So in this case, or how it schedules it. 

So in this case, if you had, uh, four tiles and all, all of these are neighboring tiles, right? These are the closest tiles at the start of the matrix in the output tile. Um, and you end up scheduling them. The default schedule would put the first one and first XCD, the second one and second XCD, third one, third, fourth one, fourth. 

So they're all separate L2s and they will all get, uh, cache misses. Whereas if you put them on the same, um, on the same XCD, maybe I have a picture for that too. I don't have a picture for that. 

If you put them on the same XCD, you will get pairs of them using the same L2, right? Um, and how you fix it is through this little piece of code. Um, so, uh, this is how you do, um, L2 swizzling, right? Like, I don't know what you would call it, but in a bigger term, it's called like swizzle tile or swizzling, some sort of swizzling, uh, which is getting the reuse in L2. Um, so you have like, you do some math where you create these like space filling curves. 

So this is the reorder function. Um, and then you return the reordered, uh, work group ID or a thread block ID to the user. And the user uses that to then, uh, uh, fetch their resultant tile. 

Uh, so to fix this, uh, we have something called swizzle tile, which, um, where is it? This right here. Swizzle chiplet. So it's like, um, before you swizzle the tile, you have to swizzle the chiplet by number of XCDs. 

It's like just a mod and a divide function based on this numXCD parameter. Uh, and I can send you these, this snippets of code. I actually, Ryan has posted this already, so you can kind of read it on your own. 

Um, but I think of it as like default was this order where every color was going to a separate XCD. Um, and how it fixed it was it put all the XCDs, uh, all the neighboring tiles on first on the same XCD. And then on top of it, then you can form squares afterwards, um, to get even more reuse. 

So this is, this is after it's fixed with swizzle chiplet. And then this is swizzle chiplet plus the traditional swizzle tile, which is like the space filling curves that you wanted to make. Um, and then you get these like nice space filling curves.

(27:04 - 28:40)
Sorry, that's a lot of it, but I hope that kind of explains what the problem is and potentially how to fix it. Uh, yes, this is, um, this is super helpful. Um, I think like two things in mind. 

So one, when I look at the rock proof, um, statistics, it shows roughly a 77% L2 cache hit rate versus something like 80% for the PyTorch, um, the CIJ or whatever that we're also profiling. Um, and so I wasn't a hundred percent sure if that was our, um, just share our entire bottleneck. So this is what happens when we profile our like micro TK, um, kernel that is just, you know, in, in here. 

And we can see a pretty decent L2 cache hit rate and a pretty, uh, it's not great, but like non-zero L1 cache hit rate. Um, and so we were, and I guess sort of like, yeah, no conflicts like that. And so I think there's probably room to improve there. 

Cause we're definitely not doing any of this swizzle pattern that you talked about. We're literally just taking row and call as. So that, that is a swizzle right there, right? The divide by two mod by two. 

Is that not? Oh, okay. Yeah. Yeah. 

Yeah. Yeah. Um, so, so you will have some level of it, but it's the, what is explained would be like above it, I guess it would go like above.

(28:40 - 29:33)
Okay. Okay. I think, um, sorry. 

Hey guys, uh, just joined from another customer meeting, but I think that warp ID could be technically separate from the, the L2 is more work group level, right? And if I'm not sure that if that warp ID really means wave ID, if it does, then it could be potentially independent of the work group shuffle or group swizzling that I the Triton tutorial has, for example. Yeah. Yeah. 

This is just like wave ID. So I don't think we're doing like anything fancy there. The only goal of that was, um, well, my point there would be like, I think it might be implicitly happening like, uh, with, with how the warps are getting scheduled, I guess.

(29:34 - 29:59)
Yeah. That's the sense that we have from looking at this, that it's okay. I think the big issue is when we look at the trace, um, uh, when we look at one of your guys's traces, there's really nice, um, pipelining that occurs across, um, across the, uh, like say half the waves end up doing loads and half the waves end up doing compute, and then they kind of swap roles. And our trace looks like a complete mess. And so the strategy that we tried was to basically load in large-ish blocks into shared memory.

So say like 128 by 64 blocks, we're only using like four waves. And then we do a register prefetching where we first upfront load a part of the A and B. We load part of the columns and rows for the A and B matrices, and then put it into registers and kind of prefetch the next register, let the waves basically operate on the current registers and do the matrix multiplies and accumulate and then swap the register buffers. And so this is what is our best strategy to get up to like the 500 teraflop number that we talked about or 490 teraflop number that we talked about.

But obviously like this is still very sequential, like all these loads are happening and then all these compute is happening. And I think we've been repeatedly told that your best kernels use something like eight waves and have more of this half the waves are doing compute, half the waves are loading, or maybe some sort of dot slicing. Or we just think our fundamental strategy might be a bit off.

No, I think this is great. This is a great first step. And I'll tell you why.

Because to do what we wanted to do, to do that, I mean, we call it ping pong. So to do the ping pong thing, we would need this. But the missing bit, that's why it's harder to talk through text, but I think it's easier to talk through like this video call as we can introduce kind of like this lag between like the waves.

So think about it this way. Say you have two waves in a single SIMD. And this is also why we have eight waves, by the way.

Because if you have eight waves, it forces the scheduler to put two waves in a single SIMD. And when you do that, you can actually do, for example, before you go to the loop, you can do, for example, if you are wave one, or like if you're wave one, you wait, you do a work group barrier. That way it kind of waits for this other wave to go through.

Like, for example, say you can put a read and then you put another work group barrier. That way, one of the waves is always one step ahead. I don't know if that makes sense.

So if you share your screen again, sorry, with the kernel, one thing we can do is, for example, you can add a work group barrier. After the prefetch register tile. And then before the prefetch register tile, you could do an if condition.

And you can check your warp ID. Say, I'm just giving some random example. If your warp ID is divisible by two, then you do a work group barrier.

So what it does is, all the wave IDs that's divisible by two will stop there for a moment. And then after the wave ID non-divisible by two does the prefetching, then those waves divisible by two will start moving. I see what you're saying.

So it's like injecting an artificial, not artificial, but injecting a manual lag so that the hardware just executes on its own. Exactly. It's not like an explicit producer-consumer type work specialization.

It's more of just like everyone's doing everything, but just in off state. But that's why you see the ping-ponging. If you see what happened with our ping-pong schedule, it's really all offset by one.

That's what you're seeing. But really, the trick is to do it inside the for loop. So you do the conditional barrier outside the for loop.

So within the for loop, it's just pure work group barriers without any conditional. We call it conditional barriers. And that would be really quick.

Okay, this is really helpful. So we'll try the L2 swizzling approach that you talked about in Triton for a boost there. And then we'll try this staggering approach on the work groups and the conditional piece as concrete next steps.

Do you guys have a target number in mind, by the way? Or is it just do our best kind of deal? No, no, our target was the HIP-LT number. So basically, we're comparing... Well, I mean wrapped in PyTorch. We're comparing to PyTorch, which gets roughly 700 teraflops.

I see, I see. So we're looking for about 200 more. Yeah, so this thing, what I've seen, it can give you a good 30 to 50% boost.

Like 1.3, 1.5x. That sounds like a significant chunk there. I mean, our trace is clearly... Something is going wrong. So, okay, we'll try that then.

Sounds good, sounds good. For the work group barriers, is that just the S-barrier? Or do you have something else? Yes, it is the S-barrier. Okay.

And that's a barrier for the entire work group. Yeah, exactly. Exactly.

But that's how you kind of do the ping pong, because it calls the barrier for the entire work group. But you artificially, as Simran correctly identified, you artificially block some, half of the waves first. And then, as everyone hits the same work group barrier... Well, so the trick is, they shouldn't be hitting the same work group barriers.

But they all need to call the work group barrier together to be unblocked. I don't know if that makes sense. So, you have a conditional for, say... So, you have two wave groups.

Each wave group has like four wave fronts. There's a conditional in the four-to-four loop that's checking if your work group... If your wave group is zero, you're doing these prefetching loads. But then everybody hits a barrier-wide... Yeah, exactly.

Is that outside the conditional? It's outside the conditional, exactly. Right, okay. Yep, yep, yep.

So, if you think about it, before the four-loop, you can add a conditional barrier to block half the work... One of the, what do you call it? Wave groups? So, you can block one of the wave groups. And then, the other wave group will still be going, right? And once it finishes prefetching, if it calls work group barrier as well, now they'll both be unlocked. Now, the wave front that was initially blocked will start prefetching, and then the other wave front will start doing the MMAs.

That way, you have the quote-unquote ping-pong. But then, you would still have to have conditionals inside the four-loop? You just have one more conditional outside the four-loop to even it out. So, inside the four-loop, your wave fronts will always be one lagging behind.

Sure. So, that's why you don't really care about the conditionals, because it's treated as one lag, and it just keeps going. Yeah, okay. 

That makes a lot of sense. So, we'll just add it right here, and right here. Yeah, let me see if I can even show... If there's an example, that'd be great, too.

Yep. And the last little semantic thing we wanted to understand on the bank conflict side is, obviously, there's 64 threads and 32 banks. And so, when the profiler registers zero bank conflicts, it's basically just saying that all 32 banks are being accessed in one clock cycle? Yes, and no.

Just because when you do dsread64, it has multiple clock cycles, and then each thread is accessing... Depends on the clock cycle, you'll have different threads accessing different pieces. So, I think you just need to make sure that the threads in the same clock cycle shouldn't be, you know, accessing the same bank. It means you're still maximizing LDS bandwidth, right? Yes, for sure, for sure. 

Zero bank conflicts are always a good sign. Yeah, yeah, yeah. Okay.

Let me see if I can share my screen real quick. Hang on here. Sure.

So, these are all open source. I could even send it over to you guys. Okay, great.

So, this is, I mean, this is one of our tasks. But, essentially, what we're doing here, I don't know if you can see, so we're slice... I mean, we have a slice local load, so we do a slice on LHS and RHS. So, we have these guys, and then we can get these, let me see, scheduling barriers.

Do you guys see the workgroup barriers? Ah, perfect. So, this is, you can imagine that this is inside the for loop, and you can think about it this way. So, at time zero, the wave group, one of the wave group will still be outside the for loop.

And the first wave group will be going into this, right? Will be doing this piece. And then at time one, once you call the workgroup barrier, that other wave group will start doing this piece. And then our, you know, older wave group will start doing this piece until the workgroup barrier.

And then once it hits, now they both hit the workgroup barrier, now the one that was initially outside will start doing this piece. And then you will have the other wave doing this piece. So, that way there's always like one leg behind with the workgroup barrier.

And then we kind of add, I think that we call it asymmetry. We have this thing, I mean, we add another pass that just adds a conditional barrier. Before and after. 

Before and after. Before and after the wave. Sorry, before and after the hot loop.

So, we have S wave high, S wave low. To kind of like even them out. So, this is kind of like what makes the first lag, right? We tell the first wave group to wait.

Does that slightly make sense? Yes, yes. And under the hood, you're again wrapping S barrier for the conditional barriers. Yep, yep, yep.

Perfect. I mean, conditional barrier is really an SCF, it's like an F and a barrier. So, yeah.

Yeah, perfect. Sounds good. We'll try this.

Are there any more questions on your end, Will? I think I'm good. I think I'm good too. Okay, so my interpretation of an S barrier now is that it's a synchronization mechanism between warp groups.

Within threads in a warp group. So, only one warp group. Okay, yeah.

But you could use it between wave groups, I guess. So, it's a synchronization mechanism for all the threads within a warp group, but what happens when different warp groups call it? Doesn't matter. Doesn't matter. 

Okay, sure. It's just synchronization within a warp group. Yep.

So, when you're trying to, I guess, buffer these two different warp groups, one happens maybe a step earlier than the other? But this is not different warp groups, right? Sorry. This is different waves, essentially. Different waves, but in the same warp group.

How big are your warp group sizes? So, it'll be 8. 8 waves, which is 8 times 64, 512. I think a thread block is a warp group. Yes, each thread block is a warp group.

Gotcha. Okay, that makes a lot of sense. But if it's at a warp group level and we want to only do a barrier on 4 little waves, then how does that work? Exactly, because say you have, because if you do a barrier within 4 of the waves, now those 4 waves are going to wait until the other 4 waves have another warp group barrier.

Oh, I see. It's kind of like n-barriers in those cases. Yeah, yeah. 

Okay. Got it. That makes a lot of sense.

And sync threads is just between multiple warp groups. I think so. If you have multiple warp groups scheduled onto the same... Yeah, okay, sure. 

Yeah, that makes sense. So, I need to check also sync threads. I don't know how confident I am to say, hey, it is within warp group, it is external to warp group.

I think, yeah, I think it depends on the compiler. Okay, if an entire block just consists of a single warp group, then that means sync threads and the S-barrier are equivalent? Yeah, yeah. Okay, yeah. 

That makes sense. Okay. But, yeah, I personally, I don't know if this is the best practice.

I personally would just call out the barrier manually, just because you don't know what's going on. I know that sync thread has been, it's very like, it feels very ambiguous almost. Sync thread between what, right? So, yeah.

Okay. Sure. Okay.

And, okay, I think we can try this strategy. I think it makes a lot of sense. Yeah.

Oh, one more thing. You said you fit 256 by 64. We're already getting register spills with like 128 by 64 with our four wave current kernel.

Is there anything? What data type? BF16. BF16. Oh, okay.

Wait, so even with dot slicing, you guys are seeing that or no? Oh, so actually specifically, oh, okay. Actually, with dot slicing, no. We haven't combined dot slicing and the register prefetching yet, as you can see here.

And also, we set it up to do two on the launch bounds and are not maxing out shared memory. So, we can reconfigure it to be launch bounds one and, yeah, I guess. Okay.

So, that's probably what you're doing. That is one of the main reasons behind why we came up with dot slicing. It's for this register pressure.

Okay, perfect. Okay, sounds good. So, you're combining the 256 by 64 plus some kind of prefetching, which increases register pressure, but you're counteracting that with the dot slicing.

Yeah. Exactly. Makes sense.

And just sorry for all the screen shares, but this is what dot slicing is, right? Like, we just inner loop on a sub, like, I guess we have typically been doing, like, loading in a full case step, but instead we just load in tiny chunks in an inner loop of our overall loop. Yeah, I think, so the dot slicing itself, you should have it load A, load B, MMA. Then you wouldn't, that way you kind of like, because if you do all the loads and then all the MMAs, you still need those registers, right? So, you need to load A, load B, MMA, load A, load B, MMA.

Because that way, yeah, you don't need those initial registers anymore. Okay, that makes sense. And I'm assuming for now you're using, like, a nicely divisible K length? Yes.

Yeah, exactly. Yeah. It's all very clean.

More. Yeah. It's hard to fix.

Yeah. Okay. Sure.

Okay. Thank you. Yeah, this is really, really helpful.

Thanks. No worries. Yeah.

And, you know, we're always on Slack, so let us know if you have any questions. Those thread trays definitely helped, because we can see what's actually going on. Yeah.

But yeah, it's cool. You guys got started with Omniperf and all this stuff, which is really nice. Yeah, it's a fun, I hadn't used the AMD chips until last week, and then I bought a bunch of stock, because I was like, it's very good.

Oh, yeah. I can't say anything there. I hope you didn't buy stocks after using our profiling tools, because... Oh, I hope they get... It's pretty good.

Yeah, I like it. But okay, I think, yeah, we're trying to do this one, and maybe one more kernel, and like write a blog post, and we can all share it out and stuff like that. So it'd be good to, yeah, trying to just finish up and get it out there, see what people think.

Yeah. And have to do our DNA, I guess. But yeah.

Yeah, no, this sounds great. This sounds great. It's super exciting to have you guys doing porting this.

And, you know, yeah, I mean, I've told Dan as well, because even internally, like this ping pong scheduling is a work of many different teams, right? Like, we're all mind sharing. So it'd be cool to have you guys onboarded onto this schedule optimization, because that's where the play is at. Yeah.

Yeah. Yeah. And it's really interesting to learn about the differences from AMD and NVIDIA and Cerberus and all the hardware.

Because I think, as I said, we build a lot of architecture, like alternatives to the transformer. And part of what we're really excited about is, if you start from the hardware first, what type of architecture makes the most sense? Rather than saying, let me map a transformer everywhere or map Mamba everywhere. Like, what if you actually just start from the strengths and weaknesses and differences of a particular platform? Then what makes the most sense? So yeah, Jose and I have talked a little bit about this on Slack.

But yeah, that's what we're downstream interested in as well. Very cool. Very cool.

Yeah. You guys should send up a link. Maybe I missed it.

But yeah, it would be really cool to see this new AMD-based architecture. Yeah. Yeah.

But yeah, we'll keep chatting. All right. All right.

Thanks, everyone. I'll try to circulate the video. I don't know if I'm allowed to send it to you on the website.

I have to see how the permissions work. But if I can, I'll just send it your way. Great.

Thanks, everyone, for joining. Thank you. Bye.

Bye. Thanks, everyone. Nice seeing you all.